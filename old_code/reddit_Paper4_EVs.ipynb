{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0034177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163abe11",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pickle5  # pickle is part of the standard library, but pickle5 can be used for enhanced features\n",
    "# !pip install numpy\n",
    "# !pip install requests\n",
    "# !pip install pandas\n",
    "# !pip install glob2  # glob is part of the standard library, but glob2 is an enhanced version\n",
    "# !pip install scipy\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install tabulate\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def toc(start_time):\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    \n",
    "# from zipfile import ZipFile\n",
    "import re\n",
    "# from pprint import pprint\n",
    "# import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.transforms as mtransforms\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.rcParams['xtick.major.size'] = 3\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "sns.set_context('talk', font_scale=.65)\n",
    "mpl.rcParams.update({'text.usetex': False})\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import pandas as pd\n",
    "# # Choose an embedding backend\n",
    "# !pip install bertopic[flair, gensim, spacy, use]\n",
    "\n",
    "# # Topic modeling with images\n",
    "# !pip install bertopic[vision]\n",
    "# !pip install xgi==0.7.3\n",
    "# !pip install distinctipy\n",
    "# Load document info and topic frequencies\n",
    "\n",
    "# --- Define sector keywords as before ---\n",
    "sector_keyword_strength = {\n",
    "    'transport_strong': [\n",
    "        \"electric vehicle\", \"evs\", \"bev\", \"battery electric\", \"battery-electric vehicle\",\n",
    "        \"tesla model\", \"model 3\", \"model y\", \"chevy bolt\", \"nissan leaf\",\n",
    "        \"ioniq 5\", \"mustang mach-e\", \"id.4\", \"rivian\", \"lucid air\",\n",
    "        \"supercharger\", \"gigafactory\", \"zero emission vehicle\", \"zero-emission vehicle\",\n",
    "        \"pure electric\", \"all-electric\", \"fully electric\", \"100% electric\",\n",
    "        \"electric powertrain\", \"electric drivetrain\", \"electric motor vehicle\",\n",
    "        \"level 2 charger\", \"dc fast charger\", \"public charger\", \"home charger\",\n",
    "        \"charging network\", \"range anxiety\", \"mpge\",\n",
    "        \"bike lane\", \"protected cycleway\", \"car-free\", \"low emission zone\"\n",
    "    ],\n",
    "    'transport_weak': [\n",
    "        \"electric car\", \"electric truck\", \"electric suv\", \"plug-in hybrid\",\n",
    "        \"phev\", \"charging station\", \"charge point\", \"kw charger\", \n",
    "        \"battery swap\", \"solid-state battery\", \"gigacast\",\n",
    "        \"tax credit\", \"zev mandate\", \"ev rebate\", \"phase-out ice\",\n",
    "        \"e-bike\", \"micro-mobility\", \"last-mile delivery\", \"transit electrification\",\n",
    "        \"tesla\", \"spacex launch price?\", \"elon says\",\n",
    "        \"rail electrification\", \"hydrogen truck\", \"low carbon transport\"\n",
    "    ],\n",
    "    'housing_strong': [\n",
    "        \"rooftop solar\", \"solar pv\", \"pv panel\", \"photovoltaics\",\n",
    "        \"solar array\", \"net metering\", \"feed-in tariff\", \"solar inverter\",\n",
    "        \"kwh generated\", \"solar roof\", \"sunrun\", \"sunpower\",\n",
    "        r\"solar\\s+panel(s)?\", r\"solar\\s+pv\", r\"rooftop\\s+solar\",\n",
    "        r\"solar\\s+power\", r\"photovoltaic(s)?\"\n",
    "    ],\n",
    "    'housing_weak': [\n",
    "        \"solar panels\", \"solar power\", \"solar installer\",\n",
    "        \"battery storage\", \"powerwall\", \"home battery\", \"smart thermostat\",\n",
    "        \"energy audit\", \"energy efficiency upgrade\", \"led retrofit\",\n",
    "        \"green home\", \"net-zero house\", \"zero-energy building\",\n",
    "        \"solar tax credit\", \"pvgis\", \"renewable portfolio standard\",\n",
    "        \"community solar\", \"virtual power plant\", \"rooftop rebate\"\n",
    "    ],\n",
    "    'food_strong': [\n",
    "        \"vegan\", \"plant-based diet\", \"veganism\", \"veganuary\", \"vegetarian\", \"veg lifestyle\",\n",
    "        \"carnivore diet\", \"meat lover\", \"steakhouse\", \"barbecue festival\",\n",
    "        \"bacon double\", \"grass-fed beef\", \"factory farming\",\n",
    "        \"meatless monday\", \"beyond meat\", \"impossible burger\",\n",
    "        \"plant-based burger\", \"animal cruelty free\"\n",
    "    ],\n",
    "    'food_weak': [\n",
    "        \"red meat\", \"beef consumption\", \"dairy free\", \"plant protein\",\n",
    "        \"soy burger\", \"nutritional yeast\", \"seitan\", \"tofurky\",\n",
    "        \"agricultural emissions\", \"methane footprint\", \"carbon hoofprint\",\n",
    "        \"cow burps\", \"livestock emissions\", \"feedlot\",\n",
    "        \"recipe vegan\", \"tofu scramble\", \"almond milk\", \"oat milk\",\n",
    "        \"flexitarian\", \"climatetarian\",\n",
    "        \"cultivated meat\", \"lab-grown meat\", \"precision fermentation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Combine all keywords for regex search\n",
    "sector_keywords = (\n",
    "    sector_keyword_strength['transport_strong'] +\n",
    "    sector_keyword_strength['transport_weak'] +\n",
    "    sector_keyword_strength['housing_strong'] +\n",
    "    sector_keyword_strength['housing_weak'] +\n",
    "    sector_keyword_strength['food_strong'] +\n",
    "    sector_keyword_strength['food_weak']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786ad6c",
   "metadata": {},
   "source": [
    "## climate subreddit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3349b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Directory containing the CSVs\n",
    "# csv_dir = r\"C:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\data\\reddit_subreddits24_csv\"\n",
    "\n",
    "# # Get all CSV files ending with \".csv\" (case insensitive), except those with \"stupid\" in the name\n",
    "# all_csvs = [\n",
    "#     os.path.join(csv_dir, f)\n",
    "#     for f in os.listdir(csv_dir)\n",
    "#     if f.lower().endswith(\".csv\")\n",
    "#     and \"stupid\" not in f.lower()\n",
    "# ]\n",
    "\n",
    "# # Separate submission and comment CSVs\n",
    "# sub_csvs = [f for f in all_csvs if \"submission\" in os.path.basename(f).lower()]\n",
    "# com_csvs = [f for f in all_csvs if \"comment\" in os.path.basename(f).lower()]\n",
    "\n",
    "# # Load and merge all submissions and comments\n",
    "# if sub_csvs:\n",
    "#     df_sub = pd.concat([pd.read_csv(f) for f in sub_csvs], ignore_index=True)\n",
    "# else:\n",
    "#     df_sub = pd.DataFrame()\n",
    "\n",
    "# if com_csvs:\n",
    "#     df_com = pd.concat([pd.read_csv(f) for f in com_csvs], ignore_index=True)\n",
    "# else:\n",
    "#     df_com = pd.DataFrame()\n",
    "\n",
    "# # Filter comments: keep only those with non-empty body and author, and remove automoderator and deleted/removed comments\n",
    "# mask = df_com['body'].str.len() >= 1\n",
    "# df_com = df_com.loc[mask]\n",
    "# df_com['body'] = df_com['body'].astype('str')\n",
    "# mask = df_com['author'].str.len() >= 1\n",
    "# df_com = df_com.loc[mask]\n",
    "# df_com['author'] = df_com['author'].astype('str')\n",
    "\n",
    "# # Filter comments: keep only those with at least 2 words in the body\n",
    "# mask = df_com['body'].str.split().apply(len) >= 2\n",
    "# df_com = df_com.loc[mask]\n",
    "\n",
    "# # Filter submissions: keep only those with non-empty title, and remove those with title '[deleted by user]'\n",
    "# mask = df_sub['title'].str.len() >= 1\n",
    "# df_sub = df_sub.loc[mask]\n",
    "# df_sub['title'] = df_sub['title'].astype('str')\n",
    "# df_sub = df_sub[df_sub['title'] != '[deleted by user]']\n",
    "# df_sub['body'] = df_sub['title']\n",
    "\n",
    "# # filter out deleted comments and comments by automoderator\n",
    "# automoderator = df_com['author'].str.lower() == 'automoderator'\n",
    "# # Use a raw string for the regex pattern to avoid SyntaxWarning\n",
    "# deleted_com = df_com['body'].str.contains(r'\\[deleted\\]|\\[removed\\]', regex=True)\n",
    "# df_com = df_com[~(automoderator | deleted_com)]\n",
    "\n",
    "# df_joined = pd.concat([df_sub, df_com])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique comments/submissions matched per keyword:\n",
      "vegan: 2246374\n",
      "tesla: 453630\n",
      "veganism: 389782\n",
      "evs: 231101\n",
      "vegetarian: 212757\n",
      "model 3: 86518\n",
      "plant-based diet: 57631\n",
      "bev: 55064\n",
      "phev: 54186\n",
      "red meat: 49357\n",
      "tax credit: 49259\n",
      "electric car: 48245\n",
      "solar panels: 44011\n",
      "supercharger: 42813\n",
      "model y: 39576\n",
      "rivian: 39295\n",
      "electric vehicle: 37192\n",
      "seitan: 35114\n",
      "charging station: 29103\n",
      "almond milk: 28022\n",
      "ioniq 5: 25787\n",
      "nutritional yeast: 22969\n",
      "tesla model: 22272\n",
      "factory farming: 21388\n",
      "oat milk: 21363\n",
      "id.4: 19817\n",
      "e-bike: 18405\n",
      "charging network: 17847\n",
      "net metering: 16933\n",
      "nissan leaf: 13308\n",
      "lab-grown meat: 13279\n",
      "beyond meat: 13149\n",
      "powerwall: 12889\n",
      "solar power: 11451\n",
      "impossible burger: 11152\n",
      "sunpower: 10527\n",
      "chevy bolt: 10412\n",
      "all-electric: 10004\n",
      "tofu scramble: 9690\n",
      "plug-in hybrid: 9313\n",
      "sunrun: 8039\n",
      "range anxiety: 7357\n",
      "public charger: 7144\n",
      "level 2 charger: 7138\n",
      "dairy free: 6881\n",
      "bike lane: 6529\n",
      "battery storage: 5691\n",
      "solar installer: 5362\n",
      "veganuary: 5190\n",
      "flexitarian: 5162\n",
      "home charger: 4901\n",
      "tofurky: 4651\n",
      "solar array: 4636\n",
      "dc fast charger: 4510\n",
      "electric truck: 4488\n",
      "meatless monday: 4485\n",
      "rooftop\\s+solar: 4477\n",
      "rooftop solar: 4475\n",
      "plant protein: 4157\n",
      "mustang mach-e: 3757\n",
      "fully electric: 3505\n",
      "lucid air: 3262\n",
      "solar roof: 3016\n",
      "carnivore diet: 2968\n",
      "solar pv: 2950\n",
      "mpge: 2726\n",
      "charge point: 2681\n",
      "battery swap: 2491\n",
      "battery electric: 2360\n",
      "kw charger: 2324\n",
      "electric suv: 2278\n",
      "steakhouse: 2249\n",
      "gigafactory: 1989\n",
      "solid-state battery: 1815\n",
      "grass-fed beef: 1765\n",
      "plant-based burger: 1742\n",
      "pure electric: 1641\n",
      "home battery: 1581\n",
      "pv panel: 1569\n",
      "community solar: 1443\n",
      "solar inverter: 1383\n",
      "feedlot: 1356\n",
      "battery-electric vehicle: 1267\n",
      "meat lover: 1260\n",
      "solar tax credit: 1127\n",
      "electric drivetrain: 1122\n",
      "ev rebate: 1025\n",
      "100% electric: 1002\n",
      "car-free: 792\n",
      "photovoltaics: 790\n",
      "micro-mobility: 740\n",
      "zero-emission vehicle: 673\n",
      "electric powertrain: 599\n",
      "cultivated meat: 551\n",
      "virtual power plant: 527\n",
      "beef consumption: 495\n",
      "feed-in tariff: 494\n",
      "precision fermentation: 463\n",
      "energy audit: 435\n",
      "soy burger: 370\n",
      "elon says: 348\n",
      "zero emission vehicle: 308\n",
      "kwh generated: 291\n",
      "smart thermostat: 275\n",
      "zev mandate: 241\n",
      "hydrogen truck: 232\n",
      "agricultural emissions: 193\n",
      "cow burps: 164\n",
      "last-mile delivery: 158\n",
      "animal cruelty free: 139\n",
      "energy efficiency upgrade: 117\n",
      "pvgis: 105\n",
      "recipe vegan: 102\n",
      "livestock emissions: 91\n",
      "renewable portfolio standard: 88\n",
      "low emission zone: 80\n",
      "bacon double: 74\n",
      "phase-out ice: 70\n",
      "veg lifestyle: 58\n",
      "green home: 53\n",
      "gigacast: 48\n",
      "methane footprint: 26\n",
      "net-zero house: 21\n",
      "electric motor vehicle: 20\n",
      "rail electrification: 15\n",
      "transit electrification: 7\n",
      "zero-energy building: 7\n",
      "led retrofit: 5\n",
      "low carbon transport: 4\n",
      "carbon hoofprint: 2\n",
      "barbecue festival: 2\n",
      "\n",
      "Subreddit x keyword stats (number of unique ids per keyword per subreddit):\n",
      "ClimateActionPlan: {'vegan': 721, 'solar panels': 667, 'electric car': 522, 'evs': 445, 'tesla': 441, 'electric vehicle': 286, 'vegetarian': 278, 'solar power': 260, 'red meat': 233, 'veganism': 130, 'battery storage': 106, 'plant-based diet': 94, 'charging station': 76, 'lab-grown meat': 76, 'rooftop solar': 67, 'rooftop\\\\s+solar': 67, 'all-electric': 62, 'model 3': 59, 'e-bike': 55, 'tax credit': 50, 'impossible burger': 48, 'beyond meat': 46, 'car-free': 34, 'solar pv': 34, 'bev': 32, 'factory farming': 30, 'supercharger': 27, 'electric truck': 26, 'bike lane': 24, 'oat milk': 24, 'solar array': 24, 'phev': 22, 'photovoltaics': 22, 'plant-based burger': 22, 'feedlot': 21, 'rivian': 21, 'charging network': 20, 'nissan leaf': 19, 'fully electric': 18, 'plug-in hybrid': 18, 'tesla model': 17, 'beef consumption': 15, 'battery electric': 14, 'community solar': 14, 'gigafactory': 14, 'almond milk': 13, 'range anxiety': 13, 'powerwall': 12, 'grass-fed beef': 11, 'seitan': 11, 'zero-emission vehicle': 11, '100% electric': 10, 'flexitarian': 10, 'plant protein': 10, 'solar roof': 10, 'meatless monday': 9, 'precision fermentation': 8, 'pv panel': 8, 'feed-in tariff': 7, 'pure electric': 7, 'battery-electric vehicle': 6, 'chevy bolt': 6, 'cow burps': 6, 'electric suv': 6, 'micro-mobility': 6, 'virtual power plant': 6, 'zero emission vehicle': 6, 'agricultural emissions': 5, 'charge point': 5, 'home battery': 5, 'nutritional yeast': 5, 'battery swap': 4, 'cultivated meat': 4, 'energy audit': 4, 'model y': 4, 'net metering': 4, 'public charger': 4, 'smart thermostat': 4, 'ev rebate': 3, 'kwh generated': 3, 'low emission zone': 3, 'mustang mach-e': 3, 'renewable portfolio standard': 3, 'electric powertrain': 2, 'energy efficiency upgrade': 2, 'level 2 charger': 2, 'meat lover': 2, 'methane footprint': 2, 'solid-state battery': 2, 'steakhouse': 2, 'veganuary': 2, 'carnivore diet': 1, 'dc fast charger': 1, 'green home': 1, 'home charger': 1, 'hydrogen truck': 1, 'ioniq 5': 1, 'mpge': 1, 'solar installer': 1, 'solar tax credit': 1, 'soy burger': 1, 'sunpower': 1, 'sunrun': 1, 'tofurky': 1, 'zero-energy building': 1, 'zev mandate': 1}\n",
      "ClimateChaos: {'electric car': 54, 'tesla': 33, 'electric vehicle': 30, 'model 3': 20, 'solar panels': 12, 'vegan': 12, 'solar power': 10, 'rivian': 9, 'tesla model': 9, 'evs': 8, 'fully electric': 5, 'veganism': 4, 'all-electric': 3, 'plug-in hybrid': 3, '100% electric': 2, 'flexitarian': 2, 'tax credit': 2, 'battery electric': 1, 'community solar': 1, 'electric truck': 1, 'factory farming': 1, 'green home': 1, 'livestock emissions': 1, 'net metering': 1, 'photovoltaics': 1, 'plant-based burger': 1, 'range anxiety': 1, 'red meat': 1, 'rooftop solar': 1, 'rooftop\\\\s+solar': 1, 'smart thermostat': 1, 'solar tax credit': 1, 'steakhouse': 1, 'vegetarian': 1}\n",
      "ClimateMemes: {'vegan': 812, 'veganism': 209, 'electric car': 107, 'solar panels': 104, 'vegetarian': 94, 'evs': 50, 'electric vehicle': 38, 'plant-based diet': 37, 'red meat': 37, 'tesla': 37, 'factory farming': 33, 'lab-grown meat': 33, 'bike lane': 26, 'battery storage': 20, 'solar power': 20, 'e-bike': 19, 'oat milk': 10, 'car-free': 9, 'beyond meat': 7, 'flexitarian': 7, 'almond milk': 6, 'tax credit': 5, 'pv panel': 4, 'rooftop solar': 4, 'rooftop\\\\s+solar': 4, 'seitan': 4, '100% electric': 3, 'meatless monday': 3, 'rivian': 3, 'agricultural emissions': 2, 'all-electric': 2, 'cow burps': 2, 'feedlot': 2, 'plant protein': 2, 'plant-based burger': 2, 'beef consumption': 1, 'carnivore diet': 1, 'dairy free': 1, 'fully electric': 1, 'grass-fed beef': 1, 'home battery': 1, 'impossible burger': 1, 'model 3': 1, 'nutritional yeast': 1, 'powerwall': 1, 'pure electric': 1, 'range anxiety': 1, 'solar array': 1, 'solar pv': 1, 'solar roof': 1, 'soy burger': 1}\n",
      "ClimateOffensive: {'vegan': 1306, 'solar panels': 419, 'vegetarian': 385, 'electric car': 382, 'veganism': 292, 'red meat': 220, 'evs': 215, 'electric vehicle': 185, 'plant-based diet': 176, 'tesla': 169, 'solar power': 104, 'lab-grown meat': 89, 'factory farming': 55, 'e-bike': 45, 'bike lane': 44, 'car-free': 44, 'tax credit': 41, 'battery storage': 36, 'charging station': 36, 'beyond meat': 35, 'rooftop solar': 35, 'rooftop\\\\s+solar': 35, 'oat milk': 33, 'meatless monday': 31, 'almond milk': 27, 'all-electric': 25, 'community solar': 25, 'impossible burger': 25, 'seitan': 24, 'solar pv': 21, 'feedlot': 20, 'grass-fed beef': 16, 'flexitarian': 15, 'rivian': 15, 'nissan leaf': 13, 'solar array': 13, 'bev': 12, 'electric truck': 12, 'photovoltaics': 12, 'plant protein': 12, 'beef consumption': 10, 'agricultural emissions': 9, 'micro-mobility': 9, 'plug-in hybrid': 9, 'range anxiety': 9, 'battery electric': 8, 'phev': 8, 'charging network': 7, 'fully electric': 7, 'model 3': 7, 'net metering': 7, 'nutritional yeast': 7, 'pv panel': 7, 'solar roof': 7, '100% electric': 6, 'battery-electric vehicle': 6, 'powerwall': 6, 'tesla model': 6, 'virtual power plant': 6, 'cow burps': 5, 'meat lover': 5, 'veganuary': 5, 'carnivore diet': 4, 'dairy free': 4, 'precision fermentation': 4, 'renewable portfolio standard': 4, 'zero-emission vehicle': 4, 'home battery': 3, 'plant-based burger': 3, 'steakhouse': 3, 'zero emission vehicle': 3, 'charge point': 2, 'chevy bolt': 2, 'cultivated meat': 2, 'electric drivetrain': 2, 'electric suv': 2, 'energy efficiency upgrade': 2, 'ev rebate': 2, 'kwh generated': 2, 'level 2 charger': 2, 'solar inverter': 2, 'soy burger': 2, 'supercharger': 2, 'electric powertrain': 1, 'feed-in tariff': 1, 'gigafactory': 1, 'green home': 1, 'hydrogen truck': 1, 'low emission zone': 1, 'methane footprint': 1, 'phase-out ice': 1, 'pure electric': 1, 'recipe vegan': 1, 'smart thermostat': 1, 'sunpower': 1, 'sunrun': 1, 'tofu scramble': 1, 'veg lifestyle': 1, 'zero-energy building': 1}\n",
      "ClimateShitposting: {'vegan': 7558, 'veganism': 1692, 'solar panels': 1330, 'vegetarian': 804, 'red meat': 453, 'electric car': 412, 'evs': 397, 'solar power': 326, 'factory farming': 274, 'battery storage': 259, 'plant-based diet': 249, 'lab-grown meat': 234, 'tesla': 205, 'electric vehicle': 137, 'e-bike': 82, 'oat milk': 75, 'solar pv': 75, 'seitan': 67, 'bike lane': 61, 'rooftop solar': 59, 'rooftop\\\\s+solar': 59, 'bev': 54, 'almond milk': 51, 'car-free': 48, 'photovoltaics': 47, 'beef consumption': 40, 'meatless monday': 38, 'solar array': 35, 'pv panel': 33, 'tax credit': 32, 'plant protein': 30, 'flexitarian': 29, 'impossible burger': 29, 'feedlot': 25, 'agricultural emissions': 22, 'grass-fed beef': 21, 'beyond meat': 20, 'nutritional yeast': 17, 'all-electric': 15, 'carnivore diet': 13, 'phev': 12, 'solar roof': 11, 'charging station': 10, 'home battery': 10, 'livestock emissions': 10, 'meat lover': 10, 'battery electric': 8, 'virtual power plant': 8, 'plant-based burger': 7, 'plug-in hybrid': 7, 'charging network': 6, 'fully electric': 6, 'kwh generated': 6, 'model 3': 6, 'powerwall': 6, 'precision fermentation': 6, 'steakhouse': 6, 'community solar': 5, 'rivian': 5, 'tesla model': 5, 'dairy free': 4, 'net metering': 4, 'renewable portfolio standard': 4, 'cultivated meat': 3, 'electric suv': 3, 'electric truck': 3, 'feed-in tariff': 3, 'soy burger': 3, 'veganuary': 3, '100% electric': 2, 'battery-electric vehicle': 2, 'elon says': 2, 'gigafactory': 2, 'mustang mach-e': 2, 'pure electric': 2, 'rail electrification': 2, 'supercharger': 2, 'chevy bolt': 1, 'cow burps': 1, 'energy efficiency upgrade': 1, 'last-mile delivery': 1, 'lucid air': 1, 'methane footprint': 1, 'micro-mobility': 1, 'nissan leaf': 1, 'public charger': 1, 'smart thermostat': 1, 'solar installer': 1, 'solid-state battery': 1, 'sunpower': 1, 'tofu scramble': 1, 'tofurky': 1}\n",
      "ElectricScooters: {'e-bike': 10988, 'bike lane': 5271, 'tesla': 1046, 'electric vehicle': 849, 'micro-mobility': 479, 'electric car': 463, 'range anxiety': 380, 'evs': 238, 'all-electric': 236, 'pure electric': 161, 'charging station': 135, 'model 3': 88, 'battery swap': 76, 'tesla model': 72, 'solar panels': 68, 'car-free': 49, 'supercharger': 49, 'nissan leaf': 36, 'battery storage': 34, 'solar power': 32, 'model y': 25, 'vegan': 24, 'charge point': 18, 'fully electric': 16, 'solid-state battery': 13, 'bev': 12, 'plug-in hybrid': 9, 'tax credit': 9, 'battery electric': 8, 'chevy bolt': 8, 'phev': 8, 'red meat': 8, 'dc fast charger': 7, 'powerwall': 7, 'rivian': 7, '100% electric': 4, 'ioniq 5': 4, 'electric powertrain': 3, 'home charger': 3, 'level 2 charger': 3, 'solar roof': 3, 'electric suv': 2, 'electric truck': 2, 'home battery': 2, 'last-mile delivery': 2, 'low emission zone': 2, 'public charger': 2, 'solar array': 2, 'solar inverter': 2, 'zero-emission vehicle': 2, 'battery-electric vehicle': 1, 'charging network': 1, 'electric motor vehicle': 1, 'gigafactory': 1, 'led retrofit': 1, 'meat lover': 1, 'mpge': 1, 'solar pv': 1, 'steakhouse': 1, 'veganism': 1, 'vegetarian': 1, 'zero emission vehicle': 1}\n",
      "Electricmotorcycles: {'e-bike': 811, 'tesla': 177, 'electric vehicle': 100, 'evs': 94, 'charging station': 61, 'electric car': 54, 'nissan leaf': 40, 'tax credit': 29, 'supercharger': 28, 'all-electric': 26, 'level 2 charger': 25, 'range anxiety': 22, 'bike lane': 18, 'battery swap': 16, 'model 3': 14, 'public charger': 13, 'chevy bolt': 11, 'phev': 11, 'solar panels': 11, 'dc fast charger': 10, 'tesla model': 10, 'charging network': 9, 'rivian': 9, 'charge point': 7, 'fully electric': 6, 'electric drivetrain': 5, 'battery storage': 4, 'home charger': 4, 'pure electric': 4, 'solid-state battery': 4, 'powerwall': 3, '100% electric': 2, 'bev': 2, 'electric truck': 2, 'ev rebate': 2, 'kw charger': 2, 'mpge': 2, 'battery electric': 1, 'battery-electric vehicle': 1, 'electric powertrain': 1, 'electric suv': 1, 'home battery': 1, 'id.4': 1, 'ioniq 5': 1, 'model y': 1, 'plug-in hybrid': 1, 'rooftop solar': 1, 'rooftop\\\\s+solar': 1, 'solar array': 1, 'solar power': 1, 'solar roof': 1, 'vegan': 1}\n",
      "climate: {'vegan': 4352, 'solar panels': 2595, 'evs': 2176, 'electric car': 1864, 'tesla': 1280, 'electric vehicle': 1141, 'vegetarian': 1118, 'red meat': 1010, 'plant-based diet': 951, 'veganism': 815, 'solar power': 740, 'rooftop solar': 418, 'rooftop\\\\s+solar': 418, 'tax credit': 407, 'lab-grown meat': 304, 'e-bike': 303, 'charging station': 256, 'battery storage': 233, 'factory farming': 215, 'all-electric': 183, 'solar pv': 147, 'bev': 126, 'car-free': 110, 'feedlot': 107, 'impossible burger': 97, 'beef consumption': 89, 'bike lane': 89, 'solar array': 81, 'plug-in hybrid': 71, 'phev': 65, 'fully electric': 62, 'electric truck': 61, 'grass-fed beef': 61, 'model 3': 61, 'community solar': 60, 'almond milk': 59, 'beyond meat': 59, 'oat milk': 59, 'photovoltaics': 59, 'precision fermentation': 55, 'charging network': 54, 'agricultural emissions': 52, 'rivian': 51, 'flexitarian': 49, 'solar roof': 47, 'plant protein': 45, 'seitan': 39, 'tesla model': 39, 'cow burps': 38, 'net metering': 37, 'supercharger': 37, 'nissan leaf': 36, 'battery electric': 35, 'zero-emission vehicle': 34, 'pv panel': 33, 'range anxiety': 33, 'cultivated meat': 31, 'plant-based burger': 29, 'chevy bolt': 28, 'meatless monday': 28, 'model y': 28, 'renewable portfolio standard': 25, 'solar installer': 25, 'powerwall': 24, 'battery-electric vehicle': 23, '100% electric': 22, 'carnivore diet': 20, 'livestock emissions': 20, 'nutritional yeast': 20, 'public charger': 18, 'electric suv': 17, 'home battery': 17, 'energy audit': 16, 'feed-in tariff': 15, 'micro-mobility': 15, 'mpge': 15, 'veganuary': 15, 'meat lover': 14, 'gigafactory': 13, 'sunrun': 11, 'level 2 charger': 10, 'virtual power plant': 10, 'home charger': 8, 'battery swap': 7, 'charge point': 7, 'ioniq 5': 7, 'pure electric': 7, 'zero emission vehicle': 7, 'ev rebate': 6, 'kwh generated': 6, 'steakhouse': 6, 'electric drivetrain': 5, 'green home': 5, 'smart thermostat': 5, 'low emission zone': 4, 'methane footprint': 4, 'solar tax credit': 4, 'sunpower': 4, 'elon says': 3, 'energy efficiency upgrade': 3, 'hydrogen truck': 3, 'lucid air': 3, 'solid-state battery': 3, 'dairy free': 2, 'electric motor vehicle': 2, 'id.4': 2, 'solar inverter': 2, 'tofurky': 2, 'bacon double': 1, 'carbon hoofprint': 1, 'last-mile delivery': 1, 'low carbon transport': 1, 'net-zero house': 1, 'phase-out ice': 1, 'rail electrification': 1, 'recipe vegan': 1, 'soy burger': 1, 'tofu scramble': 1, 'transit electrification': 1, 'zero-energy building': 1, 'zev mandate': 1}\n",
      "climatechange: {'solar panels': 2692, 'evs': 2021, 'electric car': 1564, 'tesla': 924, 'electric vehicle': 823, 'solar power': 641, 'vegan': 507, 'battery storage': 365, 'vegetarian': 331, 'bev': 282, 'plant-based diet': 255, 'tax credit': 171, 'e-bike': 164, 'solar pv': 160, 'charging station': 156, 'rooftop solar': 151, 'rooftop\\\\s+solar': 151, 'veganism': 137, 'all-electric': 122, 'red meat': 96, 'range anxiety': 79, 'solar array': 79, 'model 3': 67, 'tesla model': 65, 'factory farming': 64, 'photovoltaics': 64, 'pv panel': 64, 'plug-in hybrid': 60, 'bike lane': 54, 'model y': 46, 'solar roof': 42, 'car-free': 40, 'net metering': 38, 'community solar': 37, 'electric truck': 37, 'beef consumption': 36, 'phev': 35, 'rivian': 35, 'battery electric': 33, 'lab-grown meat': 33, 'powerwall': 33, 'nissan leaf': 32, 'cow burps': 29, 'fully electric': 27, 'agricultural emissions': 24, 'feedlot': 24, 'supercharger': 24, 'oat milk': 21, 'impossible burger': 20, 'grass-fed beef': 19, 'almond milk': 17, 'energy audit': 17, 'precision fermentation': 17, 'home charger': 16, '100% electric': 15, 'charging network': 14, 'meatless monday': 14, 'plant protein': 14, 'solid-state battery': 14, 'battery swap': 13, 'battery-electric vehicle': 13, 'gigafactory': 13, 'home battery': 13, 'solar installer': 13, 'chevy bolt': 12, 'virtual power plant': 12, 'livestock emissions': 11, 'flexitarian': 10, 'beyond meat': 9, 'carnivore diet': 9, 'feed-in tariff': 9, 'kwh generated': 9, 'pure electric': 7, 'zero-emission vehicle': 7, 'ioniq 5': 6, 'mpge': 6, 'plant-based burger': 6, 'id.4': 5, 'level 2 charger': 5, 'net-zero house': 5, 'smart thermostat': 5, 'electric suv': 4, 'micro-mobility': 4, 'public charger': 4, 'zero emission vehicle': 4, 'dc fast charger': 3, 'energy efficiency upgrade': 3, 'green home': 3, 'methane footprint': 3, 'renewable portfolio standard': 3, 'seitan': 3, 'sunpower': 3, 'sunrun': 3, 'charge point': 2, 'electric drivetrain': 2, 'elon says': 2, 'ev rebate': 2, 'hydrogen truck': 2, 'kw charger': 2, 'low carbon transport': 2, 'mustang mach-e': 2, 'nutritional yeast': 2, 'solar tax credit': 2, 'veganuary': 2, 'cultivated meat': 1, 'last-mile delivery': 1, 'low emission zone': 1, 'lucid air': 1, 'meat lover': 1, 'solar inverter': 1, 'soy burger': 1, 'transit electrification': 1, 'zero-energy building': 1}\n",
      "climatedisalarm: {'electric vehicle': 1556, 'electric car': 1325, 'tesla': 1189, 'evs': 1060, 'solar panels': 487, 'charging station': 270, 'all-electric': 232, 'solar power': 218, 'tesla model': 146, 'vegan': 88, 'electric truck': 86, 'tax credit': 83, 'model 3': 81, 'plug-in hybrid': 76, 'zero-emission vehicle': 72, 'battery electric': 64, 'fully electric': 57, 'bev': 56, 'battery-electric vehicle': 51, 'range anxiety': 51, 'rivian': 49, 'battery storage': 48, 'model y': 46, 'red meat': 44, 'mustang mach-e': 43, 'chevy bolt': 34, 'charging network': 32, 'phev': 32, 'solar array': 32, 'supercharger': 31, 'nissan leaf': 26, 'e-bike': 25, 'vegetarian': 20, 'electric suv': 16, 'lab-grown meat': 16, 'beyond meat': 15, 'plant-based diet': 14, 'rooftop solar': 14, 'rooftop\\\\s+solar': 14, 'zero emission vehicle': 13, 'public charger': 11, '100% electric': 10, 'kw charger': 10, 'gigafactory': 9, 'low emission zone': 9, 'lucid air': 9, 'bike lane': 8, 'charge point': 8, 'home charger': 8, 'level 2 charger': 8, 'pure electric': 8, 'car-free': 7, 'veganism': 7, 'dc fast charger': 6, 'plant-based burger': 6, 'solar pv': 6, 'electric powertrain': 5, 'cow burps': 4, 'ev rebate': 4, 'micro-mobility': 4, 'powerwall': 4, 'solid-state battery': 4, 'agricultural emissions': 3, 'cultivated meat': 3, 'green home': 3, 'id.4': 3, 'photovoltaics': 3, 'pv panel': 3, 'beef consumption': 2, 'community solar': 2, 'electric drivetrain': 2, 'home battery': 2, 'impossible burger': 2, 'ioniq 5': 2, 'meatless monday': 2, 'mpge': 2, 'carbon hoofprint': 1, 'carnivore diet': 1, 'dairy free': 1, 'electric motor vehicle': 1, 'elon says': 1, 'feedlot': 1, 'net metering': 1, 'renewable portfolio standard': 1, 'solar roof': 1}\n",
      "climateskeptics: {'solar panels': 2827, 'evs': 2370, 'electric car': 2364, 'tesla': 2038, 'vegan': 1153, 'electric vehicle': 972, 'solar power': 893, 'vegetarian': 356, 'tax credit': 349, 'red meat': 309, 'charging station': 288, 'solar pv': 249, 'battery storage': 206, 'all-electric': 202, 'veganism': 154, 'rooftop solar': 140, 'rooftop\\\\s+solar': 140, 'model 3': 114, 'tesla model': 111, 'solar array': 105, 'nissan leaf': 95, 'pv panel': 88, 'bev': 85, 'plug-in hybrid': 78, 'lab-grown meat': 67, 'rivian': 64, 'plant-based diet': 60, 'feed-in tariff': 59, 'e-bike': 58, 'factory farming': 58, 'photovoltaics': 58, 'electric truck': 52, 'battery electric': 45, 'supercharger': 45, 'phev': 40, 'cow burps': 39, 'range anxiety': 39, 'powerwall': 28, 'bike lane': 27, 'beyond meat': 26, 'feedlot': 25, 'gigafactory': 25, 'model y': 25, 'fully electric': 24, 'net metering': 22, 'zero-emission vehicle': 21, 'car-free': 19, 'grass-fed beef': 19, 'solar roof': 19, 'chevy bolt': 17, 'plant protein': 17, 'battery swap': 16, 'charging network': 16, '100% electric': 15, 'battery-electric vehicle': 15, 'home battery': 15, 'home charger': 15, 'pure electric': 13, 'low emission zone': 12, 'meatless monday': 12, 'agricultural emissions': 11, 'electric drivetrain': 11, 'impossible burger': 11, 'kwh generated': 11, 'carnivore diet': 10, 'beef consumption': 9, 'charge point': 9, 'level 2 charger': 9, 'mpge': 9, 'public charger': 9, 'kw charger': 8, 'solar installer': 8, 'zero emission vehicle': 8, 'dc fast charger': 7, 'electric suv': 7, 'renewable portfolio standard': 7, 'community solar': 6, 'oat milk': 6, 'almond milk': 5, 'elon says': 5, 'mustang mach-e': 5, 'plant-based burger': 5, 'smart thermostat': 5, 'soy burger': 5, 'energy audit': 4, 'livestock emissions': 4, 'solar tax credit': 4, 'sunpower': 4, 'ev rebate': 3, 'precision fermentation': 3, 'solar inverter': 3, 'solid-state battery': 3, 'steakhouse': 3, 'bacon double': 2, 'electric powertrain': 2, 'ioniq 5': 2, 'micro-mobility': 2, 'veganuary': 2, 'virtual power plant': 2, 'cultivated meat': 1, 'dairy free': 1, 'last-mile delivery': 1, 'low carbon transport': 1, 'lucid air': 1, 'meat lover': 1, 'methane footprint': 1, 'phase-out ice': 1, 'pvgis': 1, 'seitan': 1, 'sunrun': 1, 'zev mandate': 1}\n",
      "electricvehicles: {'tesla': 417572, 'evs': 219003, 'model 3': 85332, 'bev': 54200, 'phev': 53668, 'supercharger': 42451, 'model y': 39210, 'rivian': 38649, 'electric car': 35804, 'tax credit': 30601, 'electric vehicle': 29907, 'charging station': 27434, 'ioniq 5': 25665, 'tesla model': 21509, 'id.4': 19778, 'charging network': 17670, 'nissan leaf': 12670, 'chevy bolt': 10148, 'plug-in hybrid': 8795, 'solar panels': 8785, 'all-electric': 8064, 'public charger': 7058, 'level 2 charger': 6960, 'range anxiety': 6710, 'e-bike': 5648, 'home charger': 4808, 'dc fast charger': 4452, 'electric truck': 4158, 'mustang mach-e': 3684, 'lucid air': 3239, 'fully electric': 3181, 'mpge': 2682, 'charge point': 2591, 'battery swap': 2348, 'kw charger': 2301, 'electric suv': 2212, 'battery electric': 2123, 'powerwall': 1835, 'gigafactory': 1790, 'solid-state battery': 1760, 'solar power': 1574, 'battery storage': 1506, 'pure electric': 1406, 'battery-electric vehicle': 1139, 'electric drivetrain': 1095, 'ev rebate': 998, 'solar roof': 952, 'solar array': 898, 'vegan': 826, 'bike lane': 806, 'home battery': 793, '100% electric': 784, 'rooftop solar': 779, 'rooftop\\\\s+solar': 779, 'net metering': 719, 'electric powertrain': 584, 'zero-emission vehicle': 520, 'solar pv': 440, 'sunrun': 359, 'elon says': 328, 'zero emission vehicle': 266, 'zev mandate': 237, 'hydrogen truck': 223, 'micro-mobility': 215, 'car-free': 211, 'virtual power plant': 198, 'pv panel': 168, 'last-mile delivery': 149, 'solar inverter': 135, 'vegetarian': 128, 'solar installer': 108, 'red meat': 88, 'community solar': 69, 'phase-out ice': 67, 'photovoltaics': 62, 'sunpower': 55, 'smart thermostat': 50, 'gigacast': 48, 'low emission zone': 48, 'solar tax credit': 47, 'kwh generated': 41, 'feed-in tariff': 40, 'veganism': 30, 'lab-grown meat': 19, 'electric motor vehicle': 16, 'energy audit': 16, 'plant-based diet': 16, 'steakhouse': 15, 'impossible burger': 14, 'rail electrification': 12, 'beyond meat': 10, 'almond milk': 9, 'factory farming': 9, 'energy efficiency upgrade': 8, 'feedlot': 8, 'beef consumption': 7, 'oat milk': 7, 'green home': 6, 'transit electrification': 5, 'agricultural emissions': 4, 'led retrofit': 4, 'net-zero house': 4, 'precision fermentation': 4, 'renewable portfolio standard': 4, 'flexitarian': 3, 'meatless monday': 3, 'animal cruelty free': 2, 'carnivore diet': 2, 'cow burps': 2, 'plant protein': 2, 'plant-based burger': 2, 'pvgis': 2, 'seitan': 2, 'grass-fed beef': 1, 'meat lover': 1, 'methane footprint': 1}\n",
      "solar: {'tesla': 26947, 'solar panels': 23430, 'tax credit': 17415, 'net metering': 16100, 'powerwall': 10928, 'sunpower': 10458, 'sunrun': 7662, 'solar power': 6454, 'solar installer': 5205, 'solar array': 3361, 'battery storage': 2866, 'rooftop\\\\s+solar': 2799, 'rooftop solar': 2797, 'evs': 2738, 'electric car': 1939, 'solar roof': 1914, 'solar pv': 1808, 'solar inverter': 1238, 'community solar': 1223, 'pv panel': 1158, 'solar tax credit': 1068, 'electric vehicle': 876, 'all-electric': 800, 'home battery': 718, 'model 3': 539, 'photovoltaics': 456, 'energy audit': 376, 'feed-in tariff': 357, 'charging station': 342, 'nissan leaf': 313, 'virtual power plant': 285, 'phev': 267, 'tesla model': 231, 'kwh generated': 213, 'smart thermostat': 201, 'model y': 182, 'bev': 167, 'plug-in hybrid': 161, 'e-bike': 160, 'rivian': 141, 'chevy bolt': 130, '100% electric': 115, 'gigafactory': 115, 'level 2 charger': 113, 'supercharger': 109, 'pvgis': 102, 'energy efficiency upgrade': 98, 'ioniq 5': 93, 'fully electric': 77, 'electric truck': 42, 'home charger': 38, 'renewable portfolio standard': 37, 'charge point': 32, 'green home': 32, 'dc fast charger': 24, 'id.4': 24, 'public charger': 24, 'pure electric': 22, 'battery electric': 19, 'charging network': 15, 'vegan': 15, 'battery swap': 11, 'net-zero house': 11, 'red meat': 11, 'solid-state battery': 11, 'battery-electric vehicle': 10, 'range anxiety': 10, 'vegetarian': 10, 'bike lane': 9, 'mustang mach-e': 8, 'lucid air': 7, 'mpge': 7, 'elon says': 6, 'electric suv': 5, 'ev rebate': 4, 'impossible burger': 4, 'feedlot': 3, 'steakhouse': 3, 'zero-energy building': 3, 'car-free': 2, 'hydrogen truck': 2, 'last-mile delivery': 2, 'veganism': 2, 'barbecue festival': 1, 'cow burps': 1, 'electric powertrain': 1, 'kw charger': 1, 'lab-grown meat': 1, 'methane footprint': 1, 'micro-mobility': 1, 'plant-based diet': 1, 'zero-emission vehicle': 1, 'zev mandate': 1}\n",
      "vegan: {'vegan': 2186331, 'veganism': 379977, 'vegetarian': 203893, 'plant-based diet': 55418, 'red meat': 46278, 'seitan': 34634, 'almond milk': 27620, 'nutritional yeast': 22772, 'oat milk': 21049, 'factory farming': 20395, 'beyond meat': 12863, 'lab-grown meat': 12185, 'impossible burger': 10790, 'tofu scramble': 9654, 'dairy free': 6828, 'veganuary': 5132, 'flexitarian': 4778, 'tofurky': 4592, 'meatless monday': 4104, 'plant protein': 3987, 'carnivore diet': 2867, 'steakhouse': 2168, 'plant-based burger': 1653, 'grass-fed beef': 1572, 'tesla': 1541, 'electric car': 1375, 'meat lover': 1199, 'feedlot': 1110, 'solar panels': 573, 'cultivated meat': 505, 'precision fermentation': 366, 'soy burger': 352, 'electric vehicle': 289, 'evs': 282, 'beef consumption': 280, 'rivian': 227, 'car-free': 217, 'solar power': 176, 'animal cruelty free': 137, 'model 3': 129, 'recipe vegan': 99, 'bike lane': 92, 'bacon double': 67, 'tax credit': 64, 'agricultural emissions': 61, 'veg lifestyle': 57, 'tesla model': 52, 'e-bike': 45, 'livestock emissions': 45, 'charging station': 39, 'cow burps': 37, 'bev': 36, 'all-electric': 32, 'nissan leaf': 27, 'plug-in hybrid': 25, 'phev': 18, 'fully electric': 16, 'chevy bolt': 15, '100% electric': 12, 'methane footprint': 12, 'mustang mach-e': 10, 'model y': 9, 'range anxiety': 9, 'rooftop solar': 9, 'rooftop\\\\s+solar': 9, 'battery storage': 8, 'solar pv': 8, 'solar roof': 8, 'supercharger': 8, 'electric truck': 6, 'gigafactory': 6, 'ioniq 5': 6, 'photovoltaics': 6, 'id.4': 4, 'micro-mobility': 4, 'solar array': 4, 'charging network': 3, 'electric suv': 3, 'feed-in tariff': 3, 'pv panel': 3, 'energy audit': 2, 'powerwall': 2, 'pure electric': 2, 'smart thermostat': 2, 'barbecue festival': 1, 'battery electric': 1, 'community solar': 1, 'elon says': 1, 'ev rebate': 1, 'green home': 1, 'home battery': 1, 'last-mile delivery': 1, 'level 2 charger': 1, 'lucid air': 1, 'mpge': 1, 'solar installer': 1, 'sunrun': 1, 'zero-emission vehicle': 1}\n",
      "veganarchism: {'vegan': 6239, 'veganism': 2195, 'vegetarian': 474, 'plant-based diet': 135, 'red meat': 127, 'factory farming': 96, 'lab-grown meat': 38, 'oat milk': 24, 'seitan': 21, 'meatless monday': 19, 'beyond meat': 17, 'carnivore diet': 17, 'almond milk': 16, 'impossible burger': 16, 'veganuary': 12, 'dairy free': 11, 'tofu scramble': 11, 'plant protein': 10, 'flexitarian': 7, 'nutritional yeast': 7, 'tesla': 7, 'electric car': 6, 'solar panels': 5, 'tofurky': 5, 'grass-fed beef': 4, 'plant-based burger': 3, 'steakhouse': 3, 'car-free': 2, 'e-bike': 2, 'evs': 2, 'cultivated meat': 1, 'electric vehicle': 1, 'feedlot': 1, 'meat lover': 1}\n",
      "vegancirclejerk: {'vegan': 36429, 'vegetarian': 4864, 'veganism': 4137, 'red meat': 442, 'seitan': 308, 'flexitarian': 252, 'plant-based diet': 225, 'meatless monday': 222, 'almond milk': 199, 'lab-grown meat': 184, 'factory farming': 158, 'nutritional yeast': 138, 'impossible burger': 95, 'oat milk': 55, 'tofurky': 50, 'beyond meat': 42, 'grass-fed beef': 40, 'steakhouse': 38, 'dairy free': 29, 'plant protein': 28, 'meat lover': 25, 'tesla': 24, 'carnivore diet': 23, 'tofu scramble': 22, 'veganuary': 17, 'electric car': 10, 'rivian': 10, 'feedlot': 9, 'beef consumption': 6, 'solar panels': 6, 'bacon double': 4, 'soy burger': 4, 'plant-based burger': 3, 'electric vehicle': 2, 'evs': 2, 'fully electric': 2, 'solar power': 2, 'recipe vegan': 1, 'tax credit': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- Recompute filtered_csv_paths only ---\n",
    "import os\n",
    "\n",
    "output_dir = os.path.join('paper4data', 'subreddit_filtered_by_regex')\n",
    "filtered_csv_paths = [\n",
    "    os.path.join(output_dir, f)\n",
    "    for f in os.listdir(output_dir)\n",
    "    if f.lower().endswith('.csv')\n",
    "]\n",
    "# --- Load all filtered CSVs and reconstruct variables as before ---\n",
    "all_filtered = []\n",
    "for path in filtered_csv_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        all_filtered.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading filtered csv {path}: {e}\")\n",
    "\n",
    "if all_filtered:\n",
    "    df_all_filtered = pd.concat(all_filtered, ignore_index=True)\n",
    "else:\n",
    "    df_all_filtered = pd.DataFrame()\n",
    "\n",
    "# Fill empty/null body with title\n",
    "if not df_all_filtered.empty:\n",
    "    # Check if both columns exist\n",
    "    if 'body' in df_all_filtered.columns and 'title' in df_all_filtered.columns:\n",
    "        # Count how many rows need filling\n",
    "        mask = df_all_filtered['body'].isna() | (df_all_filtered['body'] == '') | (df_all_filtered['body'].astype(str).str.strip() == '')\n",
    "        count_to_fill = mask.sum()\n",
    "        \n",
    "        # Fill empty/null body with title\n",
    "        df_all_filtered.loc[mask, 'body'] = df_all_filtered.loc[mask, 'title']\n",
    "        \n",
    "        if count_to_fill > 0:\n",
    "            print(f\"Filled {count_to_fill} empty/null body values with title\")\n",
    "\n",
    "# Reconstruct keyword_to_comments (id set for each keyword)\n",
    "keyword_to_comments = {}\n",
    "if not df_all_filtered.empty:\n",
    "    for kw, group in df_all_filtered.groupby('matched_keyword'):\n",
    "        keyword_to_comments[kw] = set(group['id'].astype(str))\n",
    "\n",
    "\n",
    "# Reconstruct subreddit_keyword_stats from keyword_to_comments and df_all_filtered (assume both in memory)\n",
    "# Print overall stats using DataFrame operations\n",
    "if not df_all_filtered.empty:\n",
    "    # Get keyword counts using value_counts\n",
    "    keyword_counts = df_all_filtered.groupby('matched_keyword')['id'].nunique().sort_values(ascending=False)\n",
    "    print(\"Total unique comments/submissions matched per keyword:\")\n",
    "    for kw, count in keyword_counts.items():\n",
    "        print(f\"{kw}: {count}\")\n",
    "    \n",
    "    # Get subreddit x keyword stats using groupby\n",
    "    print(\"\\nSubreddit x keyword stats (number of unique ids per keyword per subreddit):\")\n",
    "    subreddit_keyword_counts = df_all_filtered.groupby(['subreddit', 'matched_keyword'])['id'].nunique().unstack(fill_value=0)\n",
    "    for subreddit in subreddit_keyword_counts.index:\n",
    "        kw_dict = {kw: count for kw, count in subreddit_keyword_counts.loc[subreddit].items() if count > 0}\n",
    "        # Sort by count descending\n",
    "        kw_dict_sorted = dict(sorted(kw_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        print(f\"{subreddit}: {kw_dict_sorted}\")\n",
    "else:\n",
    "    print(\"No filtered data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dc6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'sector' column to df_all_filtered\n",
      "Sector distribution:\n",
      "sector\n",
      "food         3177356\n",
      "transport    1394862\n",
      "housing       144775\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create sector mapping from matched_keyword\n",
    "# Map each keyword to its sector (ignoring _strong/_weak distinction)\n",
    "\n",
    "# Create reverse mapping: keyword -> sector\n",
    "keyword_to_sector = {}\n",
    "\n",
    "# Map transport keywords\n",
    "for kw in sector_keyword_strength['transport_strong'] + sector_keyword_strength['transport_weak']:\n",
    "    keyword_to_sector[kw] = 'transport'\n",
    "\n",
    "# Map housing keywords\n",
    "for kw in sector_keyword_strength['housing_strong'] + sector_keyword_strength['housing_weak']:\n",
    "    keyword_to_sector[kw] = 'housing'\n",
    "\n",
    "# Map food keywords\n",
    "for kw in sector_keyword_strength['food_strong'] + sector_keyword_strength['food_weak']:\n",
    "    keyword_to_sector[kw] = 'food'\n",
    "\n",
    "# Add sector column to df_all_filtered based on matched_keyword\n",
    "if not df_all_filtered.empty and 'matched_keyword' in df_all_filtered.columns:\n",
    "    df_all_filtered['sector'] = df_all_filtered['matched_keyword'].map(keyword_to_sector)\n",
    "    print(f\"Added 'sector' column to df_all_filtered\")\n",
    "    print(f\"Sector distribution:\")\n",
    "    print(df_all_filtered['sector'].value_counts())\n",
    "else:\n",
    "    print(\"Warning: df_all_filtered is empty or missing 'matched_keyword' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9346e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 319862 empty/null body values with title\n"
     ]
    }
   ],
   "source": [
    "if 'body' in df_all_filtered.columns and 'title' in df_all_filtered.columns:\n",
    "    # Count how many rows need filling\n",
    "    mask = df_all_filtered['body'].isna() | (df_all_filtered['body'] == '') | (df_all_filtered['body'].astype(str).str.strip() == '')\n",
    "    count_to_fill = mask.sum()\n",
    "    \n",
    "    # Fill empty/null body with title\n",
    "    df_all_filtered.loc[mask, 'body'] = df_all_filtered.loc[mask, 'title']\n",
    "    \n",
    "    if count_to_fill > 0:\n",
    "        print(f\"Filled {count_to_fill} empty/null body values with title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c15a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          housing\n",
       "1             food\n",
       "2             food\n",
       "3             food\n",
       "4             food\n",
       "            ...   \n",
       "4716988       food\n",
       "4716989       food\n",
       "4716990       food\n",
       "4716991       food\n",
       "4716992       food\n",
       "Name: sector, Length: 4716993, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_filtered.sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d027866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5d7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fce500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First time run - process all keywords\n",
    "# all_comments = df_topics_all['Document'].tolist()\n",
    "# sector_candidates, keyword_comment_map = filter_sector_comments(all_comments, sector_keywords)\n",
    "# pd.to_pickle(sector_candidates, results_file)\n",
    "# pd.to_pickle(keyword_comment_map, keyword_map_file)\n",
    "# pd.to_pickle(set(sector_keywords), keyword_list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e3589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Wordmaps for each sector (all in one figure)\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 12))\n",
    "\n",
    "def get_sequential_palette_for_sector(sector, n_labels):\n",
    "    sector_cmaps = {\n",
    "        'transport': 'Reds',\n",
    "        'housing': 'Oranges',\n",
    "        'food': 'Blues'\n",
    "    }\n",
    "    cmap_name = sector_cmaps.get(sector, 'Greys')\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    colors = [cmap(x) for x in np.linspace(0.4, 2, n_labels)][::-1]\n",
    "    return colors\n",
    "\n",
    "def scale_font_sizes(freq_dict, min_font=8, max_font=50):\n",
    "    \"\"\"\n",
    "    Scale frequencies to font sizes between min_font and max_font.\n",
    "    All words are included, even those with lowest frequency.\n",
    "    \"\"\"\n",
    "    if not freq_dict:\n",
    "        return {}\n",
    "    values = list(freq_dict.values())\n",
    "    min_freq = min(values)\n",
    "    max_freq = max(values)\n",
    "    # Avoid division by zero if all frequencies are the same\n",
    "    if max_freq == min_freq:\n",
    "        return {k: (min_font + max_font) // 2 for k in freq_dict}\n",
    "    scaled = {}\n",
    "    for k, v in freq_dict.items():\n",
    "        # Linear scaling\n",
    "        size = min_font + (v - min_freq) * (max_font - min_font) / (max_freq - min_freq)\n",
    "        scaled[k] = size\n",
    "    return scaled\n",
    "\n",
    "def mpl_color_to_pil(color):\n",
    "    \"\"\"\n",
    "    Convert a matplotlib RGBA color (floats 0-1) to a PIL-compatible RGB or RGBA tuple (ints 0-255).\n",
    "    \"\"\"\n",
    "    if isinstance(color, tuple):\n",
    "        if len(color) == 4:\n",
    "            return tuple(int(round(255 * c)) for c in color[:3])  # ignore alpha for PIL\n",
    "        elif len(color) == 3:\n",
    "            return tuple(int(round(255 * c)) for c in color)\n",
    "    # fallback to black\n",
    "    return (0, 0, 0)\n",
    "\n",
    "# Create sector mapping from sector_keyword_strength\n",
    "sector_keywords_map = {\n",
    "    'transport': sector_keyword_strength['transport_strong'] + sector_keyword_strength['transport_weak'],\n",
    "    'housing': sector_keyword_strength['housing_strong'] + sector_keyword_strength['housing_weak'],\n",
    "    'food': sector_keyword_strength['food_strong'] + sector_keyword_strength['food_weak']\n",
    "}\n",
    "sectors_list = ['transport', 'housing', 'food']\n",
    "\n",
    "for idx, sector in enumerate(sectors_list):\n",
    "    # Get keywords for this sector from sector_keywords_map\n",
    "    sector_keywords = sector_keywords_map[sector]\n",
    "    \n",
    "    if len(sector_keywords) == 0:\n",
    "        # No keywords for this sector\n",
    "        axes[idx].text(0.5, 0.5, f'No keywords found for {sector}', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].axis('off')\n",
    "        continue\n",
    "    \n",
    "    # Filter keyword_counts to only include keywords from this sector\n",
    "    sector_counts = keyword_counts[keyword_counts.index.isin(sector_keywords)]\n",
    "    \n",
    "    # Filter out zero counts\n",
    "    non_zero_keywords = sector_counts[sector_counts > 0]\n",
    "    \n",
    "    if len(non_zero_keywords) == 0:\n",
    "        # No non-zero keywords for this sector\n",
    "        axes[idx].text(0.5, 0.5, f'No non-zero keywords found for {sector}', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].axis('off')\n",
    "        continue\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_keywords = dict(sorted(non_zero_keywords.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Prepare wordcloud frequencies, format as \"word (n=xx)\"\n",
    "    wc_freq = {f\"{k} (n={v})\": v for k, v in sorted_keywords.items() if v > 0}\n",
    "    \n",
    "    # Scale font sizes for all words\n",
    "    font_sizes = scale_font_sizes(wc_freq, min_font=8, max_font=40)\n",
    "    \n",
    "    # Use a color map for each sector using the helper function\n",
    "    cmap_colors = get_sequential_palette_for_sector(sector, len(wc_freq))\n",
    "    \n",
    "    # Assign a color to each word based on its rank, and convert to PIL-compatible\n",
    "    word_color_map = {}\n",
    "    for i, word in enumerate(wc_freq.keys()):\n",
    "        word_color_map[word] = mpl_color_to_pil(cmap_colors[i % len(cmap_colors)])\n",
    "    \n",
    "    # Custom color function to use the mapped color\n",
    "    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return word_color_map.get(word, (0, 0, 0))\n",
    "\n",
    "    # Generate wordcloud\n",
    "    wc = WordCloud(width=800, height=200, background_color='white',\n",
    "                   color_func=color_func, prefer_horizontal=.6, \n",
    "                   collocations=False)\n",
    "    wc.generate_from_frequencies(wc_freq)\n",
    "\n",
    "    # Now, update font sizes in layout_ directly\n",
    "    new_layout = []\n",
    "    for (word, count), font_size, position, orientation, color in wc.layout_:\n",
    "        # Use our scaled font size\n",
    "        scaled_size = font_sizes.get(word, 8)\n",
    "        new_layout.append(((word, count), scaled_size, position, orientation, color))\n",
    "    wc.layout_ = new_layout\n",
    "\n",
    "    # Draw wordcloud\n",
    "    axes[idx].imshow(wc.to_array(), interpolation='bilinear')\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"{sector.capitalize()} (n={len(non_zero_keywords)} keywords)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('paper4figs/fig1_sector_wordmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('paper4figs/fig1_sector_wordmaps.pdf', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c53083",
   "metadata": {},
   "source": [
    "# FasTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e468d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42356df2",
   "metadata": {},
   "source": [
    "# plot wordmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b12bf9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d07d1d9",
   "metadata": {},
   "source": [
    "# GPT based training , validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "import paper4_GPT_arguments\n",
    "import json\n",
    "from importlib import reload\n",
    "from paper4_GPT_arguments import main, process_single_sector\n",
    "\n",
    "# Reload the module\n",
    "reload(paper4_GPT_arguments)\n",
    "\n",
    "# Define ranges for validation data\n",
    "ranges = [\n",
    "    (4000, 4150),\n",
    "    (4150, 4300)\n",
    "]\n",
    "\n",
    "# Process each validation range\n",
    "for i, (start, end) in enumerate(ranges, start=1):\n",
    "    # Sample comments from each sector\n",
    "    sampled_comments = {\n",
    "        sector: list(comments)[start:end] if len(comments) > end else list(comments)\n",
    "        for sector, comments in comments_by_sector.items()\n",
    "    }\n",
    "\n",
    "    # Process with parallel agents\n",
    "    results, results_df, sector_labels = main(\n",
    "        comments_by_sector=sampled_comments,\n",
    "        num_runs=1,\n",
    "        max_comments_per_sector=1000,\n",
    "        num_agents=3,\n",
    "        batch_size=1,\n",
    "        use_survey_frames=False\n",
    "    )\n",
    "\n",
    "    # Create validation data dictionary\n",
    "    validation_data = {}\n",
    "    for sector in sector_labels:\n",
    "        sector_df = results_df[results_df['sector'] == sector]\n",
    "        validation_data[sector] = {}\n",
    "        \n",
    "        for idx, row in sector_df.iterrows():\n",
    "            comment = row['comment']\n",
    "            labels = []\n",
    "            for label in sector_labels[sector]:\n",
    "                col = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "                if row[col] > 0:\n",
    "                    labels.append(label)\n",
    "            if labels:  # Only add if comment has labels\n",
    "                validation_data[sector][comment] = labels\n",
    "\n",
    "    # Save results with index as validation data\n",
    "    results_filename = f\"paper4data/validation_data_by_GPT_results_{i}.json\"\n",
    "    validation_data_filename = f\"paper4data/validation_data_by_GPT_dict_{i}.json\"\n",
    "\n",
    "    with open(results_filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "        \n",
    "    with open(validation_data_filename, 'w') as f:\n",
    "        json.dump(validation_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7df343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Print overall statistics first\n",
    "# print(\"=== Overall Statistics ===\")\n",
    "# for sector in sector_labels.keys():\n",
    "#     sector_df = results_df[results_df['sector'] == sector].copy()\n",
    "#     total_comments = len(sector_df)\n",
    "#     print(f\"\\n{sector.upper()} SECTOR:\")\n",
    "#     print(f\"Total comments: {total_comments}\")\n",
    "    \n",
    "#     # Count unlabeled comments (including newly added ones)\n",
    "#     no_labels = sector_df.copy()\n",
    "#     try:\n",
    "#         for label in sector_labels[sector]:\n",
    "#             col_name = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "#             no_labels = no_labels[no_labels[col_name] == 0]\n",
    "#         unlabeled_count = len(no_labels)\n",
    "#         print(f\"Unlabeled comments: {unlabeled_count}\")\n",
    "        \n",
    "#         # Count comments per label\n",
    "#         print(\"Comments per label:\")\n",
    "#         for label in sector_labels[sector]:\n",
    "#             label_col = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "#             labeled_count = len(sector_df[sector_df[label_col] > 0])\n",
    "#             print(f\"- {label}: {labeled_count}\")\n",
    "#     except KeyError as e:\n",
    "#         print(f\"Warning: Could not process labels for sector {sector}. Missing column: {str(e)}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Create single output file for all sectors\n",
    "# with open(f'paper4data/gpt3_check_all_sectors.txt', 'w', encoding='utf-8') as f:\n",
    "#     # Add meta description at top\n",
    "#     f.write(\"=\" * 80 + \"\\n\")\n",
    "#     f.write(\"FILE CONTENTS DESCRIPTION:\\n\")\n",
    "#     f.write(\"This file contains all comments from all sectors with their applied labels\\n\")\n",
    "#     f.write(\"Format: [Label1, Label2, ...] Comment text\\n\")\n",
    "#     f.write(\"Comments longer than 4000 characters are truncated with '...'\\n\")\n",
    "#     f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "#     # Process each sector\n",
    "#     for sector in sector_labels.keys():\n",
    "#         f.write(f\"\\n{sector.upper()} SECTOR COMMENTS:\\n\")\n",
    "#         f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "#         sector_df = results_df[results_df['sector'] == sector].copy()\n",
    "        \n",
    "#         # Get all comments for this sector\n",
    "#         for _, row in sector_df.iterrows():\n",
    "#             try:\n",
    "#                 # Collect all labels that apply to this comment\n",
    "#                 applied_labels = []\n",
    "#                 for label in sector_labels[sector]:\n",
    "#                     col_name = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "#                     if row[col_name] > 0:\n",
    "#                         applied_labels.append(label)\n",
    "                \n",
    "#                 # Format comment text\n",
    "#                 comment_text = row['comment'][:4000] + \"...\" if len(row['comment']) > 4000 else row['comment']\n",
    "                \n",
    "#                 # Write comment with all its labels in one bracket\n",
    "#                 label_text = \", \".join(applied_labels) if applied_labels else \"NO LABELS\"\n",
    "#                 f.write(f\"[{label_text}] {comment_text}\\n\\n\")\n",
    "#             except KeyError as e:\n",
    "#                 print(f\"Warning: Could not process comment in sector {sector}. Missing column: {str(e)}\")\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535b4ac",
   "metadata": {},
   "source": [
    "# finetune Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python paper4_BERT_finetuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a203d",
   "metadata": {},
   "source": [
    "## optimal thesholds: probably overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ef3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python paper4_multilabel_threshold_optimizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41181716",
   "metadata": {},
   "source": [
    "# Dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing label co-occurrences for transport sector:\n",
      "Saved matrices to paper4data/eval_class_overlaps_transport_*.xlsx\n",
      "\n",
      "Analyzing label co-occurrences for housing sector:\n",
      "Saved matrices to paper4data/eval_class_overlaps_housing_*.xlsx\n",
      "\n",
      "Analyzing label co-occurrences for food sector:\n",
      "Saved matrices to paper4data/eval_class_overlaps_food_*.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def label_cooccurrence_matrix(all_true_labels, label_names):\n",
    "#     \"\"\"\n",
    "#     all_true_labels: list of binary lists (len = num_labels)\n",
    "#     label_names: list of labels, len = num_labels\n",
    "#     returns: \n",
    "#       - co_occur: DataFrame count of how often i&j both = 1\n",
    "#       - jaccard: Jaccard similarity matrix\n",
    "#       - cond_prob: P(j|i) = co_occur[i,j] / total_i\n",
    "#     \"\"\"\n",
    "#     Y = np.array(all_true_labels)  # shape (N, L)\n",
    "#     L = Y.shape[1]\n",
    "#     co = np.dot(Y.T, Y)            # co[i,j] = #samples where both label i and j are 1\n",
    "#     total = Y.sum(axis=0)          # total[i] = #samples where label i is 1\n",
    "\n",
    "#     # build DataFrames\n",
    "#     co_occur = pd.DataFrame(co, index=label_names, columns=label_names)\n",
    "    \n",
    "#     # Calculate Jaccard similarity\n",
    "#     # For i=j, intersection (co_occur) equals union (total), so jaccard = 1\n",
    "#     # For ij, jaccard = intersection / union = co_occur / (total_i + total_j - co_occur)\n",
    "#     union = total[:,None] + total[None,:] - co_occur\n",
    "#     jaccard = co_occur / union\n",
    "    \n",
    "#     # Calculate conditional probability, avoiding division by zero\n",
    "#     total_clipped = np.maximum(total[:,None], 1)  # Ensure denominator is at least 1\n",
    "#     cond_prob = co_occur / total_clipped   # P(j|i)\n",
    "    \n",
    "#     return co_occur, jaccard, cond_prob\n",
    "\n",
    "# # Calculate and save co-occurrence matrices for each sector\n",
    "# for sector, model_info in all_sector_models.items():\n",
    "#     print(f\"\\nAnalyzing label co-occurrences for {sector} sector:\")\n",
    "    \n",
    "#     # Get true labels and label names for this sector\n",
    "#     sector_data = all_data[sector]\n",
    "#     label_names = model_info['label_names']\n",
    "    \n",
    "#     # Convert true labels to binary format\n",
    "#     true_labels_list = []\n",
    "#     for true_labels in sector_data.values():\n",
    "#         binary_labels = [1 if label in true_labels else 0 for label in label_names]\n",
    "#         true_labels_list.append(binary_labels)\n",
    "    \n",
    "#     # Calculate matrices\n",
    "#     co_occur, jaccard, cond_prob = label_cooccurrence_matrix(true_labels_list, label_names)\n",
    "    \n",
    "#     # Save matrices to Excel files\n",
    "#     jaccard.to_excel(f'paper4data/eval_class_overlaps_{sector}_jaccard.xlsx')\n",
    "#     cond_prob.to_excel(f'paper4data/eval_class_overlaps_{sector}_condprob.xlsx')\n",
    "    \n",
    "#     print(f\"Saved matrices to paper4data/eval_class_overlaps_{sector}_*.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda359d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing label relationships across sectors...\n",
      "\n",
      "--- TRANSPORT SECTOR ---\n",
      "Saved lift matrix: paper4data/eval_class_overlaps_transport_lift.xlsx\n",
      "Saved clustering plot: paper4data/clustering_transport_lift.png\n",
      "\n",
      "Strongly associated label pairs (lift >= 2):\n",
      "Depreciation And Resale - Second Hand Market: 12.50\n",
      "Battery Cost - Reliability: 8.93\n",
      "Depreciation And Resale - Purchase Price: 5.62\n",
      "Equity And Accessibility - Second Hand Market: 5.51\n",
      "Battery Recycling And End Of Life - Depreciation And Resale: 5.00\n",
      "Fun To Drive - Running Costs: 4.69\n",
      "Depreciation And Resale - Equity And Accessibility: 4.41\n",
      "Battery Recycling And End Of Life - Mineral Supply Chain: 4.27\n",
      "Purchase Price - Second Hand Market: 3.91\n",
      "Range Anxiety - Reliability: 3.49\n",
      "Reliability - Running Costs: 3.35\n",
      "Equity And Accessibility - Purchase Price: 3.31\n",
      "Purchase Price - Range Anxiety: 3.26\n",
      "Fun To Drive - Second Hand Market: 3.12\n",
      "Battery Recycling And End Of Life - Lifecycle Emissions: 2.94\n",
      "Fun To Drive - Technology Roadmap: 2.88\n",
      "Charging Infrastructure - Range Anxiety: 2.56\n",
      "Equity And Accessibility - Grid Impact And Energy Mix: 2.45\n",
      "Battery Cost - Technology Roadmap: 2.40\n",
      "Reliability - Second Hand Market: 2.23\n",
      "Depreciation And Resale - Lifecycle Emissions: 2.21\n",
      "Equity And Accessibility - Mineral Supply Chain: 2.15\n",
      "\n",
      "--- HOUSING SECTOR ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lift matrix: paper4data/eval_class_overlaps_housing_lift.xlsx\n",
      "Saved clustering plot: paper4data/clustering_housing_lift.png\n",
      "\n",
      "Strongly associated label pairs (lift >= 2):\n",
      "Community Engagement And Nimby - Permitting And Bureaucracy: 46.00\n",
      "Community Engagement And Nimby - Wildlife: 19.71\n",
      "Permitting And Bureaucracy - Wildlife: 13.14\n",
      "Property Value Impact - Subsidy And Tariff Debate: 7.89\n",
      "Decommissioning And Waste - Visual Impact: 5.11\n",
      "Community Engagement And Nimby - Subsidy And Tariff Debate: 3.94\n",
      "Green Jobs And Workforce - Local Economy: 3.76\n",
      "Land Use - Wildlife: 2.72\n",
      "\n",
      "--- FOOD SECTOR ---\n",
      "Saved lift matrix: paper4data/eval_class_overlaps_food_lift.xlsx\n",
      "Saved clustering plot: paper4data/clustering_food_lift.png\n",
      "\n",
      "Strongly associated label pairs (lift >= 2):\n",
      "Cost - Farmer Livelihoods: 4.05\n",
      "Health - Health Deficiency Anxiety: 3.47\n",
      "Farmer Livelihoods - Systemic Vs Individual Action: 3.10\n",
      "Cultural Identity And Tradition - Health Deficiency Anxiety: 2.86\n",
      "Animal Welfare - Social Media Influence: 2.54\n",
      "Farmer Livelihoods - Health Deficiency Anxiety: 2.51\n",
      "Health - Social Media Influence: 2.49\n",
      "Cultural Identity And Tradition - Taste And Convenience: 2.36\n",
      "Lab Grown And Alt Proteins - Taste And Convenience: 2.11\n",
      "Farmer Livelihoods - Health: 2.02\n"
     ]
    }
   ],
   "source": [
    "# # Analyze label relationships across sectors\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.spatial.distance import squareform\n",
    "\n",
    "# def analyze_label_relationships(sector_models, all_data):\n",
    "#     \"\"\"Analyze relationships between labels using lift and hierarchical clustering\"\"\"\n",
    "#     print(\"\\nAnalyzing label relationships across sectors...\")\n",
    "    \n",
    "#     for sector, model_info in sector_models.items():\n",
    "#         print(f\"\\n--- {sector.upper()} SECTOR ---\")\n",
    "        \n",
    "#         # Get true labels and label names\n",
    "#         sector_data = all_data[sector]\n",
    "#         label_names = model_info['label_names']\n",
    "        \n",
    "#         # Convert to binary format\n",
    "#         true_labels_list = []\n",
    "#         for true_labels in sector_data.values():\n",
    "#             binary_labels = [1 if label in true_labels else 0 for label in label_names]\n",
    "#             true_labels_list.append(binary_labels)\n",
    "            \n",
    "#         # Calculate co-occurrence matrix\n",
    "#         Y = np.array(true_labels_list)\n",
    "#         N = Y.shape[0]  # Total number of samples\n",
    "#         co = np.dot(Y.T, Y)  # Co-occurrence counts\n",
    "#         marginal = Y.sum(axis=0)  # Marginal counts\n",
    "        \n",
    "#         # Calculate expected co-occurrence under independence\n",
    "#         expected = np.outer(marginal, marginal) / N\n",
    "        \n",
    "#         # Calculate lift matrix\n",
    "#         lift = co / expected\n",
    "#         lift = pd.DataFrame(lift, index=label_names, columns=label_names)\n",
    "        \n",
    "#         # Save lift matrix\n",
    "#         lift.to_excel(f'paper4data/eval_class_overlaps_{sector}_lift.xlsx')\n",
    "        \n",
    "#         # Convert lift to distance (1/lift, capped at 1 for lift > 1)\n",
    "#         dist_lift = 1 / np.maximum(lift, 1)\n",
    "#         np.fill_diagonal(dist_lift.values, 0)  # Set diagonal to 0\n",
    "        \n",
    "#         # Perform hierarchical clustering\n",
    "#         condensed_dist = squareform(dist_lift)\n",
    "#         Z = linkage(condensed_dist, method='average')\n",
    "        \n",
    "#         # Plot dendrogram\n",
    "#         plt.figure(figsize=(10, 7))\n",
    "#         dendrogram(Z, labels=label_names, leaf_rotation=90)\n",
    "#         plt.title(f\"{sector} - Clustering by Lift\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'paper4data/clustering_{sector}_lift.png')\n",
    "#         plt.close()\n",
    "        \n",
    "#         print(f\"Saved lift matrix: paper4data/eval_class_overlaps_{sector}_lift.xlsx\")\n",
    "#         print(f\"Saved clustering plot: paper4data/clustering_{sector}_lift.png\")\n",
    "        \n",
    "#         # Print strongly associated pairs (lift >= 2)\n",
    "#         strong_pairs = []\n",
    "#         for i in range(len(label_names)):\n",
    "#             for j in range(i+1, len(label_names)):\n",
    "#                 if lift.iloc[i,j] >= 2:\n",
    "#                     strong_pairs.append((label_names[i], label_names[j], lift.iloc[i,j]))\n",
    "        \n",
    "#         if strong_pairs:\n",
    "#             print(\"\\nStrongly associated label pairs (lift >= 2):\")\n",
    "#             for l1, l2, lift_val in sorted(strong_pairs, key=lambda x: x[2], reverse=True):\n",
    "#                 print(f\"{l1} - {l2}: {lift_val:.2f}\")\n",
    "\n",
    "# # Run analysis\n",
    "# analyze_label_relationships(all_sector_models, all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ea0ed",
   "metadata": {},
   "source": [
    "## testing evaluating on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING SECTOR MODELS WITH TOP-K ACCURACY ===\n",
      "\n",
      "--- TRANSPORT SECTOR ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 378\n",
      "Top-3 accuracy: 70.37% (266/378)\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Example 1:\n",
      "Comment: With today's announcement, Plug Power has established the world's first ever Gigafactory for Proton ...\n",
      "True labels: ['Technology Roadmap']\n",
      "Top-3 predictions: ['Mineral Supply Chain', 'Policy And Mandates', 'Lifecycle Emissions']\n",
      "Hit: \n",
      "\n",
      "Example 2:\n",
      "Comment: ###[Transition to Electric Vehicles They Say. With Both Major Parties Signing Up to the United Natio...\n",
      "True labels: ['Grid Impact And Energy Mix', 'Policy And Mandates', 'Charging Infrastructure']\n",
      "Top-3 predictions: ['Policy And Mandates', 'Charging Infrastructure', 'Purchase Price']\n",
      "Hit: \n",
      "\n",
      "Example 3:\n",
      "Comment: &gt; It's hard to tell right now if this facility will be utilizing these latest discoveries or will...\n",
      "True labels: ['Technology Roadmap']\n",
      "Top-3 predictions: ['Alternative Modes', 'Environmental Benefit', 'Mineral Supply Chain']\n",
      "Hit: \n",
      "\n",
      "--- HOUSING SECTOR ---\n",
      "Total test samples: 282\n",
      "Top-3 accuracy: 76.95% (217/282)\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Example 1:\n",
      "Comment: Also, no finger wagging at people in other countries about hunting animals, then going home to a fox...\n",
      "True labels: ['Local Economy']\n",
      "Top-3 predictions: ['Grid Stability And Storage', 'Local Economy', 'Foreign Dependence And Trade']\n",
      "Hit: \n",
      "\n",
      "Example 2:\n",
      "Comment: Nobody charges their EV through solar panels. In reality it's all done directly through \"the grid\", ...\n",
      "True labels: ['Grid Stability And Storage']\n",
      "Top-3 predictions: ['Grid Stability And Storage', 'Decommissioning And Waste', 'Land Use']\n",
      "Hit: \n",
      "\n",
      "Example 3:\n",
      "Comment: ***\"Your rare earth article is pretty old.\"***\\n\\nSo what?  Anyway, Here is one from 2019 that says ...\n",
      "True labels: ['Cost Volatility And Economic Risk', 'Foreign Dependence And Trade']\n",
      "Top-3 predictions: ['Grid Stability And Storage', 'Local Economy', 'Foreign Dependence And Trade']\n",
      "Hit: \n",
      "\n",
      "--- FOOD SECTOR ---\n",
      "Total test samples: 443\n",
      "Top-3 accuracy: 93.68% (415/443)\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Example 1:\n",
      "Comment: &gt; Veganism does not \"distract\" from anything, nor can you prove that it does.\\n\\n[Emphasizing ind...\n",
      "True labels: ['Systemic Vs Individual Action']\n",
      "Top-3 predictions: ['Environmental Impact', 'Systemic Vs Individual Action', 'Cost']\n",
      "Hit: \n",
      "\n",
      "Example 2:\n",
      "Comment: This is the best tl;dr I could make, [original](https://www.theguardian.com/environment/2017/sep/29/...\n",
      "True labels: ['Environmental Impact']\n",
      "Top-3 predictions: ['Environmental Impact', 'Systemic Vs Individual Action', 'Cost']\n",
      "Hit: \n",
      "\n",
      "Example 3:\n",
      "Comment: Ideally, the whole world would have embraced a /r/PlantBasedDiet decades ago, but better late than n...\n",
      "True labels: ['Environmental Impact']\n",
      "Top-3 predictions: ['Environmental Impact', 'Health', 'Systemic Vs Individual Action']\n",
      "Hit: \n"
     ]
    }
   ],
   "source": [
    "# !python evaluate_trained_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e933f54",
   "metadata": {},
   "source": [
    "## Raw hypothesis counts to remove hypthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dca15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and merged test data for 3 sectors\n",
      "\n",
      "============================================================\n",
      "EVALUATING ON TEST DATA WITH OPTIMAL THRESHOLDS\n",
      "============================================================\n",
      "\n",
      "--- TRANSPORT SECTOR ---\n",
      "Number of test comments: 378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting predictions:   1%|          | 4/378 [00:00<00:24, 15.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-averaged Jaccard Index: 0.161\n",
      "\n",
      "Alternative Modes (appears in 47 test comments):\n",
      "Jaccard Index: 0.179\n",
      "Threshold: 0.27\n",
      "\n",
      "Battery Cost (appears in 12 test comments):\n",
      "Jaccard Index: 0.067\n",
      "Threshold: 0.14\n",
      "\n",
      "Battery Recycling And End Of Life (appears in 7 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.12\n",
      "\n",
      "Charging Infrastructure (appears in 77 test comments):\n",
      "Jaccard Index: 0.270\n",
      "Threshold: 0.28\n",
      "\n",
      "Depreciation And Resale (appears in 3 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "Environmental Benefit (appears in 55 test comments):\n",
      "Jaccard Index: 0.309\n",
      "Threshold: 0.31\n",
      "\n",
      "Equity And Accessibility (appears in 24 test comments):\n",
      "Jaccard Index: 0.028\n",
      "Threshold: 0.13\n",
      "\n",
      "Fun To Drive (appears in 10 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.10\n",
      "\n",
      "Grid Impact And Energy Mix (appears in 51 test comments):\n",
      "Jaccard Index: 0.156\n",
      "Threshold: 0.19\n",
      "\n",
      "Insurance And Ownership Risk (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.58\n",
      "\n",
      "Lifecycle Emissions (appears in 28 test comments):\n",
      "Jaccard Index: 0.155\n",
      "Threshold: 0.26\n",
      "\n",
      "Mineral Supply Chain (appears in 36 test comments):\n",
      "Jaccard Index: 0.163\n",
      "Threshold: 0.24\n",
      "\n",
      "Policy And Mandates (appears in 76 test comments):\n",
      "Jaccard Index: 0.407\n",
      "Threshold: 0.37\n",
      "\n",
      "Purchase Price (appears in 35 test comments):\n",
      "Jaccard Index: 0.225\n",
      "Threshold: 0.26\n",
      "\n",
      "Range Anxiety (appears in 24 test comments):\n",
      "Jaccard Index: 0.080\n",
      "Threshold: 0.15\n",
      "\n",
      "Reliability (appears in 10 test comments):\n",
      "Jaccard Index: 0.053\n",
      "Threshold: 0.11\n",
      "\n",
      "Running Costs (appears in 22 test comments):\n",
      "Jaccard Index: 0.042\n",
      "Threshold: 0.22\n",
      "\n",
      "Safety And Fire Risk (appears in 36 test comments):\n",
      "Jaccard Index: 0.250\n",
      "Threshold: 0.18\n",
      "\n",
      "Second Hand Market (appears in 7 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.14\n",
      "\n",
      "Technology Roadmap (appears in 19 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.13\n",
      "\n",
      "Battery Cost_Battery Recycling And End Of Life (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.13\n",
      "\n",
      "Battery Cost_Mineral Supply Chain (appears in 4 test comments):\n",
      "Jaccard Index: 0.143\n",
      "Threshold: 0.19\n",
      "\n",
      "Battery Cost_Reliability (appears in 3 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.13\n",
      "\n",
      "Battery Cost_Second Hand Market (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.14\n",
      "\n",
      "Battery Cost_Technology Roadmap (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.14\n",
      "\n",
      "Battery Recycling And End Of Life_Grid Impact And Energy Mix (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.15\n",
      "\n",
      "Battery Recycling And End Of Life_Lifecycle Emissions (appears in 4 test comments):\n",
      "Jaccard Index: 0.074\n",
      "Threshold: 0.19\n",
      "\n",
      "Battery Recycling And End Of Life_Second Hand Market (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.13\n",
      "\n",
      "Battery Recycling And End Of Life_Technology Roadmap (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.12\n",
      "\n",
      "Charging Infrastructure_Range Anxiety (appears in 10 test comments):\n",
      "Jaccard Index: 0.135\n",
      "Threshold: 0.22\n",
      "\n",
      "Charging Infrastructure_Reliability (appears in 5 test comments):\n",
      "Jaccard Index: 0.052\n",
      "Threshold: 0.20\n",
      "\n",
      "Depreciation And Resale_Equity And Accessibility (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.12\n",
      "\n",
      "Depreciation And Resale_Purchase Price (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Equity And Accessibility_Mineral Supply Chain (appears in 5 test comments):\n",
      "Jaccard Index: 0.077\n",
      "Threshold: 0.18\n",
      "\n",
      "Equity And Accessibility_Second Hand Market (appears in 4 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.13\n",
      "\n",
      "Fun To Drive_Reliability (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "Insurance And Ownership Risk_Purchase Price (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.42\n",
      "\n",
      "Insurance And Ownership Risk_Range Anxiety (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.36\n",
      "\n",
      "Insurance And Ownership Risk_Safety And Fire Risk (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.38\n",
      "\n",
      "Lifecycle Emissions_Mineral Supply Chain (appears in 9 test comments):\n",
      "Jaccard Index: 0.031\n",
      "Threshold: 0.25\n",
      "\n",
      "Lifecycle Emissions_Second Hand Market (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.20\n",
      "\n",
      "Mineral Supply Chain_Technology Roadmap (appears in 4 test comments):\n",
      "Jaccard Index: 0.077\n",
      "Threshold: 0.19\n",
      "\n",
      "Range Anxiety_Running Costs (appears in 4 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Running Costs_Second Hand Market (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "--- HOUSING SECTOR ---\n",
      "Number of test comments: 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-averaged Jaccard Index: 0.163\n",
      "\n",
      "Community Engagement And Nimby (appears in 5 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.69\n",
      "\n",
      "Cost Volatility And Economic Risk (appears in 23 test comments):\n",
      "Jaccard Index: 0.088\n",
      "Threshold: 0.20\n",
      "\n",
      "Decommissioning And Waste (appears in 30 test comments):\n",
      "Jaccard Index: 0.244\n",
      "Threshold: 0.26\n",
      "\n",
      "Foreign Dependence And Trade (appears in 39 test comments):\n",
      "Jaccard Index: 0.188\n",
      "Threshold: 0.35\n",
      "\n",
      "Green Jobs And Workforce (appears in 14 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "Grid Stability And Storage (appears in 85 test comments):\n",
      "Jaccard Index: 0.447\n",
      "Threshold: 0.36\n",
      "\n",
      "Land Use (appears in 25 test comments):\n",
      "Jaccard Index: 0.156\n",
      "Threshold: 0.20\n",
      "\n",
      "Local Economy (appears in 73 test comments):\n",
      "Jaccard Index: 0.165\n",
      "Threshold: 0.36\n",
      "\n",
      "Permitting And Bureaucracy (appears in 4 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.24\n",
      "\n",
      "Property Value Impact (appears in 4 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.41\n",
      "\n",
      "Resilience And Blackout Protection (appears in 8 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "Subsidy And Tariff Debate (appears in 38 test comments):\n",
      "Jaccard Index: 0.241\n",
      "Threshold: 0.32\n",
      "\n",
      "Utility Bills (appears in 29 test comments):\n",
      "Jaccard Index: 0.047\n",
      "Threshold: 0.21\n",
      "\n",
      "Visual Impact (appears in 4 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.12\n",
      "\n",
      "Wildlife (appears in 20 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "Community Engagement And Nimby_Cost Volatility And Economic Risk (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.45\n",
      "\n",
      "Community Engagement And Nimby_Decommissioning And Waste (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.47\n",
      "\n",
      "Community Engagement And Nimby_Green Jobs And Workforce (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.40\n",
      "\n",
      "Community Engagement And Nimby_Land Use (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.45\n",
      "\n",
      "Community Engagement And Nimby_Permitting And Bureaucracy (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.47\n",
      "\n",
      "Community Engagement And Nimby_Property Value Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.55\n",
      "\n",
      "Community Engagement And Nimby_Resilience And Blackout Protection (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.40\n",
      "\n",
      "Community Engagement And Nimby_Subsidy And Tariff Debate (appears in 3 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.51\n",
      "\n",
      "Community Engagement And Nimby_Utility Bills (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.45\n",
      "\n",
      "Community Engagement And Nimby_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.41\n",
      "\n",
      "Community Engagement And Nimby_Wildlife (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.40\n",
      "\n",
      "Cost Volatility And Economic Risk_Permitting And Bureaucracy (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.22\n",
      "\n",
      "Cost Volatility And Economic Risk_Property Value Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.30\n",
      "\n",
      "Cost Volatility And Economic Risk_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.16\n",
      "\n",
      "Decommissioning And Waste_Permitting And Bureaucracy (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.25\n",
      "\n",
      "Decommissioning And Waste_Property Value Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.33\n",
      "\n",
      "Decommissioning And Waste_Resilience And Blackout Protection (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.19\n",
      "\n",
      "Decommissioning And Waste_Visual Impact (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.19\n",
      "\n",
      "Foreign Dependence And Trade_Green Jobs And Workforce (appears in 4 test comments):\n",
      "Jaccard Index: 0.125\n",
      "Threshold: 0.23\n",
      "\n",
      "Foreign Dependence And Trade_Property Value Impact (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.38\n",
      "\n",
      "Foreign Dependence And Trade_Visual Impact (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.23\n",
      "\n",
      "Green Jobs And Workforce_Local Economy (appears in 9 test comments):\n",
      "Jaccard Index: 0.074\n",
      "Threshold: 0.24\n",
      "\n",
      "Green Jobs And Workforce_Permitting And Bureaucracy (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Green Jobs And Workforce_Property Value Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.26\n",
      "\n",
      "Green Jobs And Workforce_Resilience And Blackout Protection (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "Green Jobs And Workforce_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.12\n",
      "\n",
      "Land Use_Permitting And Bureaucracy (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.22\n",
      "\n",
      "Land Use_Property Value Impact (appears in 3 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.30\n",
      "\n",
      "Land Use_Visual Impact (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.16\n",
      "\n",
      "Land Use_Wildlife (appears in 7 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.16\n",
      "\n",
      "Local Economy_Property Value Impact (appears in 3 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.38\n",
      "\n",
      "Permitting And Bureaucracy_Property Value Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.32\n",
      "\n",
      "Permitting And Bureaucracy_Resilience And Blackout Protection (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Permitting And Bureaucracy_Utility Bills (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.22\n",
      "\n",
      "Permitting And Bureaucracy_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Permitting And Bureaucracy_Wildlife (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.17\n",
      "\n",
      "Property Value Impact_Resilience And Blackout Protection (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.26\n",
      "\n",
      "Property Value Impact_Utility Bills (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.31\n",
      "\n",
      "Property Value Impact_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.26\n",
      "\n",
      "Property Value Impact_Wildlife (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.26\n",
      "\n",
      "Resilience And Blackout Protection_Utility Bills (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.16\n",
      "\n",
      "Resilience And Blackout Protection_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.12\n",
      "\n",
      "Utility Bills_Visual Impact (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.16\n",
      "\n",
      "Visual Impact_Wildlife (appears in 3 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.11\n",
      "\n",
      "--- FOOD SECTOR ---\n",
      "Number of test comments: 443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-averaged Jaccard Index: 0.421\n",
      "\n",
      "Animal Welfare (appears in 58 test comments):\n",
      "Jaccard Index: 0.309\n",
      "Threshold: 0.30\n",
      "\n",
      "Cost (appears in 27 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.32\n",
      "\n",
      "Cultural Identity And Tradition (appears in 16 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.17\n",
      "\n",
      "Environmental Impact (appears in 247 test comments):\n",
      "Jaccard Index: 0.764\n",
      "Threshold: 0.60\n",
      "\n",
      "Farmer Livelihoods (appears in 7 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Health (appears in 51 test comments):\n",
      "Jaccard Index: 0.227\n",
      "Threshold: 0.45\n",
      "\n",
      "Health Deficiency Anxiety (appears in 20 test comments):\n",
      "Jaccard Index: 0.250\n",
      "Threshold: 0.32\n",
      "\n",
      "Lab Grown And Alt Proteins (appears in 32 test comments):\n",
      "Jaccard Index: 0.262\n",
      "Threshold: 0.31\n",
      "\n",
      "Psychology And Identity (appears in 54 test comments):\n",
      "Jaccard Index: 0.238\n",
      "Threshold: 0.42\n",
      "\n",
      "Social Media Influence (appears in 13 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.15\n",
      "\n",
      "Systemic Vs Individual Action (appears in 79 test comments):\n",
      "Jaccard Index: 0.355\n",
      "Threshold: 0.33\n",
      "\n",
      "Taste And Convenience (appears in 56 test comments):\n",
      "Jaccard Index: 0.432\n",
      "Threshold: 0.33\n",
      "\n",
      "Cost_Farmer Livelihoods (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.25\n",
      "\n",
      "Cost_Health (appears in 8 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.39\n",
      "\n",
      "Cultural Identity And Tradition_Farmer Livelihoods (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.18\n",
      "\n",
      "Cultural Identity And Tradition_Health Deficiency Anxiety (appears in 2 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.24\n",
      "\n",
      "Cultural Identity And Tradition_Social Media Influence (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.16\n",
      "\n",
      "Farmer Livelihoods_Social Media Influence (appears in 1 test comments):\n",
      "Jaccard Index: 0.000\n",
      "Threshold: 0.17\n",
      "\n",
      "Health_Health Deficiency Anxiety (appears in 9 test comments):\n",
      "Jaccard Index: 0.200\n",
      "Threshold: 0.38\n",
      "\n",
      "Test evaluation results saved to 'test_evaluation_results_jaccard.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# !python analyze_hypothesis_counts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428a0f0",
   "metadata": {},
   "source": [
    "## Test sector models with dynamic top-k accuracy (n where n is number of true labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc00141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_sector_models_dynamic_top_k(sector_models, test_data):\n",
    "#     \"\"\"Test sector models with dynamic top-k accuracy where k = number of true labels\"\"\"\n",
    "#     print(\"=== TESTING SECTOR MODELS WITH DYNAMIC TOP-K ACCURACY ===\")\n",
    "    \n",
    "#     for sector, model_data in sector_models.items():\n",
    "#         if sector not in test_data:\n",
    "#             print(f\"No test data for {sector} sector\")\n",
    "#             continue\n",
    "            \n",
    "#         print(f\"\\n--- {sector.upper()} SECTOR ---\")\n",
    "#         model = model_data['model']\n",
    "#         tokenizer = model_data['tokenizer']\n",
    "#         label_names = model_data['label_names']\n",
    "        \n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         model.to(device)\n",
    "#         model.eval()\n",
    "        \n",
    "#         sector_test_data = test_data[sector]\n",
    "#         total_samples = 0\n",
    "#         dynamic_hits = 0\n",
    "        \n",
    "#         for comment, true_labels in sector_test_data.items():\n",
    "#             if not true_labels or not comment.strip():\n",
    "#                 continue\n",
    "                \n",
    "#             total_samples += 1\n",
    "            \n",
    "#             # Dynamic k based on number of true labels\n",
    "#             k = len(true_labels)\n",
    "            \n",
    "#             # Tokenize and predict\n",
    "#             inputs = tokenizer(comment, return_tensors='pt', truncation=True, \n",
    "#                              padding=True, max_length=512)\n",
    "#             inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(**inputs)\n",
    "#                 # Handle different model output formats\n",
    "#                 if hasattr(outputs, 'logits'):\n",
    "#                     logits = outputs.logits\n",
    "#                 else:\n",
    "#                     # If outputs is a tensor directly\n",
    "#                     logits = outputs\n",
    "#                 probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "            \n",
    "#             # Get top-k predictions where k = number of true labels\n",
    "#             top_k_indices = np.argsort(probabilities)[-k:][::-1]\n",
    "#             top_k_labels = [label_names[i] for i in top_k_indices]\n",
    "            \n",
    "#             # Check if any true label is in top-k predictions\n",
    "#             if any(true_label in top_k_labels for true_label in true_labels):\n",
    "#                 dynamic_hits += 1\n",
    "        \n",
    "#         # Calculate dynamic top-k accuracy\n",
    "#         if total_samples > 0:\n",
    "#             dynamic_accuracy = (dynamic_hits / total_samples) * 100\n",
    "#             print(f\"Total test samples: {total_samples}\")\n",
    "#             print(f\"Dynamic top-n accuracy: {dynamic_accuracy:.2f}% ({dynamic_hits}/{total_samples})\")\n",
    "#         else:\n",
    "#             print(f\"No valid test samples for {sector} sector\")\n",
    "\n",
    "# # Test with dynamic top-k accuracy\n",
    "# test_sector_models_dynamic_top_k(all_sector_models, all_test_data)\n",
    "# # test_sector_models_with_top_k(all_sector_models, all_test_data, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1011129",
   "metadata": {},
   "source": [
    "# checking with GPT o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fcecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python generate_detailed_test_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc770e02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc82f604",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259695f3",
   "metadata": {},
   "source": [
    "# Plot Roberta : EVs, solar, vegetarian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70159aa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df, sector_labels\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Generate predictions and plotting dataframe, and save results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m results_df, sector_labels = \u001b[43mget_model_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments_by_sector\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mget_model_predictions\u001b[39m\u001b[34m(comments_by_sector, save_dir, thresholded_save_dir)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get model predictions for a sample of 100 comments per sector, save raw and thresholded results to disk, one CSV per sector\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Find and load all trained models\u001b[39;00m\n\u001b[32m     15\u001b[39m model_dirs = glob.glob(\u001b[33m\"\u001b[39m\u001b[33mmodels/top7_*_distilbert_base_uncased\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluate_trained_models import (\n",
    "    load_trained_model, \n",
    "    load_test_data, \n",
    "    predict_with_scores\n",
    ")\n",
    "def get_model_predictions(comments_by_sector, \n",
    "                         save_dir=\"paper4data/sectorwise_roberta_classifications\",\n",
    "                         thresholded_save_dir=\"paper4data/sectorwise_roberta_classifications_thresholded\"):\n",
    "    \"\"\"Get model predictions for a sample of 100 comments per sector, save raw and thresholded results to disk, one CSV per sector\"\"\"\n",
    "    \n",
    "    import os\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Find and load all trained models\n",
    "    model_dirs = glob.glob(\"models/top7_*_distilbert_base_uncased\")\n",
    "    \n",
    "    if not model_dirs:\n",
    "        print(\"No trained models found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load all sector models\n",
    "    all_sector_models = {}\n",
    "    optimal_thresholds = {}\n",
    "    sector_labels = {}\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        # Extract sector from directory name\n",
    "        sector = model_dir.split('_')[1]  # top7_SECTOR_model\n",
    "        \n",
    "        print(f\"Loading {sector.upper()} model...\")\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer, label_names, sector_optimal_thresholds = load_trained_model(model_dir, device)\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"Failed to load {sector} model\")\n",
    "            continue\n",
    "            \n",
    "        all_sector_models[sector] = {\n",
    "            'model': model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'label_names': label_names\n",
    "        }\n",
    "        \n",
    "        optimal_thresholds[sector] = sector_optimal_thresholds\n",
    "        sector_labels[sector] = label_names\n",
    "    \n",
    "    if not all_sector_models:\n",
    "        print(\"No models loaded successfully!\")\n",
    "        return None, None\n",
    "        \n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(thresholded_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Store all results for return\n",
    "    all_results = []\n",
    "    all_thresholded_results = []\n",
    "    \n",
    "    # Process each sector, only first 100 comments\n",
    "    for sector, comments in comments_by_sector.items():\n",
    "        if sector not in all_sector_models:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {sector} comments (sampling 100)...\")\n",
    "        \n",
    "        model = all_sector_models[sector]['model']\n",
    "        tokenizer = all_sector_models[sector]['tokenizer']\n",
    "        label_names = all_sector_models[sector]['label_names']\n",
    "        \n",
    "        # Sample up to 100 comments\n",
    "        sample_comments = list(comments)[:]\n",
    "        \n",
    "        # Get predictions for each comment\n",
    "        sector_results = []\n",
    "        sector_thresholded_results = []\n",
    "        for text in tqdm(sample_comments):\n",
    "            # Get predictions\n",
    "            prediction_results = predict_with_scores(text, model, tokenizer, label_names, device)\n",
    "            scores = [res['score'] for res in prediction_results]\n",
    "            \n",
    "            # Create result dict\n",
    "            result = {'sector': sector, 'text': text}\n",
    "            \n",
    "            # Add scores for each label\n",
    "            for label, score in zip(label_names, scores):\n",
    "                result[f\"{sector}_{label}\"] = score\n",
    "                \n",
    "            sector_results.append(result)\n",
    "        \n",
    "        # Save raw results for this sector\n",
    "        sector_df = pd.DataFrame(sector_results)\n",
    "        sector_csv_path = os.path.join(save_dir, f\"{sector}_Roberta_classifications.csv\")\n",
    "        sector_df.to_csv(sector_csv_path, index=False)\n",
    "        print(f\"Saved {sector} Roberta classifications to {sector_csv_path}\")\n",
    "        all_results.extend(sector_results)\n",
    "        \n",
    "        # Now create thresholded version for this sector\n",
    "        thresholds = optimal_thresholds.get(sector, {})\n",
    "        for row in sector_results:\n",
    "            thresholded_row = {'sector': sector, 'text': row['text']}\n",
    "            for label in label_names:\n",
    "                col = f\"{sector}_{label}\"\n",
    "                score = row[col]\n",
    "                # Use optimal threshold if available, else 0.5\n",
    "                threshold = thresholds.get(label, 0.5)\n",
    "                thresholded_row[col] = int(score > threshold)\n",
    "            sector_thresholded_results.append(thresholded_row)\n",
    "        thresholded_sector_df = pd.DataFrame(sector_thresholded_results)\n",
    "        thresholded_sector_csv_path = os.path.join(thresholded_save_dir, f\"{sector}_Roberta_classifications_thresholded.csv\")\n",
    "        thresholded_sector_df.to_csv(thresholded_sector_csv_path, index=False)\n",
    "        print(f\"Saved thresholded {sector} Roberta classifications to {thresholded_sector_csv_path}\")\n",
    "        all_thresholded_results.extend(sector_thresholded_results)\n",
    "    \n",
    "    # Optionally, return all results as a single dataframe for further use\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    thresholded_df = pd.DataFrame(all_thresholded_results)\n",
    "    \n",
    "    return results_df, sector_labels\n",
    "\n",
    "# Generate predictions and plotting dataframe, and save results\n",
    "# results_df, sector_labels = get_model_predictions(comments_by_sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9951be2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f9d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting sector: Food\n",
      "(Vegetarianism)\n",
      "Plotting sector: Housing\n",
      "(Solar)\n",
      "Plotting sector: Transport\n",
      "(EVs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sector_results_from_thresholded_csvs(\n",
    "    sector_labels, \n",
    "    data_dir=r\"C:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\paper4data\\sectorwise_roberta_classifications_thresholded\",\n",
    "    comments_by_sector=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads thresholded, binarized sector classification CSVs and plots the percentage of comments\n",
    "    for each label in each sector. The denominator for percentage is the total number of comments\n",
    "    in the sector (from comments_by_sector), not just those in the CSV (which may only include labelled).\n",
    "    Plots all three sectors in a single figure with three subplots.\n",
    "    \"\"\"\n",
    "    if comments_by_sector is None:\n",
    "        raise ValueError(\"comments_by_sector must be provided to get total counts per sector.\")\n",
    "\n",
    "    # Mapping sector to intervention name for display\n",
    "    sector_intervention = {\n",
    "        'transport': 'EVs',\n",
    "        'housing': 'Solar',\n",
    "        'food': 'Vegetarianism'\n",
    "    }\n",
    "    FONT_SIZE = 17\n",
    "    fig = plt.figure(figsize=(12, 24))\n",
    "    gs = fig.add_gridspec(3, 1, height_ratios=[1, 1, 1], hspace=0.3)\n",
    "    axes = []\n",
    "    for idx, (sector, labels) in enumerate(sector_labels.items()):\n",
    "        ax = fig.add_subplot(gs[idx, 0])\n",
    "        axes.append(ax)\n",
    "        label_counts = {}\n",
    "\n",
    "        # Load thresholded, binarized CSV for this sector\n",
    "        csv_path = os.path.join(data_dir, f\"{sector}_Roberta_classifications_thresholded.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            continue\n",
    "        sector_df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Get total number of comments in this sector from comments_by_sector\n",
    "        if sector not in comments_by_sector:\n",
    "            print(f\"Sector '{sector}' not found in comments_by_sector.\")\n",
    "            total = 0\n",
    "        else:\n",
    "            total = len(comments_by_sector[sector])\n",
    "\n",
    "        for label in labels:\n",
    "            colname = f\"{sector}_{label}\"\n",
    "            if colname in sector_df.columns:\n",
    "                # Count number of comments where label is present (==1)\n",
    "                label_counts[label] = int((sector_df[colname] == 1).sum())\n",
    "            else:\n",
    "                label_counts[label] = 0  # If column missing, count as 0\n",
    "\n",
    "        # Sort labels by frequency (descending)\n",
    "        labels_and_counts = [(label, label_counts[label]) for label in labels]\n",
    "        labels_and_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        sorted_labels = [label for label, count in labels_and_counts]\n",
    "        sorted_freqs = [count for label, count in labels_and_counts]\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if total > 0:\n",
    "            sorted_pcts = [count / total * 100 for count in sorted_freqs]\n",
    "        else:\n",
    "            sorted_pcts = [0 for _ in sorted_freqs]\n",
    "\n",
    "        y_pos = np.arange(len(sorted_labels))\n",
    "        color = '#43AA8B' if sector == 'transport' else '#F94144' if sector == 'food' else '#FFB703'\n",
    "        ax.barh(y_pos, sorted_pcts, color=color, alpha=0.8, height=0.6)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels([label.lower() for label in sorted_labels], fontsize=FONT_SIZE * 1.54)\n",
    "        ax.set_xlabel('% of comments', fontsize=FONT_SIZE * 1.54, labelpad=10)\n",
    "        ax.tick_params(axis='x', labelsize=FONT_SIZE * 1.54)\n",
    "        # Compose sector title with intervention\n",
    "        intervention = sector_intervention.get(sector, \"\")\n",
    "        if intervention:\n",
    "            sector_title = f\"{sector.capitalize()}\\n({intervention})\"\n",
    "        else:\n",
    "            sector_title = sector.capitalize()\n",
    "        # Set title location to x=0.6, y=0.8\n",
    "        ax.set_title(\n",
    "            sector_title,\n",
    "            fontsize=FONT_SIZE * 1.5,\n",
    "            pad=20,\n",
    "            loc='left',\n",
    "            rotation=0,\n",
    "            x=0.8,\n",
    "            y=0.75,\n",
    "            bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.1')\n",
    "        )\n",
    "        print(f\"Plotting sector: {sector_title}\")  # Print sector name with intervention\n",
    "        for i, (pct, count) in enumerate(zip(sorted_pcts, sorted_freqs)):\n",
    "            ax.text(pct + 1, i, f'{pct:.1f}% (n={count})', va='center',\n",
    "                   fontsize=FONT_SIZE * 1.32, color='#444444')\n",
    "                   \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.grid(False)\n",
    "        ax.set_axisbelow(True)\n",
    "    # Save the single figure with all three sectors\n",
    "    os.makedirs('paper4figs', exist_ok=True)\n",
    "    plt.savefig(f'paper4figs/paper4_fig2_allsectors.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(f'paper4figs/paper4_fig2_allsectors.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# You must now pass comments_by_sector as an argument!\n",
    "plot_sector_results_from_thresholded_csvs(sector_labels, comments_by_sector=comments_by_sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e47c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': ['Animal Welfare',\n",
       "  'Environmental Impact',\n",
       "  'Health',\n",
       "  'Lab Grown And Alt Proteins',\n",
       "  'Psychology And Identity',\n",
       "  'Systemic Vs Individual Action',\n",
       "  'Taste And Convenience'],\n",
       " 'housing': ['Decommissioning And Waste',\n",
       "  'Foreign Dependence And Trade',\n",
       "  'Grid Stability And Storage',\n",
       "  'Land Use',\n",
       "  'Local Economy',\n",
       "  'Subsidy And Tariff Debate',\n",
       "  'Utility Bills'],\n",
       " 'transport': ['Alternative Modes',\n",
       "  'Charging Infrastructure',\n",
       "  'Environmental Benefit',\n",
       "  'Grid Impact And Energy Mix',\n",
       "  'Mineral Supply Chain',\n",
       "  'Policy And Mandates',\n",
       "  'Purchase Price']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get model directories as in get_model_predictions\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "def get_sector_labels_from_models():\n",
    "    \"\"\"\n",
    "    Finds all model directories and loads each model to extract its label names.\n",
    "    Returns a dict: {sector: [label1, label2, ...], ...}\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Find all trained model directories\n",
    "    model_dirs = glob.glob(\"models/top7_*_distilbert_base_uncased\")\n",
    "    sector_labels = {}\n",
    "    for model_dir in model_dirs:\n",
    "        # Extract sector from directory name\n",
    "        sector = model_dir.split('_')[1]  # expects format: top7_SECTOR_model\n",
    "        # Load model, tokenizer, label_names, thresholds\n",
    "        model, tokenizer, label_names, _ = load_trained_model(model_dir, device)\n",
    "        if label_names is not None:\n",
    "            sector_labels[sector] = label_names\n",
    "    return sector_labels\n",
    "\n",
    "sector_labels = get_sector_labels_from_models()\n",
    "sector_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0373a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size for sector 'transport': 14924\n",
      "Total size for sector 'housing': 9388\n",
      "Total size for sector 'food': 8414\n"
     ]
    }
   ],
   "source": [
    "for sector, comments in comments_by_sector.items():\n",
    "    print(f\"Total size for sector '{sector}': {len(comments)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57acf47",
   "metadata": {},
   "source": [
    "# Plot Temporal Roberta : EVs, solar, vegetarian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6c84df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted and saved high-aesthetic stacked area temporal trend for all sectors in a single vertical figure (3-month rolling average), with left insets showing per-label fraction.\n"
     ]
    }
   ],
   "source": [
    "  # --- Load binarized data with optimal thresholds, merge with created_utc_, and plot temporal trends (high-aesthetic stacked area charts, 3-month rolling average) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "sns.set_context(\"notebook\", font_scale=1.0)\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "binarized_data_dir = r\"C:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\paper4data\\sectorwise_roberta_classifications_thresholded\"\n",
    "\n",
    "sector_dfs = []\n",
    "for sector in sector_labels.keys():\n",
    "    csv_path = os.path.join(binarized_data_dir, f\"{sector}_Roberta_classifications_thresholded.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['sector'] = sector\n",
    "    sector_dfs.append(df)\n",
    "binarized_df = pd.concat(sector_dfs, ignore_index=True)\n",
    "\n",
    "if 'Document' not in binarized_df.columns and 'text' in binarized_df.columns:\n",
    "    binarized_df = binarized_df.rename(columns={'text': 'Document'})\n",
    "merged_df = pd.merge(\n",
    "    binarized_df,\n",
    "    df_joint_w_topics[['Document', 'created_utc_']],\n",
    "    on='Document',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop_duplicates(subset=['Document', 'created_utc_'])\n",
    "\n",
    "if not np.issubdtype(merged_df['created_utc_'].dtype, np.datetime64):\n",
    "    merged_df['created_utc_'] = pd.to_datetime(merged_df['created_utc_'], errors='coerce')\n",
    "\n",
    "merged_df['month'] = merged_df['created_utc_'].dt.to_period('M')\n",
    "\n",
    "def get_sequential_palette_for_sector(sector, n_labels):\n",
    "    sector_cmaps = {\n",
    "        'transport': 'Reds',\n",
    "        'housing': 'Oranges',\n",
    "        'food': 'Blues'\n",
    "    }\n",
    "    cmap_name = sector_cmaps.get(sector, 'Greys')\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    colors = [cmap(x) for x in np.linspace(0.25, 1, n_labels)]\n",
    "    return colors\n",
    "\n",
    "def plot_temporal_sector_results_combined_vertical(\n",
    "    merged_df, \n",
    "    sector_labels, \n",
    "    time_col='month',\n",
    "    save_dir='paper4figs'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots temporal trends of label raw counts for all sectors as high-aesthetic stacked area charts in a single figure (one column, three rows).\n",
    "    Uses a 3-month rolling average for smoothing (centered).\n",
    "    Font size for x/y/labels/titles is increased by 10% over previous.\n",
    "    All subplots share the same x-axis range (start and end year/month), so all plots are aligned in time.\n",
    "    Each subplot has its own y-axis max (not shared).\n",
    "    Only the bottom plot has an x-axis label; all have y-axis labels.\n",
    "    Annotation arrows have high transparency (alpha=0.2).\n",
    "    Curves inside each sector's plot are ordered so that the lowest volume subsector is at the bottom, then next smallest, with biggest on top.\n",
    "    For each sector, an inset axis is added on the left showing the fraction (percentage) for each subsector (normalized at each time point by the sum of all label counts at that time, which can be >100% due to multi-labels per comment).\n",
    "    \"\"\"\n",
    "    sector_intervention = {\n",
    "        'transport': 'EVs',\n",
    "        'housing': 'Solar',\n",
    "        'food': 'Vegetarianism'\n",
    "    }\n",
    "\n",
    "    # Font size settings (10% larger than before)\n",
    "    BASE_FONT_SIZE = 15\n",
    "    SMALLER_FONT_SIZE = BASE_FONT_SIZE * 0.7\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    n_sectors = len(sector_labels)\n",
    "    fig, axes = plt.subplots(n_sectors, 1, figsize=(12, 14), dpi=300, sharex=True, sharey=False)\n",
    "    if n_sectors == 1:\n",
    "        axes = [axes]\n",
    "    elif n_sectors == 0:\n",
    "        print(\"No sectors to plot.\")\n",
    "        return\n",
    "\n",
    "    # --- Find the global min and max month across all sectors ---\n",
    "    all_months = merged_df['month'].dropna().sort_values().unique()\n",
    "    if len(all_months) == 0:\n",
    "        print(\"No months found in merged_df.\")\n",
    "        return\n",
    "\n",
    "    # Set the minimum month to 2012-01 if available\n",
    "    min_month = all_months[0]\n",
    "    max_month = all_months[-1]\n",
    "    min_plot_month = pd.Period('2012-01', freq='M')\n",
    "    # If the data starts after 2012-01, use the data's min month; otherwise, force 2012-01\n",
    "    if min_month > min_plot_month:\n",
    "        min_month_for_range = min_month\n",
    "    else:\n",
    "        min_month_for_range = min_plot_month\n",
    "\n",
    "    # Create a full range of months from min_month_for_range to max_month (inclusive)\n",
    "    all_months_range = pd.period_range(start=min_month_for_range, end=max_month, freq='M')\n",
    "    months_str = [str(m) for m in all_months_range]\n",
    "\n",
    "    for idx, (ax, (sector, labels)) in enumerate(zip(axes, sector_labels.items())):\n",
    "        sector_df = merged_df[merged_df['sector'] == sector].copy()\n",
    "        if sector_df.empty:\n",
    "            print(f\"No data for sector: {sector}\")\n",
    "            continue\n",
    "\n",
    "        # --- Compute total volume for each label (for ordering) ---\n",
    "        label_total_counts = {}\n",
    "        for label in labels:\n",
    "            colname = f\"{sector}_{label}\"\n",
    "            if colname in sector_df.columns:\n",
    "                label_total_counts[label] = sector_df[colname].sum()\n",
    "            else:\n",
    "                label_total_counts[label] = 0\n",
    "\n",
    "        # Order labels by total volume (ascending: smallest at bottom, largest at top)\n",
    "        ordered_labels = sorted(labels, key=lambda l: label_total_counts[l])\n",
    "\n",
    "        # --- Compute rolling monthly counts for each label in the new order ---\n",
    "        label_time_counts = {}\n",
    "        for label in ordered_labels:\n",
    "            colname = f\"{sector}_{label}\"\n",
    "            if colname in sector_df.columns:\n",
    "                # Group by month and sum, then rolling average (window=3, center=True)\n",
    "                monthly_counts = sector_df.groupby(time_col)[colname].sum().reindex(all_months_range, fill_value=0)\n",
    "                rolling_counts = monthly_counts.rolling(window=3, center=True, min_periods=1).mean()\n",
    "                label_time_counts[label] = rolling_counts\n",
    "            else:\n",
    "                label_time_counts[label] = pd.Series(dtype=float, index=all_months_range)\n",
    "\n",
    "        # Use the global all_months_range for all sectors\n",
    "        count_matrix = []\n",
    "        for label in ordered_labels:\n",
    "            counts = label_time_counts[label].reindex(all_months_range, fill_value=0)\n",
    "            count_matrix.append(counts.values)\n",
    "        count_matrix = np.vstack(count_matrix)  # shape: (n_labels, n_months)\n",
    "\n",
    "        n_labels = len(ordered_labels)\n",
    "        colors = get_sequential_palette_for_sector(sector, n_labels)\n",
    "\n",
    "        # Get total number of comments for this sector\n",
    "        if isinstance(comments_by_sector, dict):\n",
    "            sector_total_comments = len(comments_by_sector[sector])\n",
    "        elif hasattr(comments_by_sector, 'loc'):\n",
    "            sector_total_comments = comments_by_sector.loc[comments_by_sector['sector'] == sector].shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"comments_by_sector must be a dict or DataFrame with sector info.\")\n",
    "\n",
    "        polys = ax.stackplot(\n",
    "            np.arange(len(months_str)), count_matrix, \n",
    "            labels=[label.lower() for label in ordered_labels], \n",
    "            colors=colors, \n",
    "            alpha=1.0,\n",
    "            edgecolor='white', \n",
    "            linewidth=0.7\n",
    "         )\n",
    "\n",
    "        ax.set_facecolor('white')\n",
    "        ax.grid(axis='y', color='#e5e5e5', linestyle='--', linewidth=1.2, alpha=0.7, zorder=0)\n",
    "\n",
    "        intervention = sector_intervention.get(sector, \"\")\n",
    "        if intervention:\n",
    "            sector_title = f\"{sector.capitalize()} ({intervention})\"\n",
    "        else:\n",
    "            sector_title = sector.capitalize()\n",
    "        ax.set_title(\n",
    "            sector_title,\n",
    "            fontsize=BASE_FONT_SIZE,\n",
    "            pad=18,\n",
    "            loc='left',\n",
    "            x=0.01,\n",
    "            y=.8,\n",
    "            fontweight='bold',\n",
    "            color='#222222',\n",
    "            bbox=dict(facecolor='white', edgecolor='#bbbbbb', boxstyle='round,pad=0.18', alpha=0.95)\n",
    "        )\n",
    "        # Only set x-axis label for the bottom plot\n",
    "        if idx == n_sectors - 1:\n",
    "            ax.set_xlabel(\"year\", fontsize=BASE_FONT_SIZE, labelpad=10, fontweight='semibold')\n",
    "        # Set y-axis label for all plots, add (rolling) to label\n",
    "        ax.set_ylabel(\"monthly counts (rolling)\", fontsize=BASE_FONT_SIZE, labelpad=10, fontweight='semibold')\n",
    "        ax.tick_params(axis='x', labelsize=BASE_FONT_SIZE, rotation=0, length=0)\n",
    "        ax.tick_params(axis='y', labelsize=BASE_FONT_SIZE, length=0)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        # --- Annotate the last value for each label, using adjustText to avoid overlap, with arrows (no head) pointing to original placement ---\n",
    "        annotation_texts = []\n",
    "        # No arrows: do not define arrowprops, do not use in adjust_text, do not draw arrows\n",
    "        for i, label in enumerate(ordered_labels):\n",
    "            if i >= count_matrix.shape[0]:\n",
    "                print(f\"Warning: Not enough rows in count_matrix for label '{label}' in sector '{sector}'. Skipping annotation.\")\n",
    "                continue\n",
    "            y_curve = count_matrix[i]\n",
    "            valid_indices = np.where(~np.isnan(y_curve))[0]\n",
    "            if len(valid_indices) == 0:\n",
    "                continue\n",
    "            last_idx = valid_indices[-1]\n",
    "            x = last_idx\n",
    "            y_sum = np.sum(count_matrix[:i+1, x])\n",
    "            # For annotation, use the sum of the original (not rolling) counts for cumulative count\n",
    "            colname = f\"{sector}_{label}\"\n",
    "            if colname in sector_df.columns:\n",
    "                orig_monthly_counts = sector_df.groupby(time_col)[colname].sum().reindex(all_months_range, fill_value=0)\n",
    "                cumulative_count = int(np.nansum(orig_monthly_counts.values))\n",
    "            else:\n",
    "                cumulative_count = 0\n",
    "            if sector_total_comments > 0:\n",
    "                pct = 100.0 * cumulative_count / sector_total_comments\n",
    "            else:\n",
    "                pct = 0.0\n",
    "            annotation_str = f\"{label.replace('_', ' ').capitalize()}\\n n={cumulative_count:,} ({pct:.1f}%)\"\n",
    "            annotation = ax.text(\n",
    "                x, y_sum,\n",
    "                annotation_str,\n",
    "                va='center', ha='left',\n",
    "                fontsize=BASE_FONT_SIZE * 0.7,\n",
    "                color=colors[i],\n",
    "                fontweight='bold',\n",
    "                alpha=1.0,\n",
    "                bbox=dict(facecolor='white', edgecolor='white', alpha=0.7, pad=0.0)\n",
    "            )\n",
    "            annotation_texts.append(annotation)\n",
    "            annotation._original_xy = (x, y_sum)\n",
    "\n",
    "        adjust_text(\n",
    "            annotation_texts,\n",
    "            ax=ax,\n",
    "            expand_text=(1.1, 1.2),\n",
    "            expand_points=(1.2, 1.4),\n",
    "            # No arrowprops argument\n",
    "            only_move={'points': 'y', 'text': 'y'},\n",
    "            autoalign='y',\n",
    "            force_text=0.5,\n",
    "            force_points=0.5,\n",
    "            lim=200,\n",
    "            precision=0.01\n",
    "        )\n",
    "        # Shift all annotation texts right by a fixed amount, and left-align all texts to the same x position\n",
    "        renderer = ax.figure.canvas.get_renderer()\n",
    "        trans, inv = ax.transData, ax.transData.inverted()\n",
    "        # Choose a fixed x position in data coordinates to align all texts (e.g., furthest right + offset)\n",
    "        # Find the maximum x among all annotation positions\n",
    "        max_x = max(txt.get_position()[0] for txt in annotation_texts)\n",
    "        # Shift all texts to a fixed x position to the right of max_x (e.g., by 1.5 units)\n",
    "        fixed_x = max_x + 15.5\n",
    "        for txt in annotation_texts:\n",
    "            _, y = txt.get_position()\n",
    "            txt.set_position((fixed_x, y))\n",
    "            txt.set_ha('left')  # Ensure left alignment\n",
    "\n",
    "        for txt in annotation_texts:\n",
    "            if hasattr(txt, '_original_xy'):\n",
    "                txt.set_bbox(dict(facecolor='white', edgecolor='none', alpha=1.0, pad=0.0))\n",
    "\n",
    "        # --- Custom x-axis: show only years, in gaps of 2 or 3, and ensure x ticks are equally spaced ---\n",
    "        months_years = [int(str(m)[:4]) for m in all_months_range]\n",
    "        unique_years = sorted(set(months_years))\n",
    "        if len(unique_years) > 10:\n",
    "            step = 3\n",
    "        else:\n",
    "            step = 2\n",
    "        year_ticks = unique_years[::step]\n",
    "        year_tick_indices = []\n",
    "        year_tick_labels = []\n",
    "        for y in year_ticks:\n",
    "            try:\n",
    "                idx = months_years.index(y)\n",
    "                year_tick_indices.append(idx)\n",
    "                year_tick_labels.append(str(y))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        ax.set_xticks(np.arange(len(months_str)))\n",
    "        xtick_labels = []\n",
    "        used_years = set()\n",
    "        for i, m in enumerate(all_months_range):\n",
    "            year = int(str(m)[:4])\n",
    "            if year in year_ticks and months_years[i] == year and year not in used_years:\n",
    "                xtick_labels.append(str(year))\n",
    "                used_years.add(year)\n",
    "            else:\n",
    "                xtick_labels.append(\"\")\n",
    "        ax.set_xticklabels(xtick_labels)\n",
    "        ax.tick_params(axis='x', labelsize=BASE_FONT_SIZE, rotation=0, length=0)\n",
    "\n",
    "        # Set individual y-axis max for each plot (do NOT share y max)\n",
    "        y_max = np.nanmax(np.sum(count_matrix, axis=0))\n",
    "        ax.set_ylim(0, y_max * 1.08 if y_max > 0 else 1)\n",
    "\n",
    "        # ----------- INSET: Fractional (percentage) stacked area plot -----------\n",
    "        # For each time point, sum all label counts (can be > number of comments due to multi-labels)\n",
    "        # Then, for each label, compute fraction of total label counts at that time point\n",
    "        # Use same color order as main plot, no legend, no axis labels, y-axis 0-1.2 (can sum >1)\n",
    "        # Place inset on left side of main axis\n",
    "\n",
    "        # --- Compute cumulative sum of label counts up to each month ---\n",
    "        # For each label, compute cumulative sum over time\n",
    "        cumulative_label_counts = np.cumsum(count_matrix, axis=1)  # shape: (n_labels, n_months)\n",
    "        # For all labels, compute total cumulative sum at each time point\n",
    "        cumulative_total_counts = np.sum(cumulative_label_counts, axis=0)  # shape: (n_months,)\n",
    "\n",
    "        # To avoid division by zero, set cumulative_total_counts to 1 where zero (temporarily)\n",
    "        cumulative_total_counts_safe = np.where(cumulative_total_counts == 0, 1, cumulative_total_counts)\n",
    "        # For each label, compute fraction of cumulative total at each time point\n",
    "        cumulative_fraction_matrix = cumulative_label_counts / cumulative_total_counts_safe  # shape: (n_labels, n_months)\n",
    "        # Set columns (months) where cumulative_total_counts == 0 to all zeros (no data)\n",
    "        zero_cum_months = (cumulative_total_counts == 0)\n",
    "        cumulative_fraction_matrix[:, zero_cum_months] = 0.0\n",
    "\n",
    "        # --- Smooth the cumulative fraction matrix with the same rolling window (3, center=True) ---\n",
    "        smoothed_cumulative_fraction_matrix = np.zeros_like(cumulative_fraction_matrix)\n",
    "        for i in range(cumulative_fraction_matrix.shape[0]):\n",
    "            s = pd.Series(cumulative_fraction_matrix[i])\n",
    "            smoothed_cumulative_fraction_matrix[i] = s.rolling(window=3, center=True, min_periods=1).mean().values\n",
    "\n",
    "        # Renormalize so that at each time point, sum of all label fractions is 1 (or 0 if no data)\n",
    "        col_sums_cum = smoothed_cumulative_fraction_matrix.sum(axis=0)\n",
    "        col_sums_cum_safe = np.where(col_sums_cum == 0, 1, col_sums_cum)\n",
    "        fraction_matrix_cum = smoothed_cumulative_fraction_matrix / col_sums_cum_safe\n",
    "        # Set columns where col_sums_cum==0 to all zeros (no data)\n",
    "        fraction_matrix_cum[:, col_sums_cum == 0] = 0.0\n",
    "\n",
    "        # --- Inset axes: place on left, occupying about 52% width, 60% height, left-aligned ---\n",
    "        inset_width = 0.32\n",
    "        inset_height = 0.40\n",
    "        inset_left = 0.1\n",
    "        inset_bottom = 0.21\n",
    "        ax_inset = ax.inset_axes([inset_left, inset_bottom, inset_width, inset_height])\n",
    "\n",
    "        # Plot stacked area of cumulative fractions\n",
    "        ax_inset.stackplot(\n",
    "            np.arange(len(months_str)), fraction_matrix_cum,\n",
    "            colors=colors,\n",
    "            alpha=1.0,\n",
    "            edgecolor='white',\n",
    "            linewidth=0.7\n",
    "        )\n",
    "        ax_inset.set_ylim(0, 1)\n",
    "        ax_inset.set_xlim(0, len(months_str)-1)\n",
    "        ax_inset.set_facecolor('white')\n",
    "        # Add 4 x ticks: 2012, 2015, 2018, 2021\n",
    "        xtick_years = [2012, 2015, 2018, 2021]\n",
    "        xtick_indices = []\n",
    "        xtick_labels = []\n",
    "        for year in xtick_years:\n",
    "            idx = next((i for i, m in enumerate(months_str) if m.startswith(str(year))), None)\n",
    "            if idx is not None:\n",
    "                xtick_indices.append(idx)\n",
    "                xtick_labels.append(str(year))\n",
    "        ax_inset.set_xticks(xtick_indices)\n",
    "        ax_inset.set_xticklabels(xtick_labels, fontsize=BASE_FONT_SIZE*0.7, color='#888888')\n",
    "        # Add y ticks at 0, 25, 50, 75, 100 percent\n",
    "        ytick_vals = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "        ytick_labels = ['0', '25', '50', '75', '100']\n",
    "        ax_inset.set_yticks(ytick_vals)\n",
    "        ax_inset.set_yticklabels(ytick_labels, fontsize=BASE_FONT_SIZE*0.7, color='#888888')\n",
    "        for spine in ax_inset.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        for spine in ['left', 'bottom', 'top', 'right']:\n",
    "            ax_inset.spines[spine].set_visible(True)\n",
    "            ax_inset.spines[spine].set_color('#bbbbbb')\n",
    "            ax_inset.spines[spine].set_linewidth(1.0)\n",
    "        ax_inset.text(0.6, .99, \"Cumulative %\", fontsize=BASE_FONT_SIZE*0.85, color='k', ha='left', va='bottom', transform=ax_inset.transAxes, alpha=1)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "    plt.savefig(os.path.join(save_dir, f\"paper4_fig2_temporal_allsectors_stackedarea_vertical.pdf\"), bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(os.path.join(save_dir, f\"paper4_fig2_temporal_allsectors_stackedarea_vertical.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Plotted and saved high-aesthetic stacked area temporal trend for all sectors in a single vertical figure (3-month rolling average), with left insets showing per-label fraction.\")\n",
    "\n",
    "# Call the function\n",
    "plot_temporal_sector_results_combined_vertical(merged_df, sector_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750a7fc",
   "metadata": {},
   "source": [
    "# Pro anti GPT training, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f6c26be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing transport sector with 150 comments...\n",
      "Processing transport (Electric Vehicles) with 150 comments...\n",
      "Processing 100 comments for Electric Vehicles with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Electric Vehicles batches: 100%|| 100/100 [00:30<00:00,  3.30task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_transport_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.180000    0.310000      0.020000    0.490000\n",
      "std      0.386123    0.464823      0.140705    0.502418\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "transport completed in 30.32 seconds\n",
      "\n",
      "Processing housing sector with 150 comments...\n",
      "Processing housing (Solar Power) with 150 comments...\n",
      "Processing 100 comments for Solar Power with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Solar Power batches: 100%|| 100/100 [00:24<00:00,  4.14task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_housing_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Solar Power: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.140000    0.280000      0.020000    0.560000\n",
      "std      0.348735    0.451261      0.140705    0.498888\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    1.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "housing completed in 24.16 seconds\n",
      "\n",
      "Processing food sector with 150 comments...\n",
      "Processing food (Vegetarianism/Veganism) with 150 comments...\n",
      "Processing 100 comments for Vegetarianism/Veganism with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Vegetarianism/Veganism batches: 100%|| 100/100 [00:36<00:00,  2.75task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_food_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.440000    0.220000      0.040000    0.300000\n",
      "std      0.498888    0.416333      0.196946    0.460566\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "food completed in 36.38 seconds\n",
      "\n",
      "Total processing time: 90.87 seconds\n",
      " Results saved to paper4data/pro_anti_all_sectors_*\n",
      "DataFrame created with shape: (300, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "- Solar Power: 100 comments\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.180000    0.310000      0.020000    0.490000\n",
      "std      0.386123    0.464823      0.140705    0.502418\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.140000    0.280000      0.020000    0.560000\n",
      "std      0.348735    0.451261      0.140705    0.498888\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    1.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.440000    0.220000      0.040000    0.300000\n",
      "std      0.498888    0.416333      0.196946    0.460566\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "\n",
      "Processing transport sector with 150 comments...\n",
      "Processing transport (Electric Vehicles) with 150 comments...\n",
      "Processing 100 comments for Electric Vehicles with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Electric Vehicles batches: 100%|| 100/100 [00:35<00:00,  2.80task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_transport_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:              pro        anti  pro_and_anti     neither\n",
      "count  100.00000  100.000000    100.000000  100.000000\n",
      "mean     0.15000    0.370000      0.020000    0.460000\n",
      "std      0.35887    0.485237      0.140705    0.500908\n",
      "min      0.00000    0.000000      0.000000    0.000000\n",
      "25%      0.00000    0.000000      0.000000    0.000000\n",
      "50%      0.00000    0.000000      0.000000    0.000000\n",
      "75%      0.00000    1.000000      0.000000    1.000000\n",
      "max      1.00000    1.000000      1.000000    1.000000\n",
      "transport completed in 35.93 seconds\n",
      "\n",
      "Processing housing sector with 150 comments...\n",
      "Processing housing (Solar Power) with 150 comments...\n",
      "Processing 100 comments for Solar Power with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Solar Power batches: 100%|| 100/100 [00:31<00:00,  3.19task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_housing_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Solar Power: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.220000    0.270000      0.050000    0.460000\n",
      "std      0.416333    0.446196      0.219043    0.500908\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "housing completed in 31.42 seconds\n",
      "\n",
      "Processing food sector with 150 comments...\n",
      "Processing food (Vegetarianism/Veganism) with 150 comments...\n",
      "Processing 100 comments for Vegetarianism/Veganism with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Vegetarianism/Veganism batches: 100%|| 100/100 [00:35<00:00,  2.85task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_food_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.420000    0.180000      0.040000    0.360000\n",
      "std      0.496045    0.386123      0.196946    0.482418\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "food completed in 35.18 seconds\n",
      "\n",
      "Total processing time: 102.53 seconds\n",
      " Results saved to paper4data/pro_anti_all_sectors_*\n",
      "DataFrame created with shape: (300, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "- Solar Power: 100 comments\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:              pro        anti  pro_and_anti     neither\n",
      "count  100.00000  100.000000    100.000000  100.000000\n",
      "mean     0.15000    0.370000      0.020000    0.460000\n",
      "std      0.35887    0.485237      0.140705    0.500908\n",
      "min      0.00000    0.000000      0.000000    0.000000\n",
      "25%      0.00000    0.000000      0.000000    0.000000\n",
      "50%      0.00000    0.000000      0.000000    0.000000\n",
      "75%      0.00000    1.000000      0.000000    1.000000\n",
      "max      1.00000    1.000000      1.000000    1.000000\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.220000    0.270000      0.050000    0.460000\n",
      "std      0.416333    0.446196      0.219043    0.500908\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.420000    0.180000      0.040000    0.360000\n",
      "std      0.496045    0.386123      0.196946    0.482418\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "\n",
      "Processing transport sector with 150 comments...\n",
      "Processing transport (Electric Vehicles) with 150 comments...\n",
      "Processing 100 comments for Electric Vehicles with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Electric Vehicles batches: 100%|| 100/100 [00:31<00:00,  3.13task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_transport_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.200000    0.380000      0.040000    0.380000\n",
      "std      0.402015    0.487832      0.196946    0.487832\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "transport completed in 32.03 seconds\n",
      "\n",
      "Processing housing sector with 150 comments...\n",
      "Processing housing (Solar Power) with 150 comments...\n",
      "Processing 100 comments for Solar Power with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Solar Power batches: 100%|| 100/100 [00:26<00:00,  3.77task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_housing_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Solar Power: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Solar Power:              pro        anti  pro_and_anti     neither\n",
      "count  100.00000  100.000000    100.000000  100.000000\n",
      "mean     0.21000    0.300000      0.050000    0.440000\n",
      "std      0.40936    0.460566      0.219043    0.498888\n",
      "min      0.00000    0.000000      0.000000    0.000000\n",
      "25%      0.00000    0.000000      0.000000    0.000000\n",
      "50%      0.00000    0.000000      0.000000    0.000000\n",
      "75%      0.00000    1.000000      0.000000    1.000000\n",
      "max      1.00000    1.000000      1.000000    1.000000\n",
      "housing completed in 26.61 seconds\n",
      "\n",
      "Processing food sector with 150 comments...\n",
      "Processing food (Vegetarianism/Veganism) with 150 comments...\n",
      "Processing 100 comments for Vegetarianism/Veganism with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Vegetarianism/Veganism batches: 100%|| 100/100 [00:35<00:00,  2.84task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_food_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000     100.00000  100.000000\n",
      "mean     0.490000    0.100000       0.08000    0.330000\n",
      "std      0.502418    0.301511       0.27266    0.472582\n",
      "min      0.000000    0.000000       0.00000    0.000000\n",
      "25%      0.000000    0.000000       0.00000    0.000000\n",
      "50%      0.000000    0.000000       0.00000    0.000000\n",
      "75%      1.000000    0.000000       0.00000    1.000000\n",
      "max      1.000000    1.000000       1.00000    1.000000\n",
      "food completed in 35.30 seconds\n",
      "\n",
      "Total processing time: 93.94 seconds\n",
      " Results saved to paper4data/pro_anti_all_sectors_*\n",
      "DataFrame created with shape: (300, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "- Solar Power: 100 comments\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.200000    0.380000      0.040000    0.380000\n",
      "std      0.402015    0.487832      0.196946    0.487832\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Solar Power:              pro        anti  pro_and_anti     neither\n",
      "count  100.00000  100.000000    100.000000  100.000000\n",
      "mean     0.21000    0.300000      0.050000    0.440000\n",
      "std      0.40936    0.460566      0.219043    0.498888\n",
      "min      0.00000    0.000000      0.000000    0.000000\n",
      "25%      0.00000    0.000000      0.000000    0.000000\n",
      "50%      0.00000    0.000000      0.000000    0.000000\n",
      "75%      0.00000    1.000000      0.000000    1.000000\n",
      "max      1.00000    1.000000      1.000000    1.000000\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000     100.00000  100.000000\n",
      "mean     0.490000    0.100000       0.08000    0.330000\n",
      "std      0.502418    0.301511       0.27266    0.472582\n",
      "min      0.000000    0.000000       0.00000    0.000000\n",
      "25%      0.000000    0.000000       0.00000    0.000000\n",
      "50%      0.000000    0.000000       0.00000    0.000000\n",
      "75%      1.000000    0.000000       0.00000    1.000000\n",
      "max      1.000000    1.000000       1.00000    1.000000\n",
      "\n",
      "Processing transport sector with 150 comments...\n",
      "Processing transport (Electric Vehicles) with 150 comments...\n",
      "Processing 100 comments for Electric Vehicles with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Electric Vehicles batches: 100%|| 100/100 [00:31<00:00,  3.15task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_transport_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.170000    0.390000      0.050000    0.390000\n",
      "std      0.377525    0.490207      0.219043    0.490207\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "transport completed in 31.84 seconds\n",
      "\n",
      "Processing housing sector with 150 comments...\n",
      "Processing housing (Solar Power) with 150 comments...\n",
      "Processing 100 comments for Solar Power with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Solar Power batches: 100%|| 100/100 [00:26<00:00,  3.82task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_housing_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Solar Power: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Solar Power:               pro        anti  pro_and_anti  neither\n",
      "count  100.000000  100.000000    100.000000   100.00\n",
      "mean     0.260000    0.270000      0.020000     0.45\n",
      "std      0.440844    0.446196      0.140705     0.50\n",
      "min      0.000000    0.000000      0.000000     0.00\n",
      "25%      0.000000    0.000000      0.000000     0.00\n",
      "50%      0.000000    0.000000      0.000000     0.00\n",
      "75%      1.000000    1.000000      0.000000     1.00\n",
      "max      1.000000    1.000000      1.000000     1.00\n",
      "housing completed in 26.24 seconds\n",
      "\n",
      "Processing food sector with 150 comments...\n",
      "Processing food (Vegetarianism/Veganism) with 150 comments...\n",
      "Processing 100 comments for Vegetarianism/Veganism with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Vegetarianism/Veganism batches: 100%|| 100/100 [00:33<00:00,  3.01task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_food_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.370000    0.190000      0.050000    0.390000\n",
      "std      0.485237    0.394277      0.219043    0.490207\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "food completed in 33.22 seconds\n",
      "\n",
      "Total processing time: 91.30 seconds\n",
      " Results saved to paper4data/pro_anti_all_sectors_*\n",
      "DataFrame created with shape: (300, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "- Solar Power: 100 comments\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.170000    0.390000      0.050000    0.390000\n",
      "std      0.377525    0.490207      0.219043    0.490207\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Solar Power:               pro        anti  pro_and_anti  neither\n",
      "count  100.000000  100.000000    100.000000   100.00\n",
      "mean     0.260000    0.270000      0.020000     0.45\n",
      "std      0.440844    0.446196      0.140705     0.50\n",
      "min      0.000000    0.000000      0.000000     0.00\n",
      "25%      0.000000    0.000000      0.000000     0.00\n",
      "50%      0.000000    0.000000      0.000000     0.00\n",
      "75%      1.000000    1.000000      0.000000     1.00\n",
      "max      1.000000    1.000000      1.000000     1.00\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.370000    0.190000      0.050000    0.390000\n",
      "std      0.485237    0.394277      0.219043    0.490207\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "\n",
      "Processing transport sector with 150 comments...\n",
      "Processing transport (Electric Vehicles) with 150 comments...\n",
      "Processing 100 comments for Electric Vehicles with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Electric Vehicles batches: 100%|| 100/100 [00:28<00:00,  3.46task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_transport_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.230000    0.340000      0.020000    0.410000\n",
      "std      0.422953    0.476095      0.140705    0.494311\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "transport completed in 28.95 seconds\n",
      "\n",
      "Processing housing sector with 150 comments...\n",
      "Processing housing (Solar Power) with 150 comments...\n",
      "Processing 100 comments for Solar Power with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Solar Power batches: 100%|| 100/100 [00:27<00:00,  3.66task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_housing_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Solar Power: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000        100.00  100.000000\n",
      "mean     0.240000    0.270000          0.01    0.470000\n",
      "std      0.429235    0.446196          0.10    0.501614\n",
      "min      0.000000    0.000000          0.00    0.000000\n",
      "25%      0.000000    0.000000          0.00    0.000000\n",
      "50%      0.000000    0.000000          0.00    0.000000\n",
      "75%      0.000000    1.000000          0.00    1.000000\n",
      "max      1.000000    1.000000          1.00    1.000000\n",
      "housing completed in 27.37 seconds\n",
      "\n",
      "Processing food sector with 150 comments...\n",
      "Processing food (Vegetarianism/Veganism) with 150 comments...\n",
      "Processing 100 comments for Vegetarianism/Veganism with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Vegetarianism/Veganism batches: 100%|| 100/100 [00:29<00:00,  3.37task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_food_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.440000    0.170000      0.070000    0.320000\n",
      "std      0.498888    0.377525      0.256432    0.468826\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "food completed in 29.72 seconds\n",
      "\n",
      "Total processing time: 86.05 seconds\n",
      " Results saved to paper4data/pro_anti_all_sectors_*\n",
      "DataFrame created with shape: (300, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "- Solar Power: 100 comments\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.230000    0.340000      0.020000    0.410000\n",
      "std      0.422953    0.476095      0.140705    0.494311\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      0.000000    1.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000        100.00  100.000000\n",
      "mean     0.240000    0.270000          0.01    0.470000\n",
      "std      0.429235    0.446196          0.10    0.501614\n",
      "min      0.000000    0.000000          0.00    0.000000\n",
      "25%      0.000000    0.000000          0.00    0.000000\n",
      "50%      0.000000    0.000000          0.00    0.000000\n",
      "75%      0.000000    1.000000          0.00    1.000000\n",
      "max      1.000000    1.000000          1.00    1.000000\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.440000    0.170000      0.070000    0.320000\n",
      "std      0.498888    0.377525      0.256432    0.468826\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "\n",
      "Processing transport sector with 150 comments...\n",
      "Processing transport (Electric Vehicles) with 150 comments...\n",
      "Processing 100 comments for Electric Vehicles with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Electric Vehicles batches: 100%|| 100/100 [00:29<00:00,  3.43task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_transport_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:              pro        anti  pro_and_anti     neither\n",
      "count  100.00000  100.000000    100.000000  100.000000\n",
      "mean     0.21000    0.350000      0.020000    0.420000\n",
      "std      0.40936    0.479372      0.140705    0.496045\n",
      "min      0.00000    0.000000      0.000000    0.000000\n",
      "25%      0.00000    0.000000      0.000000    0.000000\n",
      "50%      0.00000    0.000000      0.000000    0.000000\n",
      "75%      0.00000    1.000000      0.000000    1.000000\n",
      "max      1.00000    1.000000      1.000000    1.000000\n",
      "transport completed in 29.18 seconds\n",
      "\n",
      "Processing housing sector with 150 comments...\n",
      "Processing housing (Solar Power) with 150 comments...\n",
      "Processing 100 comments for Solar Power with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Solar Power batches: 100%|| 100/100 [00:23<00:00,  4.18task/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_housing_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Solar Power: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.270000    0.230000      0.030000    0.470000\n",
      "std      0.446196    0.422953      0.171447    0.501614\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "housing completed in 23.92 seconds\n",
      "\n",
      "Processing food sector with 150 comments...\n",
      "Processing food (Vegetarianism/Veganism) with 150 comments...\n",
      "Processing 100 comments for Vegetarianism/Veganism with 3 parallel agents, batch size 1, 1 runs per batch\n",
      "Created 100 batches of 1 comments each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Vegetarianism/Veganism batches: 100%|| 100/100 [00:31<00:00,  3.18task/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100/100 tasks\n",
      "Processed 100 batches with 100 successful runs\n",
      " Results saved to paper4data/pro_anti_food_*\n",
      "DataFrame created with shape: (100, 6)\n",
      "Comments per topic:\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000     100.00000  100.000000\n",
      "mean     0.390000    0.170000       0.08000    0.360000\n",
      "std      0.490207    0.377525       0.27266    0.482418\n",
      "min      0.000000    0.000000       0.00000    0.000000\n",
      "25%      0.000000    0.000000       0.00000    0.000000\n",
      "50%      0.000000    0.000000       0.00000    0.000000\n",
      "75%      1.000000    0.000000       0.00000    1.000000\n",
      "max      1.000000    1.000000       1.00000    1.000000\n",
      "food completed in 31.42 seconds\n",
      "\n",
      "Total processing time: 84.52 seconds\n",
      " Results saved to paper4data/pro_anti_all_sectors_*\n",
      "DataFrame created with shape: (300, 6)\n",
      "Comments per topic:\n",
      "- Electric Vehicles: 100 comments\n",
      "- Solar Power: 100 comments\n",
      "- Vegetarianism/Veganism: 100 comments\n",
      "\n",
      "Sample scores (should be between 0.0 and 1.0):\n",
      "Electric Vehicles:              pro        anti  pro_and_anti     neither\n",
      "count  100.00000  100.000000    100.000000  100.000000\n",
      "mean     0.21000    0.350000      0.020000    0.420000\n",
      "std      0.40936    0.479372      0.140705    0.496045\n",
      "min      0.00000    0.000000      0.000000    0.000000\n",
      "25%      0.00000    0.000000      0.000000    0.000000\n",
      "50%      0.00000    0.000000      0.000000    0.000000\n",
      "75%      0.00000    1.000000      0.000000    1.000000\n",
      "max      1.00000    1.000000      1.000000    1.000000\n",
      "Solar Power:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000    100.000000  100.000000\n",
      "mean     0.270000    0.230000      0.030000    0.470000\n",
      "std      0.446196    0.422953      0.171447    0.501614\n",
      "min      0.000000    0.000000      0.000000    0.000000\n",
      "25%      0.000000    0.000000      0.000000    0.000000\n",
      "50%      0.000000    0.000000      0.000000    0.000000\n",
      "75%      1.000000    0.000000      0.000000    1.000000\n",
      "max      1.000000    1.000000      1.000000    1.000000\n",
      "Vegetarianism/Veganism:               pro        anti  pro_and_anti     neither\n",
      "count  100.000000  100.000000     100.00000  100.000000\n",
      "mean     0.390000    0.170000       0.08000    0.360000\n",
      "std      0.490207    0.377525       0.27266    0.482418\n",
      "min      0.000000    0.000000       0.00000    0.000000\n",
      "25%      0.000000    0.000000       0.00000    0.000000\n",
      "50%      0.000000    0.000000       0.00000    0.000000\n",
      "75%      1.000000    0.000000       0.00000    1.000000\n",
      "max      1.000000    1.000000       1.00000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Reload the module and set up for train/val/test splits\n",
    "from importlib import reload\n",
    "import paper4_pro_anti_GPT_classifier\n",
    "reload(paper4_pro_anti_GPT_classifier)\n",
    "from paper4_pro_anti_GPT_classifier import classify_comments_pro_anti, main_pro_anti\n",
    "\n",
    "import json\n",
    "\n",
    "# User option: choose starting point for range\n",
    "START_IDX = 5000  # <-- Set this to your desired starting index (e.g., 0, 1000, etc.)\n",
    "\n",
    "# Define split sizes and ranges (150 per batch)\n",
    "split_sizes = {\n",
    "    \"train\": 600,      # 4 batches of 150\n",
    "    \"validation\": 150, # 1 batch of 150\n",
    "    \"test\": 150        # 1 batch of 150\n",
    "}\n",
    "batch_size = 150\n",
    "\n",
    "# For each sector, get the comments as a list\n",
    "comments_by_sector_lists = {\n",
    "    sector: list(comments)\n",
    "    for sector, comments in comments_by_sector.items()\n",
    "}\n",
    "\n",
    "# Helper to get batch indices for each split, with custom start\n",
    "def get_split_indices(split, split_sizes, batch_size, start_idx):\n",
    "    if split == \"train\":\n",
    "        # 4 batches: start_idx to start_idx+600\n",
    "        return [(start_idx + i * batch_size, start_idx + (i + 1) * batch_size) for i in range(split_sizes[\"train\"] // batch_size)]\n",
    "    elif split == \"validation\":\n",
    "        # Next batch: start_idx+600 to start_idx+750\n",
    "        return [(start_idx + split_sizes[\"train\"], start_idx + split_sizes[\"train\"] + batch_size)]\n",
    "    elif split == \"test\":\n",
    "        # Next batch: start_idx+750 to start_idx+900\n",
    "        return [(start_idx + split_sizes[\"train\"] + batch_size, start_idx + split_sizes[\"train\"] + 2 * batch_size)]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown split\")\n",
    "\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    split_indices = get_split_indices(split, split_sizes, batch_size, START_IDX)\n",
    "    # For each batch in the split, process and accumulate results\n",
    "    all_results = {}\n",
    "    all_results_df = []\n",
    "    for batch_num, (start, end) in enumerate(split_indices, start=1):\n",
    "        # Sample comments for this batch for each sector\n",
    "        sampled_comments = {\n",
    "            sector: comments[start:end] if len(comments) > end else comments[start:]\n",
    "            for sector, comments in comments_by_sector_lists.items()\n",
    "        }\n",
    "        # Run pro/anti classifier for all sectors\n",
    "        results, results_df = main_pro_anti(\n",
    "            comments_by_sector=sampled_comments,\n",
    "            num_runs=1,\n",
    "            max_comments_per_sector=1000,\n",
    "            num_agents=3,\n",
    "            batch_size=1\n",
    "        )\n",
    "        # Accumulate results\n",
    "        all_results[f\"batch_{batch_num}\"] = results\n",
    "        all_results_df.append(results_df)\n",
    "\n",
    "    # Save results with split name in filenames\n",
    "    results_filename = f\"paper4data/{split}_data_by_GPT_results_pro_anti_v2.json\"\n",
    "    results_df_filename = f\"paper4data/{split}_data_by_GPT_df_pro_anti_v2.csv\"\n",
    "\n",
    "    with open(results_filename, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    # Concatenate all DataFrames for this split\n",
    "    import pandas as pd\n",
    "    pd.concat(all_results_df, ignore_index=True).to_csv(results_df_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "37d03827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All comments match: True\n",
      "Number of unique comments in paper4data/train_data_by_GPT_df_pro_anti_2.csv: 1180\n",
      "Number of unique comments in paper4data/train_data_by_GPT_df_pro_anti_2v2.csv: 1180\n",
      "Number of matched comments (intersection): 1240\n",
      "Percentage of comments matched (intersection/union): 105.08%\n",
      "WARNING: Percentage of comments matched (intersection/max): 105.08% (>100% means there are duplicate comments in at least one file)\n",
      "This can happen if there are duplicate 'comment' values in one or both files. The intersection (number of rows in merged) can exceed the number of unique comments in either file if duplicates exist.\n",
      "Check for duplicates in each file:\n",
      "Number of duplicate comments in paper4data/train_data_by_GPT_df_pro_anti_2.csv: 20\n",
      "Number of duplicate comments in paper4data/train_data_by_GPT_df_pro_anti_2v2.csv: 20\n",
      "Label set exact overlap: 96.05% (1191/1240)\n",
      "                                             comment   labels_1   labels_2  \\\n",
      "0                Falsely Powering Up the EV Industry     {anti}     {anti}   \n",
      "1  EVs seem to be there mainly to assuage upper a...     {anti}     {anti}   \n",
      "2  Do you really claim that kW is not a measure o...  {neither}  {neither}   \n",
      "3  Granted, that half the EVs in the entire coun...     {anti}     {anti}   \n",
      "4  &gt;My electricity is 100% renewable. My car i...     {anti}     {anti}   \n",
      "5  &gt;My electricity is 100% renewable. My car i...     {anti}     {anti}   \n",
      "6  Lithium mining and potential dependence is the...     {anti}     {anti}   \n",
      "7  As the auto industry pivots toward electrifica...     {anti}     {anti}   \n",
      "8  Do you understand the earth is 9 billion yrs o...     {anti}     {anti}   \n",
      "9  Australias paltry electric car uptake will co...     {anti}     {anti}   \n",
      "\n",
      "   label_overlap  \n",
      "0           True  \n",
      "1           True  \n",
      "2           True  \n",
      "3           True  \n",
      "4           True  \n",
      "5           True  \n",
      "6           True  \n",
      "7           True  \n",
      "8           True  \n",
      "9           True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Only compare between these two CSVs\n",
    "csv1 = \"paper4data/train_data_by_GPT_df_pro_anti_2.csv\"\n",
    "csv2 = \"paper4data/train_data_by_GPT_df_pro_anti_2v2.csv\"\n",
    "\n",
    "label_cols = [\"pro\", \"anti\", \"pro_and_anti\", \"neither\"]\n",
    "\n",
    "def get_labels_for_row(row, label_cols):\n",
    "    \"\"\"\n",
    "    For a row, return a set of label columns (from label_cols) that are 1/True.\n",
    "    \"\"\"\n",
    "    return set(label for label in label_cols if label in row and (row[label] == 1 or row[label] is True or row[label] == True))\n",
    "\n",
    "try:\n",
    "    df1 = pd.read_csv(csv1)\n",
    "    df2 = pd.read_csv(csv2)\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in label_cols:\n",
    "        if col not in df1.columns or col not in df2.columns:\n",
    "            raise ValueError(f\"Column '{col}' missing in one of the files: {csv1}, {csv2}\")\n",
    "    if 'comment' not in df1.columns or 'comment' not in df2.columns:\n",
    "        raise ValueError(\"Both files must have a 'comment' column.\")\n",
    "\n",
    "    # Ensure all comments match\n",
    "    comments1 = set(df1['comment'])\n",
    "    comments2 = set(df2['comment'])\n",
    "    all_comments_match = comments1 == comments2\n",
    "    print(f\"All comments match: {all_comments_match}\")\n",
    "    if not all_comments_match:\n",
    "        only_in_1 = comments1 - comments2\n",
    "        only_in_2 = comments2 - comments1\n",
    "        print(f\"Comments only in {csv1}: {len(only_in_1)}\")\n",
    "        print(f\"Comments only in {csv2}: {len(only_in_2)}\")\n",
    "    n_comments_1 = len(comments1)\n",
    "    n_comments_2 = len(comments2)\n",
    "    print(f\"Number of unique comments in {csv1}: {n_comments_1}\")\n",
    "    print(f\"Number of unique comments in {csv2}: {n_comments_2}\")\n",
    "\n",
    "    # Merge on comment (inner join to only compare comments present in both)\n",
    "    merged = pd.merge(\n",
    "        df1[['comment'] + label_cols],\n",
    "        df2[['comment'] + label_cols],\n",
    "        on='comment',\n",
    "        how='inner'\n",
    "    )\n",
    "    n_matched_comments = len(merged)\n",
    "    print(f\"Number of matched comments (intersection): {n_matched_comments}\")\n",
    "\n",
    "    # The percentage of matched comments should be relative to the union, not the max, to avoid >100%\n",
    "    n_union_comments = len(comments1 | comments2)\n",
    "    percent_matched_union = (n_matched_comments / n_union_comments) * 100 if n_union_comments > 0 else 0.0\n",
    "    print(f\"Percentage of comments matched (intersection/union): {percent_matched_union:.2f}%\")\n",
    "\n",
    "    # For reference, also print the old calculation (intersection/max) and explain why it can be >100%\n",
    "    percent_matched_max = (n_matched_comments / max(n_comments_1, n_comments_2)) * 100 if max(n_comments_1, n_comments_2) > 0 else 0.0\n",
    "    if percent_matched_max > 100:\n",
    "        print(f\"WARNING: Percentage of comments matched (intersection/max): {percent_matched_max:.2f}% (>100% means there are duplicate comments in at least one file)\")\n",
    "        print(\"This can happen if there are duplicate 'comment' values in one or both files. The intersection (number of rows in merged) can exceed the number of unique comments in either file if duplicates exist.\")\n",
    "        print(\"Check for duplicates in each file:\")\n",
    "        n_dupes_1 = df1['comment'].duplicated().sum()\n",
    "        n_dupes_2 = df2['comment'].duplicated().sum()\n",
    "        print(f\"Number of duplicate comments in {csv1}: {n_dupes_1}\")\n",
    "        print(f\"Number of duplicate comments in {csv2}: {n_dupes_2}\")\n",
    "    else:\n",
    "        print(f\"Percentage of comments matched (intersection/max): {percent_matched_max:.2f}%\")\n",
    "\n",
    "    # For each matched comment, get the set of labels in each file\n",
    "    def get_labels_for_row1(row):\n",
    "        return set(label for label in label_cols if row[label + '_x'] == 1 or row[label + '_x'] is True or row[label + '_x'] == True)\n",
    "    def get_labels_for_row2(row):\n",
    "        return set(label for label in label_cols if row[label + '_y'] == 1 or row[label + '_y'] is True or row[label + '_y'] == True)\n",
    "\n",
    "    # After merge, columns from df1 have _x, from df2 have _y\n",
    "    merged['labels_1'] = merged.apply(get_labels_for_row1, axis=1)\n",
    "    merged['labels_2'] = merged.apply(get_labels_for_row2, axis=1)\n",
    "\n",
    "    # Calculate overlap percentage (labels_1 == labels_2)\n",
    "    merged['label_overlap'] = merged.apply(lambda row: row['labels_1'] == row['labels_2'], axis=1)\n",
    "    n_overlap = merged['label_overlap'].sum()\n",
    "    percent_overlap = (n_overlap / n_matched_comments) * 100 if n_matched_comments > 0 else 0.0\n",
    "    print(f\"Label set exact overlap: {percent_overlap:.2f}% ({n_overlap}/{n_matched_comments})\")\n",
    "\n",
    "    # Show a sample of the merged DataFrame\n",
    "    print(merged[['comment', 'labels_1', 'labels_2', 'label_overlap']].head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error comparing {csv1} and {csv2}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d2876",
   "metadata": {},
   "source": [
    "# finteuning roberta for pro and anti labellling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9f23677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "# !python paper4_BERT_finetuning_pro_anti.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Or import and use functions\n",
    "from paper4_BERT_finetuning_pro_anti import debug_training_pro_anti_with_evaluation\n",
    "debug_training_pro_anti_with_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db31ec",
   "metadata": {},
   "source": [
    "## pro anti BERT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9752f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRO-ANTI CLASSIFICATION USING FINE-TUNED BERT MODELS\n",
      "================================================================================\n",
      "Please provide your comments_by_sector data and call get_pro_anti_predictions()\n",
      "Example:\n",
      "results_df, sector_labels = get_pro_anti_predictions(comments_by_sector)\n",
      "\n",
      "Output directories:\n",
      "  Raw predictions: paper4data/sectorwise_pro_anti_classifications/\n",
      "  Thresholded predictions: paper4data/sectorwise_pro_anti_classifications_thresholded/\n",
      "\n",
      "Expected model directories:\n",
      "  models/pro_anti_*_roberta_base\n",
      "  models/pro_anti_*_distilbert_base_uncased\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Get pro-anti predictions for comments using fine-tuned BERT models\n",
    "Similar to get_model_predictions but for pro-anti classification\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Import functions from pro-anti training and evaluation files\n",
    "from paper4_BERT_finetuning_pro_anti import (\n",
    "    ProAntiClassifier,\n",
    "    get_tokenizer,\n",
    "    predict_pro_anti_scores,\n",
    "    topic_to_sector\n",
    ")\n",
    "\n",
    "from paper4_BERT_finetuning_pro_anti_eval import (\n",
    "    apply_optimal_thresholds\n",
    ")\n",
    "\n",
    "def load_trained_pro_anti_model(model_dir, device):\n",
    "    \"\"\"\n",
    "    Load a trained pro-anti model from directory\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to model directory\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer, label_names, optimal_thresholds\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model checkpoint\n",
    "        checkpoint_path = os.path.join(model_dir, 'best_model', 'model.pt')\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"Best model not found at {checkpoint_path}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model_name = checkpoint.get('model_name', 'roberta-base')\n",
    "        label_names = checkpoint['label_names']\n",
    "        optimal_thresholds = checkpoint.get('optimal_thresholds', {})\n",
    "        \n",
    "        print(f\"Loading {model_name} model with labels: {label_names}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ProAntiClassifier(model_name, len(label_names))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = get_tokenizer(model_name)\n",
    "        tokenizer_path = os.path.join(model_dir, 'best_model')\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            tokenizer = tokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        return model, tokenizer, label_names, optimal_thresholds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_dir}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_pro_anti_predictions(comments_by_sector, \n",
    "                           save_dir=\"paper4data/sectorwise_pro_anti_classifications\",\n",
    "                           thresholded_save_dir=\"paper4data/sectorwise_pro_anti_classifications_thresholded\"):\n",
    "    \"\"\"\n",
    "    Get pro-anti predictions for comments using fine-tuned BERT models\n",
    "    \n",
    "    Args:\n",
    "        comments_by_sector: Dict of {sector: [comments]}\n",
    "        save_dir: Directory to save raw predictions\n",
    "        thresholded_save_dir: Directory to save thresholded predictions\n",
    "    \n",
    "    Returns:\n",
    "        results_df, sector_labels\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Find and load all trained pro-anti models\n",
    "    model_dirs = glob.glob(\"models/pro_anti_*_roberta_base\")\n",
    "    if not model_dirs:\n",
    "        # Fallback to DistilBERT models if RoBERTa not found\n",
    "        model_dirs = glob.glob(\"models/pro_anti_*_distilbert_base_uncased\")\n",
    "    \n",
    "    if not model_dirs:\n",
    "        print(\"No trained pro-anti models found!\")\n",
    "        print(\"Expected directories: models/pro_anti_*_roberta_base or models/pro_anti_*_distilbert_base_uncased\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Found {len(model_dirs)} pro-anti model directories: {model_dirs}\")\n",
    "    \n",
    "    # Load all sector models\n",
    "    all_sector_models = {}\n",
    "    optimal_thresholds = {}\n",
    "    sector_labels = {}\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        # Extract sector from directory name (e.g., pro_anti_food_roberta_base -> food)\n",
    "        sector = model_dir.split('_')[2]  # pro_anti_SECTOR_model\n",
    "        \n",
    "        print(f\"Loading {sector.upper()} pro-anti model...\")\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer, label_names, sector_optimal_thresholds = load_trained_pro_anti_model(model_dir, device)\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"Failed to load {sector} pro-anti model\")\n",
    "            continue\n",
    "            \n",
    "        all_sector_models[sector] = {\n",
    "            'model': model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'label_names': label_names\n",
    "        }\n",
    "        \n",
    "        optimal_thresholds[sector] = sector_optimal_thresholds\n",
    "        sector_labels[sector] = label_names\n",
    "        \n",
    "        print(f\" Loaded {sector} model with labels: {label_names}\")\n",
    "        print(f\"  Optimal thresholds: {sector_optimal_thresholds}\")\n",
    "    \n",
    "    if not all_sector_models:\n",
    "        print(\"No pro-anti models loaded successfully!\")\n",
    "        return None, None\n",
    "        \n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(thresholded_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Store all results for return\n",
    "    all_results = []\n",
    "    all_thresholded_results = []\n",
    "    \n",
    "    # Process each sector\n",
    "    for sector, comments in comments_by_sector.items():\n",
    "        if sector not in all_sector_models:\n",
    "            print(f\"Skipping {sector} - no model available\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {sector} comments for pro-anti classification...\")\n",
    "        \n",
    "        model = all_sector_models[sector]['model']\n",
    "        tokenizer = all_sector_models[sector]['tokenizer']\n",
    "        label_names = all_sector_models[sector]['label_names']\n",
    "        \n",
    "        # Get predictions for each comment\n",
    "        sector_results = []\n",
    "        sector_thresholded_results = []\n",
    "        \n",
    "        for text in tqdm(comments, desc=f\"Processing {sector}\"):\n",
    "            # Get predictions\n",
    "            prediction_results = predict_pro_anti_scores(text, model, tokenizer, label_names, device)\n",
    "            scores = [res['score'] for res in prediction_results]\n",
    "            \n",
    "            # Create result dict\n",
    "            result = {'sector': sector, 'text': text}\n",
    "            \n",
    "            # Add scores for each label (pro, anti, neither)\n",
    "            for label, score in zip(label_names, scores):\n",
    "                result[f\"{sector}_{label}\"] = score\n",
    "                \n",
    "            sector_results.append(result)\n",
    "        \n",
    "        # Save raw results for this sector\n",
    "        sector_df = pd.DataFrame(sector_results)\n",
    "        sector_csv_path = os.path.join(save_dir, f\"{sector}_pro_anti_classifications.csv\")\n",
    "        sector_df.to_csv(sector_csv_path, index=False)\n",
    "        print(f\"Saved {sector} pro-anti classifications to {sector_csv_path}\")\n",
    "        all_results.extend(sector_results)\n",
    "        \n",
    "        # Now create thresholded version for this sector\n",
    "        thresholds = optimal_thresholds.get(sector, {})\n",
    "        print(f\"Applying thresholds for {sector}: {thresholds}\")\n",
    "        \n",
    "        for row in sector_results:\n",
    "            thresholded_row = {'sector': sector, 'text': row['text']}\n",
    "            \n",
    "            # Apply optimal thresholds to get binary predictions\n",
    "            scores = [row[f\"{sector}_{label}\"] for label in label_names]\n",
    "            \n",
    "            if thresholds:\n",
    "                # Use optimal thresholds if available\n",
    "                pred_binary = apply_optimal_thresholds(scores, {sector: thresholds}, sector, label_names)\n",
    "            else:\n",
    "                # Default threshold of 0.5\n",
    "                pred_binary = [1 if score >= 0.5 else 0 for score in scores]\n",
    "            \n",
    "            # Add binary predictions\n",
    "            for label, pred in zip(label_names, pred_binary):\n",
    "                thresholded_row[f\"{sector}_{label}\"] = pred\n",
    "            \n",
    "            # Add predicted label (highest score after thresholding)\n",
    "            if sum(pred_binary) > 0:\n",
    "                predicted_label_idx = pred_binary.index(1)  # Get first label with 1\n",
    "                predicted_label = label_names[predicted_label_idx]\n",
    "            else:\n",
    "                predicted_label = 'neither'  # Default if no labels above threshold\n",
    "            \n",
    "            thresholded_row[f\"{sector}_predicted_label\"] = predicted_label\n",
    "            \n",
    "            sector_thresholded_results.append(thresholded_row)\n",
    "        \n",
    "        # Save thresholded results for this sector\n",
    "        thresholded_sector_df = pd.DataFrame(sector_thresholded_results)\n",
    "        thresholded_sector_csv_path = os.path.join(thresholded_save_dir, f\"{sector}_pro_anti_classifications_thresholded.csv\")\n",
    "        thresholded_sector_df.to_csv(thresholded_sector_csv_path, index=False)\n",
    "        print(f\"Saved thresholded {sector} pro-anti classifications to {thresholded_sector_csv_path}\")\n",
    "        all_thresholded_results.extend(sector_thresholded_results)\n",
    "    \n",
    "    # Create combined results dataframes\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    thresholded_df = pd.DataFrame(all_thresholded_results)\n",
    "    \n",
    "    # Save combined results\n",
    "    combined_csv_path = os.path.join(save_dir, \"all_sectors_pro_anti_classifications.csv\")\n",
    "    results_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Saved combined raw pro-anti classifications to {combined_csv_path}\")\n",
    "    \n",
    "    combined_thresholded_csv_path = os.path.join(thresholded_save_dir, \"all_sectors_pro_anti_classifications_thresholded.csv\")\n",
    "    thresholded_df.to_csv(combined_thresholded_csv_path, index=False)\n",
    "    print(f\"Saved combined thresholded pro-anti classifications to {combined_thresholded_csv_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PRO-ANTI CLASSIFICATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for sector in all_sector_models.keys():\n",
    "        if sector in thresholded_df['sector'].values:\n",
    "            sector_data = thresholded_df[thresholded_df['sector'] == sector]\n",
    "            \n",
    "            # Count predictions for each label\n",
    "            label_counts = {}\n",
    "            for label in sector_labels[sector]:\n",
    "                col = f\"{sector}_{label}\"\n",
    "                if col in sector_data.columns:\n",
    "                    label_counts[label] = sector_data[col].sum()\n",
    "            \n",
    "            print(f\"{sector.upper()}:\")\n",
    "            for label, count in label_counts.items():\n",
    "                percentage = (count / len(sector_data)) * 100\n",
    "                print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "            print()\n",
    "    \n",
    "    return results_df, sector_labels\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run pro-anti predictions\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"PRO-ANTI CLASSIFICATION USING FINE-TUNED BERT MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Example usage - you would replace this with your actual comments_by_sector data\n",
    "    # comments_by_sector = {\n",
    "    #     'food': ['comment1', 'comment2', ...],\n",
    "    #     'housing': ['comment1', 'comment2', ...],\n",
    "    #     'transport': ['comment1', 'comment2', ...]\n",
    "    # }\n",
    "    \n",
    "    print(\"Please provide your comments_by_sector data and call get_pro_anti_predictions()\")\n",
    "    print(\"Example:\")\n",
    "    print(\"results_df, sector_labels = get_pro_anti_predictions(comments_by_sector)\")\n",
    "    \n",
    "    print(f\"\\nOutput directories:\")\n",
    "    print(f\"  Raw predictions: paper4data/sectorwise_pro_anti_classifications/\")\n",
    "    print(f\"  Thresholded predictions: paper4data/sectorwise_pro_anti_classifications_thresholded/\")\n",
    "    \n",
    "    print(f\"\\nExpected model directories:\")\n",
    "    print(f\"  models/pro_anti_*_roberta_base\")\n",
    "    print(f\"  models/pro_anti_*_distilbert_base_uncased\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d43b4561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 3 pro-anti model directories: ['models\\\\pro_anti_food_roberta_base', 'models\\\\pro_anti_housing_roberta_base', 'models\\\\pro_anti_transport_roberta_base']\n",
      "Loading FOOD pro-anti model...\n",
      "Loading roberta-base model with labels: ['pro', 'anti', 'neither']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded food model with labels: ['pro', 'anti', 'neither']\n",
      "  Optimal thresholds: {'pro': 0.35000000000000003, 'anti': 0.5, 'neither': 0.6000000000000001}\n",
      "Loading HOUSING pro-anti model...\n",
      "Loading roberta-base model with labels: ['pro', 'anti', 'neither']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded housing model with labels: ['pro', 'anti', 'neither']\n",
      "  Optimal thresholds: {'pro': 0.55, 'anti': 0.55, 'neither': 0.45}\n",
      "Loading TRANSPORT pro-anti model...\n",
      "Loading roberta-base model with labels: ['pro', 'anti', 'neither']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded transport model with labels: ['pro', 'anti', 'neither']\n",
      "  Optimal thresholds: {'pro': 0.5, 'anti': 0.5, 'neither': 0.5}\n",
      "\n",
      "Processing transport comments for pro-anti classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transport: 100%|| 14924/14924 [58:14<00:00,  4.27it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transport pro-anti classifications to paper4data/sectorwise_pro_anti_classifications\\transport_pro_anti_classifications.csv\n",
      "Applying thresholds for transport: {'pro': 0.5, 'anti': 0.5, 'neither': 0.5}\n",
      "Saved thresholded transport pro-anti classifications to paper4data/sectorwise_pro_anti_classifications_thresholded\\transport_pro_anti_classifications_thresholded.csv\n",
      "\n",
      "Processing housing comments for pro-anti classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing housing: 100%|| 9388/9388 [36:16<00:00,  4.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved housing pro-anti classifications to paper4data/sectorwise_pro_anti_classifications\\housing_pro_anti_classifications.csv\n",
      "Applying thresholds for housing: {'pro': 0.55, 'anti': 0.55, 'neither': 0.45}\n",
      "Saved thresholded housing pro-anti classifications to paper4data/sectorwise_pro_anti_classifications_thresholded\\housing_pro_anti_classifications_thresholded.csv\n",
      "\n",
      "Processing food comments for pro-anti classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing food: 100%|| 8414/8414 [22:27<00:00,  6.25it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved food pro-anti classifications to paper4data/sectorwise_pro_anti_classifications\\food_pro_anti_classifications.csv\n",
      "Applying thresholds for food: {'pro': 0.35000000000000003, 'anti': 0.5, 'neither': 0.6000000000000001}\n",
      "Saved thresholded food pro-anti classifications to paper4data/sectorwise_pro_anti_classifications_thresholded\\food_pro_anti_classifications_thresholded.csv\n",
      "Saved combined raw pro-anti classifications to paper4data/sectorwise_pro_anti_classifications\\all_sectors_pro_anti_classifications.csv\n",
      "Saved combined thresholded pro-anti classifications to paper4data/sectorwise_pro_anti_classifications_thresholded\\all_sectors_pro_anti_classifications_thresholded.csv\n",
      "\n",
      "============================================================\n",
      "PRO-ANTI CLASSIFICATION SUMMARY\n",
      "============================================================\n",
      "FOOD:\n",
      "  pro: 3840.0 (45.6%)\n",
      "  anti: 2203.0 (26.2%)\n",
      "  neither: 2250.0 (26.7%)\n",
      "\n",
      "HOUSING:\n",
      "  pro: 2834.0 (30.2%)\n",
      "  anti: 2922.0 (31.1%)\n",
      "  neither: 3401.0 (36.2%)\n",
      "\n",
      "TRANSPORT:\n",
      "  pro: 3709.0 (24.9%)\n",
      "  anti: 5573.0 (37.3%)\n",
      "  neither: 5471.0 (36.7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from paper4_predictionss_by_finetunedBERT import get_pro_anti_predictions\n",
    "\n",
    "# Run predictions for all comments in each sector\n",
    "results_df, sector_labels = get_pro_anti_predictions(comments_by_sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656dd05",
   "metadata": {},
   "source": [
    "# TESTING THE HUGGINFACE UPLOADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a603b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL: TRANSPORT SECTOR\n",
      "============================================================\n",
      "Downloading model: sanchow/electric_vehicles-distilbert-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|| 7/7 [00:08<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "   Labels: ['Alternative Modes', 'Charging Infrastructure', 'Environmental Benefit', 'Grid Impact And Energy Mix', 'Mineral Supply Chain', 'Policy And Mandates', 'Purchase Price']\n",
      "\n",
      "Transport classifier results for transport_examples:\n",
      "\n",
      "Example 1: 'I usually opt for the subway or cycling rather than using my own vehicle.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Alternative Modes: 0.637089\n",
      "  Environmental Benefit: 0.249878\n",
      "  Purchase Price: 0.109989\n",
      "  Charging Infrastructure: 0.098296\n",
      "  Policy And Mandates: 0.081984\n",
      "  Mineral Supply Chain: 0.035143\n",
      "  Grid Impact And Energy Mix: 0.027300\n",
      "----------------------------------------\n",
      "Example 2: 'It's hard to find places to plug in my electric car when I'm out.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Charging Infrastructure: 0.509047\n",
      "  Purchase Price: 0.193760\n",
      "  Alternative Modes: 0.107299\n",
      "  Environmental Benefit: 0.093271\n",
      "  Policy And Mandates: 0.050311\n",
      "  Mineral Supply Chain: 0.021624\n",
      "  Grid Impact And Energy Mix: 0.020597\n",
      "----------------------------------------\n",
      "Example 3: 'Switching to electric cars can cut down on smog and carbon output.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Environmental Benefit: 0.791064\n",
      "  Policy And Mandates: 0.082242\n",
      "  Alternative Modes: 0.077703\n",
      "  Mineral Supply Chain: 0.066538\n",
      "  Grid Impact And Energy Mix: 0.048731\n",
      "  Charging Infrastructure: 0.047536\n",
      "  Purchase Price: 0.032100\n",
      "----------------------------------------\n",
      "Example 4: 'Charging lots of EVs simultaneously could overload our electricity network.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Grid Impact And Energy Mix: 0.655133\n",
      "  Charging Infrastructure: 0.643775\n",
      "  Environmental Benefit: 0.067308\n",
      "  Policy And Mandates: 0.063347\n",
      "  Mineral Supply Chain: 0.039363\n",
      "  Alternative Modes: 0.031763\n",
      "  Purchase Price: 0.026908\n",
      "----------------------------------------\n",
      "Example 5: 'The upfront cost for battery-powered vehicles is too high for some buyers.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Charging Infrastructure: 0.666908\n",
      "  Purchase Price: 0.356522\n",
      "  Policy And Mandates: 0.230006\n",
      "  Mineral Supply Chain: 0.079080\n",
      "  Alternative Modes: 0.030819\n",
      "  Environmental Benefit: 0.027710\n",
      "  Grid Impact And Energy Mix: 0.019902\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "MODEL: HOUSING SECTOR\n",
      "============================================================\n",
      "Downloading model: sanchow/solar_energy-distilbert-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|| 7/7 [00:11<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "   Labels: ['Decommissioning And Waste', 'Foreign Dependence And Trade', 'Grid Stability And Storage', 'Land Use', 'Local Economy', 'Subsidy And Tariff Debate', 'Utility Bills']\n",
      "\n",
      "Housing classifier results for housing_examples:\n",
      "\n",
      "Example 1: 'Solar panels can help reduce electricity bills over time.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Grid Stability And Storage: 0.431048\n",
      "  Utility Bills: 0.283009\n",
      "  Decommissioning And Waste: 0.195417\n",
      "  Land Use: 0.085058\n",
      "  Subsidy And Tariff Debate: 0.074950\n",
      "  Local Economy: 0.063686\n",
      "  Foreign Dependence And Trade: 0.032112\n",
      "----------------------------------------\n",
      "Example 2: 'Disposing of old solar panels is a growing environmental concern.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Decommissioning And Waste: 0.611688\n",
      "  Subsidy And Tariff Debate: 0.135908\n",
      "  Foreign Dependence And Trade: 0.134344\n",
      "  Local Economy: 0.117878\n",
      "  Land Use: 0.109791\n",
      "  Utility Bills: 0.065834\n",
      "  Grid Stability And Storage: 0.045119\n",
      "----------------------------------------\n",
      "Example 3: 'Relying on imported solar panels could be risky for our economy.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Local Economy: 0.355730\n",
      "  Foreign Dependence And Trade: 0.196932\n",
      "  Subsidy And Tariff Debate: 0.163371\n",
      "  Decommissioning And Waste: 0.157960\n",
      "  Land Use: 0.103743\n",
      "  Utility Bills: 0.080972\n",
      "  Grid Stability And Storage: 0.042796\n",
      "----------------------------------------\n",
      "Example 4: 'Solar power can make the grid more resilient during peak demand.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Grid Stability And Storage: 0.802471\n",
      "  Utility Bills: 0.223539\n",
      "  Decommissioning And Waste: 0.114917\n",
      "  Local Economy: 0.101887\n",
      "  Land Use: 0.096930\n",
      "  Subsidy And Tariff Debate: 0.062232\n",
      "  Foreign Dependence And Trade: 0.030996\n",
      "----------------------------------------\n",
      "Example 5: 'Installing solar panels increases the value of my home.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Utility Bills: 0.320304\n",
      "  Local Economy: 0.246066\n",
      "  Decommissioning And Waste: 0.130687\n",
      "  Land Use: 0.129748\n",
      "  Subsidy And Tariff Debate: 0.087671\n",
      "  Grid Stability And Storage: 0.085499\n",
      "  Foreign Dependence And Trade: 0.061210\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "MODEL: FOOD SECTOR\n",
      "============================================================\n",
      "Downloading model: sanchow/veganism_and_vegetarianism-distilbert-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|| 7/7 [00:09<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "   Labels: ['Animal Welfare', 'Environmental Impact', 'Health', 'Lab Grown And Alt Proteins', 'Psychology And Identity', 'Systemic Vs Individual Action', 'Taste And Convenience']\n",
      "\n",
      "Food classifier results for food_examples:\n",
      "\n",
      "Example 1: 'A plant-based diet is better for the environment.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Environmental Impact: 0.982813\n",
      "  Health: 0.117133\n",
      "  Lab Grown And Alt Proteins: 0.097548\n",
      "  Animal Welfare: 0.078281\n",
      "  Psychology And Identity: 0.039711\n",
      "  Systemic Vs Individual Action: 0.034358\n",
      "  Taste And Convenience: 0.033903\n",
      "----------------------------------------\n",
      "Example 2: 'I can't give up cheese, even though I know veganism is healthier.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Health: 0.603810\n",
      "  Taste And Convenience: 0.293494\n",
      "  Psychology And Identity: 0.179412\n",
      "  Lab Grown And Alt Proteins: 0.099888\n",
      "  Animal Welfare: 0.050795\n",
      "  Environmental Impact: 0.036347\n",
      "  Systemic Vs Individual Action: 0.034212\n",
      "----------------------------------------\n",
      "Example 3: 'Lab-grown meat could be the future of sustainable food.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Environmental Impact: 0.928679\n",
      "  Lab Grown And Alt Proteins: 0.577816\n",
      "  Health: 0.140628\n",
      "  Animal Welfare: 0.134059\n",
      "  Taste And Convenience: 0.070143\n",
      "  Systemic Vs Individual Action: 0.032653\n",
      "  Psychology And Identity: 0.013983\n",
      "----------------------------------------\n",
      "Example 4: 'Being vegan is part of my identity and values.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Psychology And Identity: 0.872146\n",
      "  Environmental Impact: 0.119633\n",
      "  Animal Welfare: 0.116534\n",
      "  Systemic Vs Individual Action: 0.114478\n",
      "  Health: 0.090464\n",
      "  Taste And Convenience: 0.069170\n",
      "  Lab Grown And Alt Proteins: 0.019421\n",
      "----------------------------------------\n",
      "Example 5: 'Eating less meat is a personal choice, not a systemic solution.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Health: 0.361565\n",
      "  Taste And Convenience: 0.212709\n",
      "  Psychology And Identity: 0.203567\n",
      "  Systemic Vs Individual Action: 0.152825\n",
      "  Environmental Impact: 0.125763\n",
      "  Animal Welfare: 0.039349\n",
      "  Lab Grown And Alt Proteins: 0.024311\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, os, tempfile\n",
    "from transformers import DistilBertTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def print_sorted_label_scores(label_scores):\n",
    "    # Sort label_scores dict by score descending\n",
    "    sorted_items = sorted(label_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for label, score in sorted_items:\n",
    "        print(f\"  {label}: {score:.6f}\")\n",
    "\n",
    "\n",
    "# ------------------ TRANSPORT ------------------\n",
    "transport_model_link = 'sanchow/electric_vehicles-distilbert-classifier'\n",
    "transport_examples = [\n",
    "    \"Switching to electric cars can cut down on smog and carbon output.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL: TRANSPORT SECTOR\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Downloading model: {transport_model_link}\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    snapshot_download(\n",
    "        repo_id=transport_model_link,\n",
    "        local_dir=temp_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    model_class_path = os.path.join(temp_dir, 'model_class.py')\n",
    "    if not os.path.exists(model_class_path):\n",
    "        print(f\"model_class.py not found in downloaded files\")\n",
    "        print(f\"   Available files: {os.listdir(temp_dir)}\")\n",
    "    else:\n",
    "        sys.path.insert(0, temp_dir)\n",
    "        from model_class import MultilabelClassifier\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(temp_dir)\n",
    "        checkpoint = torch.load(os.path.join(temp_dir, 'model.pt'), map_location='cpu', weights_only=False)\n",
    "        model = MultilabelClassifier(checkpoint['model_name'], len(checkpoint['label_names']))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"   Labels: {checkpoint['label_names']}\")\n",
    "        print(\"\\nTransport classifier results for transport_examples:\\n\")\n",
    "        for i, test_text in enumerate(transport_examples):\n",
    "            inputs = tokenizer(\n",
    "                test_text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = outputs.cpu().numpy() if isinstance(outputs, (tuple, list)) else outputs.cpu().numpy()\n",
    "            label_scores = {label: float(score) for label, score in zip(checkpoint['label_names'], predictions[0])}\n",
    "            print(f\"Example {i+1}: '{test_text}'\")\n",
    "            print(\"Predictions (all label scores, highest first):\")\n",
    "            print_sorted_label_scores(label_scores)\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "# ------------------ HOUSING ------------------\n",
    "housing_model_link = 'sanchow/solar_energy-distilbert-classifier'\n",
    "housing_examples = [\n",
    "    \"Solar panels can help reduce electricity bills over time.\",\n",
    "    \"Disposing of old solar panels is a growing environmental concern.\",\n",
    "    \"Relying on imported solar panels could be risky for our economy.\",\n",
    "    \"Solar power can make the grid more resilient during peak demand.\",\n",
    "    \"Installing solar panels increases the value of my home.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL: HOUSING SECTOR\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Downloading model: {housing_model_link}\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    snapshot_download(\n",
    "        repo_id=housing_model_link,\n",
    "        local_dir=temp_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    model_class_path = os.path.join(temp_dir, 'model_class.py')\n",
    "    if not os.path.exists(model_class_path):\n",
    "        print(f\"model_class.py not found in downloaded files\")\n",
    "        print(f\"   Available files: {os.listdir(temp_dir)}\")\n",
    "    else:\n",
    "        sys.path.insert(0, temp_dir)\n",
    "        from model_class import MultilabelClassifier\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(temp_dir)\n",
    "        checkpoint = torch.load(os.path.join(temp_dir, 'model.pt'), map_location='cpu', weights_only=False)\n",
    "        model = MultilabelClassifier(checkpoint['model_name'], len(checkpoint['label_names']))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"   Labels: {checkpoint['label_names']}\")\n",
    "        print(\"\\nHousing classifier results for housing_examples:\\n\")\n",
    "        for i, test_text in enumerate(housing_examples):\n",
    "            inputs = tokenizer(\n",
    "                test_text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = outputs.cpu().numpy() if isinstance(outputs, (tuple, list)) else outputs.cpu().numpy()\n",
    "            label_scores = {label: float(score) for label, score in zip(checkpoint['label_names'], predictions[0])}\n",
    "            print(f\"Example {i+1}: '{test_text}'\")\n",
    "            print(\"Predictions (all label scores, highest first):\")\n",
    "            print_sorted_label_scores(label_scores)\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "# ------------------ FOOD ------------------\n",
    "food_model_link = 'sanchow/veganism_and_vegetarianism-distilbert-classifier'\n",
    "food_examples = [\n",
    "    \"A plant-based diet is better for the environment.\",\n",
    "    \"I can't give up cheese, even though I know veganism is healthier.\",\n",
    "    \"Lab-grown meat could be the future of sustainable food.\",\n",
    "    \"Being vegan is part of my identity and values.\",\n",
    "    \"Eating less meat is a personal choice, not a systemic solution.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL: FOOD SECTOR\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Downloading model: {food_model_link}\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    snapshot_download(\n",
    "        repo_id=food_model_link,\n",
    "        local_dir=temp_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    model_class_path = os.path.join(temp_dir, 'model_class.py')\n",
    "    if not os.path.exists(model_class_path):\n",
    "        print(f\"model_class.py not found in downloaded files\")\n",
    "        print(f\"   Available files: {os.listdir(temp_dir)}\")\n",
    "    else:\n",
    "        sys.path.insert(0, temp_dir)\n",
    "        from model_class import MultilabelClassifier\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(temp_dir)\n",
    "        checkpoint = torch.load(os.path.join(temp_dir, 'model.pt'), map_location='cpu', weights_only=False)\n",
    "        model = MultilabelClassifier(checkpoint['model_name'], len(checkpoint['label_names']))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(f\"   Labels: {checkpoint['label_names']}\")\n",
    "        print(\"\\nFood classifier results for food_examples:\\n\")\n",
    "        for i, test_text in enumerate(food_examples):\n",
    "            inputs = tokenizer(\n",
    "                test_text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                # outputs shape: (batch_size, num_labels)\n",
    "                predictions = outputs.cpu().numpy() if isinstance(outputs, (tuple, list)) else outputs.cpu().numpy()\n",
    "            # Print all label scores for this example, sorted highest first\n",
    "            label_scores = {label: float(score) for label, score in zip(checkpoint['label_names'], predictions[0])}\n",
    "            print(f\"Example {i+1}: '{test_text}'\")\n",
    "            print(\"Predictions (all label scores, highest first):\")\n",
    "            print_sorted_label_scores(label_scores)\n",
    "            print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 7 files: 100%|| 7/7 [00:03<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "   Labels: ['Animal Welfare', 'Environmental Impact', 'Health', 'Lab Grown And Alt Proteins', 'Psychology And Identity', 'Systemic Vs Individual Action', 'Taste And Convenience']\n",
      "Example 1: 'Your first example text here.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Environmental Impact: 0.322079\n",
      "  Psychology And Identity: 0.194179\n",
      "  Systemic Vs Individual Action: 0.175152\n",
      "  Taste And Convenience: 0.114281\n",
      "  Health: 0.112443\n",
      "  Lab Grown And Alt Proteins: 0.085422\n",
      "  Animal Welfare: 0.075806\n",
      "----------------------------------------\n",
      "Example 2: 'Your second example text here.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Environmental Impact: 0.323291\n",
      "  Psychology And Identity: 0.194841\n",
      "  Systemic Vs Individual Action: 0.187242\n",
      "  Health: 0.119370\n",
      "  Taste And Convenience: 0.116698\n",
      "  Lab Grown And Alt Proteins: 0.094659\n",
      "  Animal Welfare: 0.074074\n",
      "----------------------------------------\n",
      "Example 3: 'Your third example text here.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Environmental Impact: 0.330042\n",
      "  Psychology And Identity: 0.184501\n",
      "  Systemic Vs Individual Action: 0.183944\n",
      "  Health: 0.116585\n",
      "  Taste And Convenience: 0.111938\n",
      "  Lab Grown And Alt Proteins: 0.092307\n",
      "  Animal Welfare: 0.071684\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch,sys,os,tempfile\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def print_sorted_label_scores(label_scores):\n",
    "    # Sort label_scores dict by score descending\n",
    "    sorted_items = sorted(label_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for label, score in sorted_items:\n",
    "        print(f\"  {label}: {score:.6f}\")\n",
    "\n",
    "# Download and load model\n",
    "model_link = \"sanchow/veganism_and_vegetarianism-distilbert-classifier\"\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    snapshot_download(\n",
    "        repo_id=model_link,\n",
    "        local_dir=temp_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    \n",
    "    # Import the model class\n",
    "    sys.path.insert(0, temp_dir)\n",
    "    from model_class import MultilabelClassifier\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(temp_dir)\n",
    "    checkpoint = torch.load(os.path.join(temp_dir, 'model.pt'), map_location='cpu', weights_only=False)\n",
    "    model = MultilabelClassifier(checkpoint['model_name'], len(checkpoint['label_names']))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully\")\n",
    "    print(f\"   Labels: {checkpoint['label_names']}\")\n",
    "\n",
    "# Example usage with multiple examples\n",
    "examples = [\n",
    "    \"You know much carbon a cow .\",\n",
    "    \"Your second example text here.\",\n",
    "    \"Your third example text here.\"\n",
    "]\n",
    "\n",
    "for i, test_text in enumerate(examples):\n",
    "    inputs = tokenizer(\n",
    "        test_text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.cpu().numpy() if isinstance(outputs, (tuple, list)) else outputs.cpu().numpy()\n",
    "    label_scores = {label: float(score) for label, score in zip(checkpoint['label_names'], predictions[0])}\n",
    "    print(f\"Example {i+1}: '{test_text}'\")\n",
    "    print(\"Predictions (all label scores, highest first):\")\n",
    "    print_sorted_label_scores(label_scores)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL: ELECTRIC VEHICLES SECTOR\n",
      "============================================================\n",
      "Downloading model: sanchow/electric_vehicles-distilbert-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|| 7/7 [00:04<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "   Labels: ['Alternative Modes', 'Charging Infrastructure', 'Environmental Benefit', 'Grid Impact And Energy Mix', 'Mineral Supply Chain', 'Policy And Mandates', 'Purchase Price']\n",
      "\n",
      "Electric Vehicles classifier results:\n",
      "\n",
      "Example 1: 'Electric vehicles cost money dude.'\n",
      "Predictions (all label scores, highest first):\n",
      "  Charging Infrastructure: 0.377133\n",
      "  Purchase Price: 0.266446\n",
      "  Alternative Modes: 0.197600\n",
      "  Policy And Mandates: 0.115848\n",
      "  Mineral Supply Chain: 0.049927\n",
      "  Environmental Benefit: 0.038453\n",
      "  Grid Impact And Energy Mix: 0.012627\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, os, tempfile\n",
    "from transformers import DistilBertTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def print_sorted_label_scores(label_scores):\n",
    "    # Sort label_scores dict by score descending\n",
    "    sorted_items = sorted(label_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for label, score in sorted_items:\n",
    "        print(f\"  {label}: {score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405e8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef495023",
   "metadata": {},
   "source": [
    "#PRO ANTI EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4da350e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sector  accuracy\n",
      "0       food  0.601093\n",
      "1    housing  0.654822\n",
      "2  transport  0.721649\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('paper4data/pro_anti_all_examples.csv')\n",
    "\n",
    "# Calculate sectorwise accuracy\n",
    "sector_accuracy = (\n",
    "    df.groupby('sector')['is_correct']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'is_correct': 'accuracy'})\n",
    ")\n",
    "\n",
    "print(sector_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f7c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d506a00",
   "metadata": {},
   "source": [
    "# Loads vegans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d014e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "comments_path = r'H:\\Redit project 4.09.2024\\Reddit data\\comments_reddit.csv'\n",
    "submissions_path = r'H:\\Redit project 4.09.2024\\Reddit data\\submissions_reddit.csv'\n",
    "\n",
    "# Load CSVs\n",
    "comments_df = pd.read_csv(comments_path, low_memory=False)\n",
    "submissions_df = pd.read_csv(submissions_path, low_memory=False)\n",
    "\n",
    "# Isolate rows where subreddit is 'vegan'\n",
    "comments_vegan = comments_df[comments_df['subreddit'].str.lower() == 'vegan']\n",
    "submissions_vegan = submissions_df[submissions_df['subreddit'].str.lower() == 'vegan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vegan comments and submissions DataFrames to CSV in the paper4data directory\n",
    "output_dir = 'paper4data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "comments_vegan.to_csv(os.path.join(output_dir, 'comments_vegan.csv'), index=False)\n",
    "submissions_vegan.to_csv(os.path.join(output_dir, 'submissions_vegan.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713745a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique subreddits\n",
    "unique_subreddits = comments_df['subreddit'].dropna().str.lower().unique()\n",
    "\n",
    "# Find subreddits whose name contains 'meat' or 'veg' (case-insensitive)\n",
    "matching_subreddits = [sub for sub in unique_subreddits if ('meat' in sub) or ('veg' in sub)]\n",
    "\n",
    "# Filter comments where subreddit is in the matching set\n",
    "comments_meat_veg = comments_df[comments_df['subreddit'].str.lower().isin(matching_subreddits)]\n",
    "# Get counts of comments for only the matching subreddits and sort descending\n",
    "matching_counts = comments_df[comments_df['subreddit'].str.lower().isin(matching_subreddits)]['subreddit'].str.lower().value_counts().sort_values(ascending=False)\n",
    "matching_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da336c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comments and submissions DataFrames for the specified subreddits to CSV in the paper4data directory\n",
    "\n",
    "output_dir = 'paper4data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of subreddits to save (all lowercase for matching)\n",
    "target_subreddits = [\n",
    "    'vegan', 'vegancirclejerk', 'vegetarian', 'debateavegan', 'veganfitness',\n",
    "    'veganrecipes', 'shittyveganfoodporn', 'vegande', 'antivegan', 'veganfoodporn',\n",
    "    'meat', 'exvegans', 'veganuk'\n",
    "]\n",
    "\n",
    "# Save comments and submissions for each subreddit\n",
    "for subreddit in target_subreddits:\n",
    "    # Comments\n",
    "    comments_sub = comments_df[comments_df['subreddit'].str.lower() == subreddit]\n",
    "    comments_sub.to_csv(os.path.join(output_dir, f'comments_{subreddit}.csv'), index=False)\n",
    "    # Submissions\n",
    "    submissions_sub = submissions_df[submissions_df['subreddit'].str.lower() == subreddit]\n",
    "    submissions_sub.to_csv(os.path.join(output_dir, f'submissions_{subreddit}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774711d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6c19fda",
   "metadata": {},
   "source": [
    "##  context S- BERT analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e679781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keywords: 100%|| 23/23 [00:22<00:00,  1.03keyword/s]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import paper4_Sbert_analysis\n",
    "importlib.reload(paper4_Sbert_analysis)\n",
    "from paper4_Sbert_analysis import analyze_keyword_contexts\n",
    "\n",
    "# Call the main analysis function \n",
    "keyword_context, strong_keywords = analyze_keyword_contexts(\n",
    "    keyword_comment_map,\n",
    "    sector_keyword_strength\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cb242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                               comment  \\\n",
       " 0    &gt;It just makes 0 practical sense for averag...   \n",
       " 1    &gt; Norway is currently at 95% of new cars so...   \n",
       " 2    &gt;Why not???\\n\\nIn North America 120v 15a is...   \n",
       " 3    It's definitely a concern.  But remember that ...   \n",
       " 4    &gt;Because they don't have plans to make it a...   \n",
       " ..                                                 ...   \n",
       " 295  I think sometimes the problem is just having s...   \n",
       " 296  Impact of global warming r/nature r/environmen...   \n",
       " 297  \"A meat-eaters diet requires 17 times more la...   \n",
       " 298  ethical vegan as an individual - and dont und...   \n",
       " 299  A big topic. Climate activists are downstream,...   \n",
       " \n",
       "                       topic  pro  anti  pro_and_anti  neither  \n",
       " 0         Electric Vehicles  0.0   1.0           0.0      0.0  \n",
       " 1         Electric Vehicles  0.0   0.0           0.0      1.0  \n",
       " 2         Electric Vehicles  0.0   1.0           0.0      0.0  \n",
       " 3         Electric Vehicles  0.0   0.0           0.0      1.0  \n",
       " 4         Electric Vehicles  0.0   1.0           0.0      0.0  \n",
       " ..                      ...  ...   ...           ...      ...  \n",
       " 295  Vegetarianism/Veganism  1.0   0.0           0.0      0.0  \n",
       " 296  Vegetarianism/Veganism  0.0   0.0           0.0      1.0  \n",
       " 297  Vegetarianism/Veganism  1.0   0.0           0.0      0.0  \n",
       " 298  Vegetarianism/Veganism  1.0   0.0           0.0      0.0  \n",
       " 299  Vegetarianism/Veganism  0.0   1.0           0.0      0.0  \n",
       " \n",
       " [300 rows x 6 columns]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f0497",
   "metadata": {},
   "source": [
    "# Sbert contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f53cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 1/4 [00:04<00:12,  4.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 124\u001b[39m\n",
      "\u001b[32m    118\u001b[39m sampled_comments = {\n",
      "\u001b[32m    119\u001b[39m     sector: \u001b[38;5;28mlist\u001b[39m(comments)[:\u001b[32m1000\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(comments) > \u001b[32m1000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(comments)\n",
      "\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sector, comments \u001b[38;5;129;01min\u001b[39;00m comments_by_sector.items()\n",
      "\u001b[32m    121\u001b[39m }\n",
      "\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Get dataframe with comments and scores\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m results_df = \u001b[43mget_comment_label_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_comments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msector_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mget_comment_label_scores\u001b[39m\u001b[34m(model, comments_by_sector, sector_labels)\u001b[39m\n",
      "\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(comments_list), batch_size)):\n",
      "\u001b[32m     62\u001b[39m     batch = comments_list[i:i + batch_size]\n",
      "\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     comment_embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     64\u001b[39m     similarities = cosine_similarity(comment_embeddings, label_embeddings)\n",
      "\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# Create records for each comment\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n",
      "\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n",
      "\u001b[32m   1049\u001b[39m features.update(extra_features)\n",
      "\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m   1054\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n",
      "\u001b[32m   1127\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n",
      "\u001b[32m   1128\u001b[39m         module_kwargs = {\n",
      "\u001b[32m   1129\u001b[39m             key: value\n",
      "\u001b[32m   1130\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n",
      "\u001b[32m   1131\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n",
      "\u001b[32m   1132\u001b[39m         }\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n",
      "\u001b[32m    430\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n",
      "\u001b[32m    431\u001b[39m trans_features = {\n",
      "\u001b[32m    432\u001b[39m     key: value\n",
      "\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n",
      "\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[32m    435\u001b[39m }\n",
      "\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    438\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m    439\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n",
      "\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n",
      "\u001b[32m    991\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n",
      "\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n",
      "\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n",
      "\u001b[32m    994\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1008\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m   1009\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:651\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    649\u001b[39m past_key_value = past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n",
      "\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m     80\u001b[39m         logger.warning(message)\n",
      "\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:595\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n",
      "\u001b[32m    592\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n",
      "\u001b[32m    593\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n",
      "\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n",
      "\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    598\u001b[39m outputs = (layer_output,) + outputs\n",
      "\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\pytorch_utils.py:250\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n",
      "\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n",
      "\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:608\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n",
      "\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n",
      "\u001b[32m    607\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:520\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n",
      "\u001b[32m    519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    521\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n",
      "\u001b[32m    522\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chowdhary\\Desktop\\Redit project 4.09.2024\\sbert_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n",
      "\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import snapshot_download\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def setup_local_model(model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Download and setup the SBERT model locally.\n",
    "    \"\"\"\n",
    "    cache_dir = os.path.join(os.getcwd(), 'models')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        local_model_path = os.path.join(cache_dir, model_name)\n",
    "        if not os.path.exists(local_model_path):\n",
    "            print(f\"Downloading model {model_name} to {local_model_path}...\")\n",
    "            snapshot_download(\n",
    "                repo_id=f\"sentence-transformers/{model_name}\",\n",
    "                local_dir=local_model_path,\n",
    "                local_dir_use_symlinks=False\n",
    "            )\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Device set to use {device}\")\n",
    "        \n",
    "        model = SentenceTransformer(local_model_path, device=device)\n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up local model: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_comment_label_scores(model, comments_by_sector, sector_labels):\n",
    "    \"\"\"\n",
    "    Get similarity scores between comments and labels.\n",
    "    Returns a dataframe with comments and scores for each label.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise ValueError(\"Model not initialized properly\")\n",
    "        \n",
    "    all_results = []\n",
    "    \n",
    "    # Process each sector\n",
    "    for sector, comments in comments_by_sector.items():\n",
    "        if not comments or sector not in sector_labels:\n",
    "            continue\n",
    "            \n",
    "        labels = sector_labels[sector]\n",
    "        comments_list = list(comments)\n",
    "        \n",
    "        # Get embeddings\n",
    "        label_embeddings = model.encode(labels)\n",
    "        \n",
    "        # Process comments in batches\n",
    "        batch_size = 250\n",
    "        for i in tqdm(range(0, len(comments_list), batch_size)):\n",
    "            batch = comments_list[i:i + batch_size]\n",
    "            comment_embeddings = model.encode(batch)\n",
    "            similarities = cosine_similarity(comment_embeddings, label_embeddings)\n",
    "            \n",
    "            # Create records for each comment\n",
    "            for comment, scores in zip(batch, similarities):\n",
    "                result = {\n",
    "                    'comment': comment,\n",
    "                    **{label: score for label, score in zip(labels, scores)}\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate comments by sector\n",
    "comments_by_sector = {}\n",
    "for sector in ['transport', 'housing', 'food']:\n",
    "    strong_comments = set()\n",
    "    weak_comments = set()\n",
    "    \n",
    "    if 'sectors' in locals() or 'sectors' in globals():\n",
    "        strong_key = f'{sector}_strong'\n",
    "        if strong_key in sectors:\n",
    "            for kw in sectors[strong_key]:\n",
    "                if 'keyword_unique_comments' in locals() and kw in keyword_unique_comments:\n",
    "                    strong_comments.update(keyword_unique_comments[kw])\n",
    "                \n",
    "        weak_key = f'{sector}_weak'\n",
    "        if weak_key in sectors:\n",
    "            for kw in sectors[weak_key]:\n",
    "                if 'keyword_unique_comments' in locals() and kw in keyword_unique_comments:\n",
    "                    weak_comments.update(keyword_unique_comments[kw])\n",
    "    \n",
    "    comments_by_sector[sector] = strong_comments.union(weak_comments)\n",
    "\n",
    "# Define sector labels\n",
    "sector_labels = {\n",
    "    'transport': [\n",
    "        'battery cost', 'purchase price', 'range anxiety', \n",
    "        'charging infrastructure', 'environmental benefit',\n",
    "        'running costs', 'reliability', 'fun to drive'\n",
    "    ],\n",
    "    'housing': [\n",
    "        'local economy', 'utility bills', 'visual impact',\n",
    "        'land use', 'tax revenue', 'wildlife'\n",
    "    ],\n",
    "    'food': [\n",
    "        'health', 'environmental impact', 'animal welfare',\n",
    "        'cost', 'taste and convenience'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Setup model and get results\n",
    "model = setup_local_model()\n",
    "\n",
    "# Sample 1000 comments from each sector\n",
    "sampled_comments = {\n",
    "    sector: list(comments)[:1000] if len(comments) > 1000 else list(comments)\n",
    "    for sector, comments in comments_by_sector.items()\n",
    "}\n",
    "\n",
    "# Get dataframe with comments and scores\n",
    "results_df = get_comment_label_scores(model, sampled_comments, sector_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a002ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9419b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example comments for battery cost (score > 0.3):\n",
      "- The small battery idea is a great one. Most US households have more than one car, so shouldn't one of these be a small, inexpensive EV? I'd love something like an $8,000 BYD with a 120-mile battery to...\n",
      "- It really depends. If you use a rough estimate of 1 kWh/panel per day, and you want to charge your P100D Tesla (100  kWh battery) from 0 to 100%, youd need 100 panels.\\n\\nThat is worst case, of cours...\n",
      "- A battery like the one in that trailer very well might be able to do a DC quick charge at 50 kW.  An electric car like the one charging can drive several miles on a kilowatt-hour of energy.  So, 5 or ...\n",
      "\n",
      "Example comments for local economy (score > 0.3):\n",
      "- Yes but so what ? Taxing the people = less consumption = a nice blow to the economy. \\n\\nAnd all this for what ? Building useless solar panels and electric cars that would solve a problem that doesnr...\n",
      "- Upcoming community solar projects in Jersey help fight income disparities...\n",
      "- Disagree.\\nMeat production counts for over 30% of all the impact, more than all transportation industry.\\nBig companies pollute to produce what you buy, start buying from responsible producers/manufac...\n",
      "\n",
      "Example comments for health (score > 0.3):\n",
      "- Vegan diet. \\nCarfree infrastructure.\\nSmall families.\\nMinimalist lifestyle. \\n\\nNuclear fuel is finite and rare. It is possible to life with no or a minimum of electricity. Our rulers are not trying...\n",
      "- Vegan Crap: Doctors Have Panned This Crap Over and Over Again...\n",
      "- The best thing the average person can do is a plant based lifestyle...\n"
     ]
    }
   ],
   "source": [
    "def plot_sector_results(results_df, sector_labels, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Create visualization of sector classification results using SBERT scores.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with comment text and label scores\n",
    "        sector_labels: Dict mapping sectors to label sets\n",
    "        threshold: Score threshold for counting a label match (default 0.3)\n",
    "    \"\"\"\n",
    "    FONT_SIZE = 17\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(14, 24))\n",
    "    gs = fig.add_gridspec(3, 1, height_ratios=[1, 1, 1], hspace=0.3)\n",
    "\n",
    "    # Plot results for each sector\n",
    "    for idx, (sector, labels) in enumerate(sector_labels.items()):\n",
    "        ax = fig.add_subplot(gs[idx, 0])\n",
    "        \n",
    "        # Count comments above threshold for each label\n",
    "        label_counts = {}\n",
    "        for label in labels:\n",
    "            matches = results_df[results_df[label] > threshold]\n",
    "            label_counts[label] = len(matches)\n",
    "            \n",
    "            # Print 3 example comments for first label only\n",
    "            if label == labels[0]:\n",
    "                print(f\"\\nExample comments for {label} (score > {threshold}):\")\n",
    "                for _, row in matches.head(3).iterrows():\n",
    "                    print(f\"- {row['comment'][:200]}...\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total = len(results_df)\n",
    "        labels = list(labels)\n",
    "        freqs = [label_counts[label] for label in labels]\n",
    "        pcts = [count/total * 100 for count in freqs]\n",
    "        \n",
    "        # Create horizontal bar plot\n",
    "        y_pos = np.arange(len(labels))\n",
    "        color = '#43AA8B' if sector == 'transport' else '#F94144' if sector == 'food' else '#FFB703'\n",
    "        ax.barh(y_pos, pcts, color=color, alpha=0.8, height=0.6)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels([label.lower() for label in labels], fontsize=FONT_SIZE*1.54)\n",
    "        ax.set_xlabel('% of comments', fontsize=FONT_SIZE*1.54, labelpad=10)\n",
    "        ax.tick_params(axis='x', labelsize=FONT_SIZE*1.54)\n",
    "        ax.set_title(sector.capitalize(), fontsize=FONT_SIZE*1.8, pad=20)\n",
    "        \n",
    "        # Add percentage and count labels\n",
    "        for i, (pct, count) in enumerate(zip(pcts, freqs)):\n",
    "            ax.text(pct + 1, i, f'{pct:.1f}% (n={count})', va='center',\n",
    "                   fontsize=FONT_SIZE*1.32, color='#444444')\n",
    "        \n",
    "        # Style adjustments\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.grid(False)\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "    try:\n",
    "        plt.savefig('paper4figs/paper4_fig2.pdf', bbox_inches='tight', dpi=300)\n",
    "        plt.savefig('paper4figs/paper4_fig2.png', bbox_inches='tight', dpi=300)\n",
    "    except PermissionError:\n",
    "        print(\"Warning: Could not save figures due to permission error\")\n",
    "    plt.close()\n",
    "\n",
    "# Plot results using the results_df\n",
    "plot_sector_results(results_df, sector_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b2df0",
   "metadata": {},
   "source": [
    "# Regex based subsectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb15b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing transport comments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 14924/14924 [00:07<00:00, 2016.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing housing comments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9388/9388 [00:03<00:00, 2857.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing food comments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8414/8414 [00:04<00:00, 1819.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of transport comments matching each category:\n",
      "Battery Cost: 1.0% (146 comments)\n",
      "Purchase Price: 0.1% (14 comments)\n",
      "Charging Infrastructure: 0.7% (101 comments)\n",
      "Environmental Benefit: 1.9% (280 comments)\n",
      "Running Costs: 0.1% (20 comments)\n",
      "Reliability: 1.0% (147 comments)\n",
      "Fun to Drive: 0.0% (0 comments)\n",
      "\n",
      "Percentage of housing comments matching each category:\n",
      "utility_bills: 4.8% (453 comments)\n",
      "visual_impact: 0.0% (0 comments)\n",
      "tax_revenue: 0.0% (0 comments)\n",
      "wildlife: 0.3% (24 comments)\n",
      "\n",
      "Percentage of food comments matching each category:\n",
      "cost: 1.7% (141 comments)\n",
      "health: 7.3% (612 comments)\n",
      "environmental_impact: 13.6% (1147 comments)\n",
      "animal_welfare: 2.6% (220 comments)\n",
      "taste_convenience: 5.3% (450 comments)\n",
      "\n",
      "\n",
      "=== EXAMPLE COMMENTS FROM TRANSPORT SECTOR ===\n",
      "\n",
      "--- Battery Cost ---\n",
      "Did you know Redwood Materials can recycle around 90% of the battery material, and then sell the components back to battery manufacturers, basically closing the loop, which could potentially help with the EV battery shortage &amp; reduce the mining burden?\\n\\nThey are basically solving the problem of what to do with dead Li-ion batteries and giving them new life in brand new cells.\n",
      "The EV industry comes part-and-parcel with an EV battery recycling industry. \\n\\nThe usual argument is 'there aren't enough of these elements to sustain EVs indefinitely.' Sure. If you don't recycle them. \\n\\nAnd recycling cars NOW is a huge part of the auto industry. It's one of the largest sources of shredded scrap. Not a dealbreaker, and definitely not a reason to keep making ICE vehicles. \\n\\nMass transit and better urban planning, combined with smart applications of EVs, is the answer.\n",
      "- Germany has earmarked 200 billion euros ($220 billion) to fund industrial transformation between now and 2026, including climate protection, hydrogen technology and expansion of the electric vehicle charging network, its finance minister said.\\n\\n    Germany Plans to Phase Out the Sale of Combustion-engine Vehicles to Help Meet Its Ambitious Goal of Getting 15 Million Electric Vehicles on the Road by 2030.\\n\\n    Germany currently has an estimated 570,000 registered battery-powered cars, compris...\n",
      "- A man in Austria saw his Tesla car totally incinerated in a road accident, in mere seconds. Now, the owner is desperately seeking to salvage it. Yet, no one seems willing to try and recycle the supposedly eco-friendly vehicle.\\n\\nThe electric cars fiery demise  and what followed  shows that fossil-fuel-free vehicles still have a way to go before they become renewable and eco-friendly enough to drive us into a green future.\\n\\nDominik Freymuth, a German motorist, had been an enthusiastic suppo...\n",
      "- The electric car revolution needs to woo buyers on average incomes to win sales in the European mass market, but it will crash and burn if manufacturers persist in providing unaffordable vehicles with fanciful range claims.\\n\\nIve been road testing electric cars regularly for more than two years now, and not once has a battery-only vehicle met the claimed capacity for its battery after hours plugged into my home charger. The average shortfall is close to 20% with the MINI electric off by 32%. T...\n",
      "\n",
      "--- Purchase Price ---\n",
      "Its literally under $20k with the tax credit\n",
      "- &gt; Ah. Okay. Anything that costs more to buy, by definition costs more to operate. Got it.\\n\\nWell, unless the functionality or lifespan is significantly larger/longer.\\n\\nAnd in the case of BEV's, the functionality (range, etc) is dramatically lower, the lifespan (especially w/o replacing the battery) is dramatically shorter -- but the initial cost of manufacture is double (and arguably triple, because -- currently at least -- the things are being made and sold in such small numbers, and at M...\n",
      "- Joe Bidens administration hopes to unleash a green revolution by offering hundreds of billions of dollars in subsidies to clean energy companies, but the US presidents flagship legislation also threatens to spark a fresh trade war.\\n\\nThe Biden administration is trying to start a green revolution by providing billions in subsidies to clean energy companies, though it may result in a new trade war. \\n\\nThe Inflation Reduction Act, which was passed by US Congress last summer, earmarks around $36...\n",
      "- Jack Norton, a history professor in Minneapolis, started shopping for an electric vehicle early last year. His requirements were simple: All-wheel drive and a price tag under $55,000.\\n\\nAt first, Norton noticed that few EVs were being shipped to Minnesota, where car companies are contending with looser emissions mandates than in much of the US. So he expanded his search to other states  and then came the sticker shock. The scant electric models Norton could track down at dealerships were only ...\n",
      "- Part 2:\\n\\n&gt;I think you also continue to ignore the negative externalities associated with battery production.\\n\\nI haven't done that at all. I've offered all along that EV production (including the battery) has a cost, but that it's easily overcome by the cost savings during the consumption phase. [Here's a well-respected LCA](http://www.environment.ucla.edu/media_IOE/files/BatteryElectricVehicleLCA2012-rh-ptd.pdf) that I don't fully agree with, but that does a decent enough job of demonstra...\n",
      "\n",
      "--- Charging Infrastructure ---\n",
      "Ohio Plans Federally-funded Fast EV Charging Station Near I-77/US 30 interchange\n",
      "- The last time I took a long trip I started with a full battery. I drove for about 70 miles, stopped at a DC fast charger for 40 minutes. Then I drove to my destination. Once there, I parked at a Level 2 EVSE while I did things for 4 hours. When I came back to the car, the battery was full, so I drove off. This was in a Public Parking lot, so I wasnt blocking anyone or anything. Parking there all day is expected behavior.\\n\\nAt the halfway point back to the house, I once again stopped at a DC fa...\n",
      "- Those trying to sell us on buying electric vehicles often claim that drivers can save money by not having to buy gas, but is that really the case? In many regards, some analysts have their doubts.\\n\\n As Fox Business recently noted, a one-to-one comparison of the cost of charging an EV versus filling a regular auto with gas showed that EV charging appeared cheaper. But many other factors need to be included in the math to make a true analysis  factors that many EV advocates are desperate to ig...\n",
      "- \\n&gt;&gt; You forgot the part where you convert the coal to electricity\\n\\n&gt;I get my electricity from solar and wind\\n\\nYou do?  Good for you not everyone has thier own wind and solar generators.  We were talking about conversion losses.  Looking at the other thread it works out similar which is not as bad as I thought. If you are happy with 25-35 mpg in real world driving conditions. \\n\\n&gt;&gt;Fantasy land. Particularly the 15 min 300 miles\\n\\n&gt;I stand corrected, 187 miles in 15 minute...\n",
      "- Start with yourself. Look at the waste you generate and the things you consume. Are there lifestyle changes you can make? Cloth napkins instead of paper? Coffee mug and french press instead of a keurig squirted in a paper cup? Things like that\\n\\nNext is influencing those around you. Don't be obnoxious, and try to appeal to the persons needs rather than preach. Frugal Aunt who doesn't care about the environment? \"Oh these cloth napkins save me $20 a month on paper towels!\" \"These solar panels sa...\n",
      "\n",
      "--- Environmental Benefit ---\n",
      "- &gt;Would it be a net positive improvement to the environment for me to sell the car and replace it with, let's say, a relatively new but used Prius? How about a brand new EV like a Tesla? \\n\\nFrom a carbon standpoint, both a used Prius and a new EV are vastly preferable to continuing to run the RSX.  This is because the large majority of any car's carbon footprint [is incurred in operations rather than manufacturing](https://www.ioes.ucla.edu/wp-content/uploads/ev-vs-gasoline-cars-practicum-fin...\n",
      "I've been a motorhead all my life. Cars are fun.\\n\\nThey aren't worth burning the planet down over, though.\\n\\nWhich is why my current motorhead project is converting an old Volvo V70 to an electric car. With a little luck it will move way faster than any gas-powered version ever did.\\n\\nThe dream lives on, emission free!\n",
      "- No argument from me.\\n\\nI actually do run LED lights for many reasons -- one being they just last dang longer than incandescent, so there's less waste happening; and yes their wattage is a fraction of incandescent.  But the argument for energy savings is old: turn OFF the lights.  Applies to incandescent or LED :)\\n\\nTo keep your 'warm home in the winter' example going: a person can make a  bigger impact on their carbon footprint if, instead of bulbs, they make their home more efficient to heat ...\n",
      "- Or, regulate/ban cruise ships? At least as a start? I understand and agree with the science for indoor air quality when it comes to gas appliances, but better regulation on ducted hood vents for gas stoves would solve this issue. Gas for heating is definitely the way bigger enemy here, not using the stove for 15-30 minutes once a day. The amount of gas used for cooking is pretty negligible. Its disingenuous to say we must ban gas stoves because they use fossil fuels when there is little to no r...\n",
      "That's 100,000 ICE cars they're not buying. And probably 100,000 used Tesla's on sale in 18 months time - likely for around 20 -30K\\nGiven how well the batteries are holding up, a used Tesla may be a better buy than a new cheaper EV. \\n\\nAnd at an average of about 50,000 miles a year, those cars will avoid about 1.5 million Kg of carbon emissions that would have been emitted if they'd been ICE. Hope the other rental firms switch as well.\n",
      "\n",
      "--- Running Costs ---\n",
      "- It does appear that the battery replacement costs are indeed a large portion of the total cost of ownership, if one keeps the car past the life of the first battery.\\n\\nI have thought of battery replacement costs, just not calculated out the cost per mile.  Now that I have, I do realize that the battery cost is as big as the cost of the electricity - but that the two together still cost less than the total cost of ownership of an ICE car, after gasoline, and the upkeep on the internal combustion...\n",
      "It does increase the value of the house some, although not the entire cost of the system, and you do save on electricity while you owned the house. I use my 10 kW panels with 10 kW battery backup to charge the car so that I save the $2500 a year I was paying for gas plus the lower maintenance cost of an electric car which makes my payback about 6-7 years. And I bought my panels about 3 years ago so the costs is now less.\n",
      "- Hmm! Excellent Scenarios!\\n\\n\\nJust crashed a new car: Insurance would presumably cover full replacement cost including carbon emission credit. Oh your insurance is more expensive to drive a ICE car? Go electric, problem solved. \\n\\n\\nOld equipment: Until there's sufficient quality used green vehicle options there's not much point to taxing emissions because you're mostly wasting time and energy taxing fuels and then either screwing over poor people or giving that $ back to those who are price s...\n",
      "- &gt;If that is the case it is a stupid way of looking at it. You need to factor in the lifetime use of the car.\\n\\nThe author does. I'm just pointing out that he considers more than just the use phase of the car.\\n\\n&gt;It is extremely short sighted when you don't do a total cost of ownership since the fuel and maintenance costs of an EV can be much cheaper.\\n\\nNot yet. Not until there are EVs that are cheaper. I'm interested in buying a Leaf, but there aren't sufficient subsidies in my state to...\n",
      "- They're cheap and getting cheaper.  \\nThere's no fuel cost but there is maintenance cost.  \\ne.g. Wind-farm have to be painted annually to prevent corrosion. Components need to be checked and replaced before they blow-out and take more equipment with them.  \\nSolar is easier still; some glass has to be polished and replaced. If tilt-motors are used they need to be oiled once a bluemoon.  \\nBoth need cycloconverters maintained but that's kind of a wash since every power-plant needs to sync to the...\n",
      "\n",
      "--- Reliability ---\n",
      "Embarrassing EVs: Ford and Rivian Recall Yet More Electric Cars and Trucks\n",
      "- Polestar, in coordination with the National Highway Transportation Safety Administration (NHTSA), has announced a recall of its first-ever model, the Polestar 1 grand touring luxury coupe.\\n\\nAccording to the government agency's documents, 66 model years 2020 and 2021 examples are affected by an issue involving the high-voltage battery system.\\n\\n\\n\\nThe problem is made even worse because there is a risk of a potential fire, and owners will receive no warnings before the battery cells could ov...\n",
      "- He knew how to market his findings. But recall, if the products are no good, they do not have real efficiencies advantages over their competition, which most miss, they are simply fads and fashions. Those pass away over time.\\n\\nIt's easy to understand with the right ideas. The bankers' rule of 72, is this. If we divide the interest (growth rate) into 72, it easily approximates the time of doubling of the principal, OR economic wealth of a company. 7% doubles in 10 years. 15% doubles in 6-7 year...\n",
      "- The puns about the recall of the Chevrolet Bolt EV almost write themselves.\\n\\nBut the significance of the recall by GM, the biggest U.S. automaker, goes far beyond Detroit. The costs of the EV push now underway are likely to be borne by all American taxpayers and ratepayers, not just EV buyers. Three weeks ago, President Joe Biden signed an executive order that aims to make half of all new cars sold in the U.S. in 2030 be electric. Making that happen, according to the automakers, will require h...\n",
      "- An Aussie bloke has exposed some of problems that come with owning a Tesla, including some hidden costs - as problems continue to pile-up for the electric car company.\\n\\nControversial social media star Luke Erwin posted a video on December 29 captioned 'All the bad things about the Tesla model 3 performance', and issued a number of stern warnings for those considering buying one of the luxury electric vehicles.\\n\\nThe man exposed several alarming flaws with the Tesla Model 3 Performance, which ...\n",
      "\n",
      "--- Fun to Drive ---\n",
      "\n",
      "\n",
      "=== EXAMPLE COMMENTS FROM HOUSING SECTOR ===\n",
      "\n",
      "--- utility_bills ---\n",
      "&gt; So every single one of the thousands of companies globally involved in solar power and wind are friends and families of the respective governments? \\n\\nOf course not.  Just the ones getting all the huge subsidies.  Solyndra comes to mind.\\n\\n.\\n\\n&gt;Do you know what perovskite is?\\n\\nWell i just googled it.  I'm guessing its something they put in solar panels now.\n",
      "- &gt;Not uranium. That would last an estimated 200 years at current consumption levels (13% of world electricity generation). Significantly increase generation and you're down to decades.\\n\\nThat's actually something of a myth because it only accounts for land-based reserves of present pricing.  A process that was developed in Japan beginning in the 80s is able to extract uranium from sea water at roughly 2-3x present market prices.  The reserves from this process are several hundred years, even ...\n",
      "The problem is a lack of political will. Reduced feed-in tariffs/reduced subsidy on solar panels, banning onshore wind production, subsidising overseas fossil fuel operations, the list goes on.\\n\\nThese things can be addressed but it will not be likely unless public pressure is involved. Thankfully thats starting to happen.\n",
      "- No argument from me.\\n\\nI actually do run LED lights for many reasons -- one being they just last dang longer than incandescent, so there's less waste happening; and yes their wattage is a fraction of incandescent.  But the argument for energy savings is old: turn OFF the lights.  Applies to incandescent or LED :)\\n\\nTo keep your 'warm home in the winter' example going: a person can make a  bigger impact on their carbon footprint if, instead of bulbs, they make their home more efficient to heat ...\n",
      "- Oh come on, that's a complete joke. Australia is one of the countries best suited for large-scale solar power on the globe. And yet the amount of installed solar power is still close to zero, only picking up recently. Just for example, the University of Queensland now sports Australia's biggest flat panel solar installation in Australia, with a [paltry 1.2 MW capacity](http://www.uq.edu.au/news/?article=22962).\\n\\nPrivate solar installations are *very* economical, a typical household can usually...\n",
      "\n",
      "--- visual_impact ---\n",
      "\n",
      "--- tax_revenue ---\n",
      "\n",
      "--- wildlife ---\n",
      "- &gt;I'm not sure I understand what you're saying. \\n\\nI'm quite certain you aren't understanding what I'm saying. That's OK, though. You are willing to engage in conversation with someone you disagree with and that's pretty rare, so I'm happy to clarify.\\n\\n&gt; I'm not sure what you're actually saying about environmental issues though, except that nuclear energy is the bees knees.\\n\\nThen allow me to expand! Frankly stated, our environmental public policy sucks in many ways. In many cases it ca...\n",
      "- Significant legislation, that would be an understatement. Have you seen the graphs of how much CO2 emission drop we would need to effect every year, here on out? It is a nearly vertical cliff that would represent the systematic and forced shutting down of almost every aspect of modern life. Remember 2020, when COVID19 lockdowns caused a massive drop in emissions? Now think of the economic disturbance that caused, the supply chain collapses, the small businesses that never came back, the millions...\n",
      "- &gt;(Though tbh wind farms are trash compared to nuclear)\\n\\nAt least we agree on this. However, as for wind and solar farms in general, if you care for the environment, you should **not** be defending them at all, in any way. \\n\\n**Renewable Energy Threatens Thousands Of Globally Important Biodiversity Areas  And Its Worsening -**    \\n[https://notrickszone.com/2020/06/01/renewable-energy-threatens-thousands-of-globally-important-biodiversity-areas-and-its-worsening/](https://notrickszone....\n",
      "the current rate of extinction has nothing to do with increased co2 or temperature,\\nits down to habitat loss, in which case you would want to limit intrusions, like not cover the earth in windmills and solar panels, but rather nuclear reactors\n",
      "- its the other way around, humans have staved off mass extinction through our co2 recycling.\\n\\nco2 has been on a steady linear decline for hundreds of millions of years and at the depth of last glacial period hit 180 ppm, and plants die out altogether at 150 ppm.\\nthe trend was that clear that we could extrapolate that it would drop to 150 in any glacial period or within 2 million years, whichever comes first, and that would be the end not just for plants but all of us that need the plants.\\n\\nh...\n",
      "\n",
      "\n",
      "=== EXAMPLE COMMENTS FROM FOOD SECTOR ===\n",
      "\n",
      "--- cost ---\n",
      "- Yes, they gain a whole bunch of weight as a result of being fed grain and soy while not moving around so much.  A similar operation is used for years at a time for dairy.\\n\\nGrass-only cattle operations which don't use a feedlot produce a lot less meat, have a meaningfully different taste due to the lower fat content, and cost something like 4x more per pound.\\n\\nIt would also help a lot if they fed the cattle seaweed (to prevent the methane) and trained the cattle to pee into a sewer to reduce ...\n",
      "- I think sometimes the problem is just having some uncertain sense of doom \"this could happen, that could happen, end of the world\", without some concrete idea of what happens.\\n\\nIf someone told you the world would end but told you exactly the details of how it happens, it already would have a different feeling. You'd have finality and you'd have specifics, not some vague boogeyman.\\n\\nReality is that the world is not ending, it's just a lot of pain is coming up. If we know what is coming up we ...\n",
      "Same here. Back when my grocery bill was half of what it is now, I would often buy plant based meats. Once the cost of living goes back down relative to wages I expect plant based meats will be popular again. Or if plant based meats get cheaper to produce\n",
      "- Stolen from u/mcfleury1000\\n\\nFirst I will address the technologies that can help to prevent or at least mitigate collapse.  Then I will address the feasibility of these solutions.\\n\\n#What Technologies can help prevent or mitigate collapse?\\n\\n##Nuclear Fusion\\n\\nThis is our holy mary pass as far as I can tell.  There is no other power source that can provide a total replacement of fossil fuels.  Renewables like wind and solar are great, but they require rare minerals that are going to be in sh...\n",
      "- You can actually do quite a bit without spending any money. In fact, some measures will actually save you money:\\n\\nOne thing you do every day is eat, so small changes to your diet and eating habits can make a huge impact to your carbon footprint. \\n\\nAgriculture is an industry with quite a large environmental impact, but the impact of different products varies substantially.\\n\\nRegardless of what you eat though, up to [up to 40% of the food in the USA is wasted ](https://www.nrdc.org/resources/...\n",
      "\n",
      "--- health ---\n",
      "True. But you can help it tremendously and also end the suffering of 85 Billion land animals who are slaughtered each year. This title is exactly the kind of thinking that has us in this mess. Its someone elses responsibility. Well if you want to get well and stay healthy a vegan diet will do that for the vast majority of humans. And this is a choice that needs to be made in a personal level.\n",
      "- I agree!! Let's all make sure we've signed/endorsed the [Plant Based Treaty ](https://plantbasedtreaty.org/)\\n\\n&gt;The adoption of a Plant Based Treaty as a companion to the UNFCCC/Paris Agreement will put food systems at the heart of combating the climate crisis. The Treaty aims to halt the widespread degradation of critical ecosystems caused by animal agriculture, to promote a shift to more healthy, sustainable plant-based diets and to actively reverse damage done to planetary functions, ecos...\n",
      "- They'll successfully get some first world inner-city or tree-change SJWs and lefties to dutifully follow the dictates of the UN secretariat and shun meat. The rest of the world is not so stupid. Here, without the pig outside the back door, the chickens all round the house, the ducks and the cows and the carabao, who would eat the grass the weeds and insects? Who would eat the scraps? And who would have time to cultivate, harvest, and preserve all the seasonal produce required to sustain health t...\n",
      "- Veganism is an extreme diet. We hunted before we started farming and cooking plants. Most plants are inedible/toxic for us in their natural state, and have only become edible through generations of genetic manipulation through agriculture. Sustained veganism is impossible in most parts of the world, and is only an option in the richest countries now by the international air transport of fruits and vegetables, which supplies bananas to your local grocery store all year round.\\n\\nAll human culture...\n",
      "- If this is accurate (bc realistically, most people on earth do not eat as much meat as the \\\\~20% that do) - it's still not realistic to just think ppl will BOOM change their habits SO drastically. Esp when the financial implications are huge, these are huge industries in nearly every country. Who is footing that bill? \\n\\nSo, what are the realistic things that we can expect people to do? \\n\\nPeople are not good at change. SO much evidence in support of this, even when it's their own health/ sur...\n",
      "\n",
      "--- environmental_impact ---\n",
      "I feel like a lot of people under the age of 40 will happily change to work against climate change. Almost all of my friends are at least cutting back on meat, we take public transit, repair our old clothes and organise clothes swaps, etc.\\n\\nIt's just so hard to know that 70% of the global CO2 emission comes from 100 companies. If they don't change, going vegan won't mean a lot. But we are still doing it, just to be sure. \n",
      "- Making factory farms illegal wouldn't suddenly stop people wanting to eat meat - it'd just cause far more non factory farms to open which are actually way less efficient in terms of land use than factory farms and would ultimately make climate change worse.  While it is interesting to think in extremes like that, it doesn't actually help, sadly.  There is not a chance that you will convince governments to shut down factory farms nor a meaningful portion of the world to give up eating all meat an...\n",
      "The easiest way to act right now is to go vegan. Don't make it harder than it sounds, just do it.\\n  \\nedit: I knew it, this is not a sub for action against climate change. This is a sub to make you believe that you can keep living the same way you lived.\n",
      "Going vegan reduces your carbon emissions by 60% vegetarianism a close to that figure too. You can also add ecosia to chrome, bike to places instead of taking a car or anything, invest in renewable energy, donate to charities, donate to www.choose.today , and have a 100% renewable energy supplier\n",
      "- &gt;Gill said she has accepted the idea that she is everybodys climate midwife and coaches them to hope through action. \\n\\nAh, to be needed is such a strong drive, and these climate cretins seem to **really** need to be needed it seems.  Hilariously, the AP even compares this \"paleo-ecologist\" (made up designation) with ER doctors during Covid-19.  Lol...can't make this shit up!   \\n\\n&gt;How climate scientists like Gill or emergency room doctors during the height of the COVID-19 pandemic co...\n",
      "\n",
      "--- animal_welfare ---\n",
      "- I thought it was relevant because we were talking about what vegans eat, but its no big deal. \\n\\n&gt; My reason for asking is why many vegans make it a point to announce themselves as vegans\\n\\nI cant speak for everyone, but Im sure you notice the loudest one the most. There are plenty of vegans who you would never know are vegan. \\n\\n&gt; Our omnivore dentition shows we aren't herbivores. Humans have evolved to be natural omnivores.\\n\\nI agree, we did evolve as omnivores. But today its pos...\n",
      "The explanation and video are great. I thought of factory farming as large farms. I guess it's more about animals. I'm nearly vegetarian and it feels so much better. Ty for a good lesson for us all.\n",
      "I think animal ag will continue to crumble as more people choose veganism. Once lab grown meat becomes available, factory farming will likely become obsolete.\n",
      "- &gt; For example, burning coal is so dirty that it singularly account for 40% of global CO2 emissions!!\\n\\nI'd like to see some backup for this because there are a few things that get mixed up when we talk about different sources of global warming gases. For example, often we'll just look at \"energy and transportation\" or we'll only look at CO2. And when we do things like that we end up with over simplistic views of the problem, but when we really dig in to it, it's obvious the problem is much m...\n",
      "- Watch the video *I* linked from Earthling Ed and you will see each point in the What I Learned video broken down. Some key points are:\\n\\n* one of the studies cited assumes that if the world went vegan, we would have to eat all the crops that would be fed to animals and we would eat 4,700 calories each PER DAY. Obviously that wouldn't happen. The land used to feed animals would be used to grow different crops better for human consumption, and the excess land could be rewilded or otherwise better...\n",
      "\n",
      "--- taste_convenience ---\n",
      "- Making factory farms illegal wouldn't suddenly stop people wanting to eat meat - it'd just cause far more non factory farms to open which are actually way less efficient in terms of land use than factory farms and would ultimately make climate change worse.  While it is interesting to think in extremes like that, it doesn't actually help, sadly.  There is not a chance that you will convince governments to shut down factory farms nor a meaningful portion of the world to give up eating all meat an...\n",
      "- [Budget Bytes](https://www.budgetbytes.com/) has cheap and easy vegetarian recipes along with step by step instructions and pictures.\\n\\nDon't try to change your entire diet right away. Ease into it. Incorporate some new recipes and experiment. Don't worry about going full vegan at first either, since a lot of people find that too challenging and get discouraged. For the environment, beef, pork, and goat are the worst, so don't feel too bad if you have some chicken or fish here and there during ...\n",
      "- I never really thought of a burger aa healthy, but I guess if it comes from.plants, I can see how you would expect it to be healthier. I've met people who claimed they were healthy becauae they were vegan, and when I looked at all the processed \"food\" they ate, I realized they were kidding themselves. It may be true that it's \"easy\" to be vegan these days, but it's not healthy if you're still eating a lot of processed foods.\\n\\nI haven't read yet about the environmental impacts etc of the imposs...\n",
      "To be fair, you can reduce your meat consumption by values less than 100%, you don't need to go full vegan to have an impact.\n",
      "- There are ways to preserve or increase the amount of forested land that don't involve increasing CO2 emissions.   Those include:\\n\\n* Reducing the population.  Less people = less demand for food.   This is reasonably achievable with biology based sex ed classes, combined with free long term birth control.\\n\\n* Eat less meat.   Animal agriculture requires 10x the land area and water to produce the same calories as plants.   Any reduction in meat consumption is a step in the right direction.   You...\n"
     ]
    }
   ],
   "source": [
    "# Load regex patterns from JSON files\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('paper4data/regex_improved.json', 'r') as f:\n",
    "    regex_patterns = json.load(f)\n",
    "\n",
    "transport_regex = regex_patterns['transport']\n",
    "housing_regex = regex_patterns['housing'] \n",
    "food_regex = regex_patterns['food']\n",
    "\n",
    "# Calculate comments by sector\n",
    "comments_by_sector = {}\n",
    "for sector in ['transport', 'housing', 'food']:\n",
    "    strong_comments = set()\n",
    "    weak_comments = set()\n",
    "    \n",
    "    if 'sectors' in locals() or 'sectors' in globals():\n",
    "        strong_key = f'{sector}_strong'\n",
    "        if strong_key in sectors:\n",
    "            for kw in sectors[strong_key]:\n",
    "                if 'keyword_unique_comments' in locals() and kw in keyword_unique_comments:\n",
    "                    strong_comments.update(keyword_unique_comments[kw])\n",
    "                \n",
    "        weak_key = f'{sector}_weak'\n",
    "        if weak_key in sectors:\n",
    "            for kw in sectors[weak_key]:\n",
    "                if 'keyword_unique_comments' in locals() and kw in keyword_unique_comments:\n",
    "                    weak_comments.update(keyword_unique_comments[kw])\n",
    "    \n",
    "    comments_by_sector[sector] = strong_comments.union(weak_comments)\n",
    "\n",
    "# Sample comments from each sector\n",
    "sampled_comments = {\n",
    "    sector: list(comments)[0:20000] if len(comments) > 1000 else list(comments)\n",
    "    for sector, comments in comments_by_sector.items()\n",
    "}\n",
    "\n",
    "# Analyze comments for each sector with appropriate regex patterns\n",
    "sector_matches = {}\n",
    "regex_patterns = {\n",
    "    'transport': transport_regex,\n",
    "    'housing': housing_regex,\n",
    "    'food': food_regex\n",
    "}\n",
    "\n",
    "for sector in ['transport', 'housing', 'food']:\n",
    "    matches = {category: 0 for category in regex_patterns[sector].keys()}\n",
    "    total_comments = len(sampled_comments[sector])\n",
    "    \n",
    "    print(f\"\\nProcessing {sector} comments:\")\n",
    "    for comment in tqdm(sampled_comments[sector], total=total_comments):\n",
    "        for category, patterns in regex_patterns[sector].items():\n",
    "            # Check if comment matches the regex patterns\n",
    "            matched = False\n",
    "            \n",
    "            # Check must-have patterns if they exist\n",
    "            if \"must\" in patterns:\n",
    "                must_match = any(re.search(pat, comment, re.IGNORECASE) for pat in patterns[\"must\"])\n",
    "                if not must_match:\n",
    "                    continue\n",
    "                    \n",
    "            # Check any patterns\n",
    "            if \"any\" in patterns:\n",
    "                any_match = any(re.search(pat, comment, re.IGNORECASE) for pat in patterns[\"any\"])\n",
    "                if not any_match:\n",
    "                    continue\n",
    "                    \n",
    "            # Check exclude patterns if they exist\n",
    "            if \"exclude\" in patterns:\n",
    "                exclude_match = any(re.search(pat, comment, re.IGNORECASE) for pat in patterns[\"exclude\"])\n",
    "                if exclude_match:\n",
    "                    continue\n",
    "                    \n",
    "            matches[category] += 1\n",
    "            \n",
    "    sector_matches[sector] = matches\n",
    "\n",
    "# Print percentages for each sector and category\n",
    "for sector in ['transport', 'housing', 'food']:\n",
    "    total_comments = len(sampled_comments[sector])\n",
    "    print(f\"\\nPercentage of {sector} comments matching each category:\")\n",
    "    for category, count in sector_matches[sector].items():\n",
    "        percentage = (count / total_comments) * 100\n",
    "        print(f\"{category}: {percentage:.1f}% ({count} comments)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== EXAMPLE COMMENTS FROM TRANSPORT SECTOR ===\n",
      "\n",
      "--- Battery Cost ---\n",
      "- Did you know Redwood Materials can recycle around 90% of the battery material, and then sell the components back to battery manufacturers, basically closing the loop, which could potentially help with the EV battery shortage &amp; reduce the mining burden?\\n\\nThey are basically solving the problem of what to do with dead Li-ion batteries and giving them new life in brand new cells\n",
      "- If you don't recycle them\n",
      "- \\n\\n    Over the past five years alone, the Energiewende has cost Germany 32 billion ($36 billion) annually, and opposition to renewables is growing in the German countryside\n",
      "- Yet, no one seems willing to try and recycle the supposedly eco-friendly vehicle\n",
      "- \\n\\nThat was until high-profile LBC news radio personality Iain Dale bought an Audi e-tron (Prices start at 80,000-$100,000 after tax and before subsidies)\n",
      "\n",
      "--- Purchase Price ---\n",
      "- Its literally under $20k with the tax credit\n",
      "- \\n\\nAnd in the case of BEV's, the functionality (range, etc) is dramatically lower, the lifespan (especially w/o replacing the battery) is dramatically shorter -- but the initial cost of manufacture is double (and arguably triple, because -- currently at least -- the things are being made and sold in such small numbers, and at MSRP price points {not even touching on the direct Government to consumer subsidy/incentives} that there is ZERO profit in them; whereas the ICE vehicles that are sold at &lt; 1/2 the MSRP price are extremely profitable {even in the most basic models})\n",
      "- \\n\\nThe Biden administration is trying to start a green revolution by providing billions in subsidies to clean energy companies, though it may result in a new trade war\n",
      "- His requirements were simple: All-wheel drive and a price tag under $55,000\n",
      "- Also their cell costs make up only a small portion of their  battery's ultimate price tag - much of the value in their pack comes from how thousands of those cells are assembled and monitored, which is no small task by any stretch\n",
      "\n",
      "--- Charging Infrastructure ---\n",
      "- Ohio Plans Federally-funded Fast EV Charging Station Near I-77/US 30 interchange\n",
      "- Once there, I parked at a Level 2 EVSE while I did things for 4 hours\n",
      "- Costs are rising per kilowatt-hour when using public stations, and using a home charger offers the biggest savings\n",
      "- \\n\\nIf you charge at a charging station fast charging costs more\n",
      "- Maybe you get your church to install a free EV charging station, get your kid's school to compost food waste, or your place of work to reduce plastic packaging\n",
      "\n",
      "--- Environmental Benefit ---\n",
      "- Either way, within three years or so, you will realize a net reduction in your carbon footprint by replacing the RSX\n",
      "- \\n\\nThe dream lives on, emission free!\n",
      "- Applies to incandescent or LED :)\\n\\nTo keep your 'warm home in the winter' example going: a person can make a  bigger impact on their carbon footprint if, instead of bulbs, they make their home more efficient to heat / cool\n",
      "- \\n\\nOnce again, its passing the buck to the average person whose carbon footprint is already very low\n",
      "- 5 million Kg of carbon emissions that would have been emitted if they'd been ICE\n",
      "\n",
      "--- Running Costs ---\n",
      "- \\n\\nSo, your earlier statement was that an electric car is NOT cheaper to operate is still false, just not as far off as I thought\n",
      "- I use my 10 kW panels with 10 kW battery backup to charge the car so that I save the $2500 a year I was paying for gas plus the lower maintenance cost of an electric car which makes my payback about 6-7 years\n",
      "- \\n\\nBut honestly driving EV's is fantastic - they already are lowest total cost of ownership, less maintenance, no stopping at gas stations, etc etc preaching to the choir here\n",
      "- \\n\\n&gt;It is extremely short sighted when you don't do a total cost of ownership since the fuel and maintenance costs of an EV can be much cheaper\n",
      "- The ability and cost of maintenance is the known\n",
      "\n",
      "--- Reliability ---\n",
      "- Embarrassing EVs: Ford and Rivian Recall Yet More Electric Cars and Trucks\n",
      "- \\n\\nThe update prevents the battery from fully charging to eliminate that overheating and fire risk\n",
      "- But recall, if the products are no good, they do not have real efficiencies advantages over their competition, which most miss, they are simply fads and fashions\n",
      "- The cost of all those charging stations will likely be passed on to ratepayers in form of higher electric bills\n",
      "- \\n\\nThe man's 'biggest disappointment' however was the availability and cost of Tesla's network of charging stations, known as Superchargers\n",
      "\n",
      "--- Fun to Drive ---\n",
      "\n",
      "\n",
      "=== EXAMPLE COMMENTS FROM HOUSING SECTOR ===\n",
      "\n",
      "--- utility_bills ---\n",
      "- Just the ones getting all the huge subsidies\n",
      "- A discussion of subsidies in one part of the world is nowhere near as comprehensive as the study I cited\n",
      "- Reduced feed-in tariffs/reduced subsidy on solar panels, banning onshore wind production, subsidising overseas fossil fuel operations, the list goes on\n",
      "- But the argument for energy savings is old: turn OFF the lights\n",
      "- The subsidies are not necessary but make this even more attractive, increasing demand and bringing down the price for everyone\n",
      "\n",
      "--- visual_impact ---\n",
      "\n",
      "--- tax_revenue ---\n",
      "\n",
      "--- wildlife ---\n",
      "- co/essays/we-are-not-edging-up-to-a-mass-extinction\\n\\nWhat you should understand about my perspective, though, is that I see a great many things out there to worry about\n",
      "- With feedback loops already kicking in, mass extinction well underway, localised ecosystem collapses and dead-zones already forming, and all unaccounted for by the IPCC because of their unpredictability, 2 is just a euphemism\n",
      "- That means Dominion Energys new solar facilities will blanket 490 square miles (313,000 acres) of beautiful croplands, scenic areas and habitats that now teem with wildlife\n",
      "- the current rate of extinction has nothing to do with increased co2 or temperature,\\nits down to habitat loss, in which case you would want to limit intrusions, like not cover the earth in windmills and solar panels, but rather nuclear reactors\n",
      "- \\n\\nhumans came off age in the last minute and staved off extinction, we have now bought life at least another 60 million years before co2 runs out, -assuming we for some  dumb reason stop the recycling today\n",
      "\n",
      "\n",
      "=== EXAMPLE COMMENTS FROM FOOD SECTOR ===\n",
      "\n",
      "--- cost ---\n",
      "- \\n\\nGrass-only cattle operations which don't use a feedlot produce a lot less meat, have a meaningfully different taste due to the lower fat content, and cost something like 4x more per pound\n",
      "- Much much so much infinitely cheaper, much easier to maintain for a large population, you always have access to things fairly easily\n",
      "- Or if plant based meats get cheaper to produce\n",
      "- Right now it is quite expensive, but thanks to fracking (/s), the technology has improved quickly and gotten cheaper\n",
      "- Youll also save money\n",
      "\n",
      "--- health ---\n",
      "- Well if you want to get well and stay healthy a vegan diet will do that for the vast majority of humans\n",
      "- The Treaty aims to halt the widespread degradation of critical ecosystems caused by animal agriculture, to promote a shift to more healthy, sustainable plant-based diets and to actively reverse damage done to planetary functions, ecosystem services and biodiversity\n",
      "- And the emperors kept them barely fed, bellies not empty, nutrition not acheived\n",
      "- Most plants are inedible/toxic for us in their natural state, and have only become edible through generations of genetic manipulation through agriculture\n",
      "- You can be vegan but if you are buying packaged goods shipped from who knows where, if you had multiple children, if you buy or lease new cars regularly, if you fly regularly, if you stream TV  or music regularly, if you have a gas stove or heat or cool your home in a non-sustainable manner, etc like you can never really offset your existence as a \"wealthy\" person in the developed world\n",
      "\n",
      "--- environmental_impact ---\n",
      "- If they don't change, going vegan won't mean a lot\n",
      "- I'm vegan\n",
      "- The easiest way to act right now is to go vegan\n",
      "- Going vegan reduces your carbon emissions by 60% vegetarianism a close to that figure too\n",
      "- She works on larger climate action instead of her more focused previous research\n",
      "\n",
      "--- animal_welfare ---\n",
      "- There are plenty of vegans who you would never know are vegan\n",
      "- I'm nearly vegetarian and it feels so much better\n",
      "- I think animal ag will continue to crumble as more people choose veganism\n",
      "- \\n\\nIf we look at every source of every greenhouse gas, things like our diet start to stand out\n",
      "- Even when you take into account land that you can't grow crops on and can only be used by animals, we would still use a lot less land by eating entirely vegan\n",
      "\n",
      "--- taste_convenience ---\n",
      "- \\n\\nIs there a single government in the world that is currently actively advocating reducing meat consumption to a level commensurate with managing climate change?  I haven't checked but I don't think so\n",
      "- Mushrooms also impart a meaty flavor, though they may not be appropriate for every recipe\n",
      "- I try to limit my meat consumption and buy more humanely treated meat where I can\n",
      "- To be fair, you can reduce your meat consumption by values less than 100%, you don't need to go full vegan to have an impact\n",
      "- Any reduction in meat consumption is a step in the right direction\n"
     ]
    }
   ],
   "source": [
    " # Print 5 example comments for each sector and category for manual inspection\n",
    "for sector in ['transport', 'housing', 'food']:\n",
    "    print(f\"\\n\\n=== EXAMPLE COMMENTS FROM {sector.upper()} SECTOR ===\")\n",
    "\n",
    "    # Get all comments for this sector\n",
    "    sector_comments = sampled_comments[sector]\n",
    "    sector_specific_regex = regex_patterns[sector]\n",
    "\n",
    "    for category in sector_specific_regex.keys():\n",
    "        print(f\"\\n--- {category} ---\")\n",
    "        examples = []\n",
    "        count = 0\n",
    "        \n",
    "        # Find comments matching this category's patterns\n",
    "        for comment in sector_comments:\n",
    "            if count >= 5:\n",
    "                break\n",
    "                \n",
    "            patterns = sector_specific_regex[category]\n",
    "            matched = True\n",
    "            \n",
    "            # Check must-have patterns\n",
    "            if \"must\" in patterns:\n",
    "                must_match = False\n",
    "                for pat in patterns[\"must\"]:\n",
    "                    for sentence in comment.split('.'):\n",
    "                        if re.search(pat, sentence, re.IGNORECASE):\n",
    "                            must_match = True\n",
    "                            matched_sentence = sentence\n",
    "                if not must_match:\n",
    "                    continue\n",
    "                    \n",
    "            # Check any patterns  \n",
    "            if \"any\" in patterns:\n",
    "                any_match = False\n",
    "                for pat in patterns[\"any\"]:\n",
    "                    for sentence in comment.split('.'):\n",
    "                        if re.search(pat, sentence, re.IGNORECASE):\n",
    "                            any_match = True\n",
    "                            matched_sentence = sentence\n",
    "                if not any_match:\n",
    "                    continue\n",
    "                    \n",
    "            # Check exclude patterns\n",
    "            if \"exclude\" in patterns:\n",
    "                exclude_match = False\n",
    "                for pat in patterns[\"exclude\"]:\n",
    "                    for sentence in comment.split('.'):\n",
    "                        if re.search(pat, sentence, re.IGNORECASE):\n",
    "                            exclude_match = True\n",
    "                if exclude_match:\n",
    "                    continue\n",
    "                    \n",
    "            examples.append(matched_sentence.strip())\n",
    "            count += 1\n",
    "            \n",
    "        # Print examples found\n",
    "        for i, example in enumerate(examples, 1):\n",
    "            print('- ' + example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4c426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a9e2ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0648fc3f",
   "metadata": {},
   "source": [
    "# SBERT/ FaceBook BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from paper4_BART_arguments import main_sbert\n",
    "# from importlib import reload\n",
    "# import paper4_BART_arguments\n",
    "# paper4_BART_arguments = reload(paper4_BART_arguments)\n",
    "# from paper4_BART_arguments import main_sbert\n",
    "# # Sample comments from each sector\n",
    "# sampled_comments = {\n",
    "#     sector: list(comments)[500:600] if len(comments) > 350 else list(comments)\n",
    "#     for sector, comments in comments_by_sector.items()\n",
    "# }\n",
    "# # Use BART instead\n",
    "# results, results_df, sector_labels = main_sbert(\n",
    "#     comments_by_sector=sampled_comments,\n",
    "#     num_runs=1,\n",
    "#     max_comments_per_sector=100,\n",
    "#     num_workers=4,\n",
    "#     batch_size=40,\n",
    "#     threshold=0.5,\n",
    "#     use_bart=False  # Switch to BART\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THRESHOLD = 0.7\n",
    "# # Print overall statistics first\n",
    "# print(\"=== Overall Statistics ===\")\n",
    "# for sector in sector_labels.keys():\n",
    "#     sector_df = results_df[results_df['sector'] == sector].copy()\n",
    "#     total_comments = len(sector_df)\n",
    "#     print(f\"\\n{sector.upper()} SECTOR:\")\n",
    "#     print(f\"Total comments: {total_comments}\")\n",
    "        \n",
    "#     # Count unlabeled comments (including newly added ones)\n",
    "#     no_labels = sector_df.copy()\n",
    "#     for label in sector_labels[sector]:\n",
    "#         col_name = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "#         no_labels = no_labels[no_labels[col_name] < THRESHOLD]  # Using threshold\n",
    "#     unlabeled_count = len(no_labels)\n",
    "#     print(f\"Unlabeled comments: {unlabeled_count}\")\n",
    "        \n",
    "#     # Count comments per label\n",
    "#     print(\"Comments per label:\")\n",
    "#     for label in sector_labels[sector]:\n",
    "#         label_col = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "#         labeled_count = len(sector_df[sector_df[label_col] >= THRESHOLD])\n",
    "#         print(f\"- {label}: {labeled_count}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Create single output file for all sectors\n",
    "# with open(f'paper4data/gpt3_check_all_sectors_Sbert.txt', 'w', encoding='utf-8') as f:\n",
    "#     # Add meta description at top\n",
    "#     f.write(\"=\" * 80 + \"\\n\")\n",
    "#     f.write(\"FILE CONTENTS DESCRIPTION:\\n\")\n",
    "#     f.write(\"This file contains all comments from all sectors with their applied labels\\n\")\n",
    "#     f.write(\"Format: [Label1, Label2, ...] Comment text\\n\")\n",
    "#     f.write(\"Comments longer than 4000 characters are truncated with '...'\\n\")\n",
    "#     f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "#     # Process each sector\n",
    "#     for sector in sector_labels.keys():\n",
    "#         f.write(f\"\\n{sector.upper()} SECTOR COMMENTS:\\n\")\n",
    "#         f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "#         # Get comments in original order from sampled_comments\n",
    "#         sector_comments = sampled_comments[sector]\n",
    "        \n",
    "#         # Process each comment in original order\n",
    "#         for comment in sector_comments:\n",
    "#             # Find matching row in results_df\n",
    "#             row = results_df[(results_df['sector'] == sector) & (results_df['comment'] == comment)].iloc[0]\n",
    "            \n",
    "#             # Collect all labels that apply to this comment\n",
    "#             applied_labels = []\n",
    "#             for label in sector_labels[sector]:\n",
    "#                 col_name = f\"{sector}_{label.lower().replace(' ', '_')}\"\n",
    "#                 if row[col_name] >= THRESHOLD:\n",
    "#                     applied_labels.append(label)\n",
    "                \n",
    "#             # Format comment text\n",
    "#             comment_text = comment[:4000] + \"...\" if len(comment) > 4000 else comment\n",
    "                \n",
    "#             # Write comment with all its labels in one bracket\n",
    "#             label_text = \", \".join(applied_labels) if applied_labels else \"NO LABELS\"\n",
    "#             f.write(f\"[{label_text}] {comment_text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3c417",
   "metadata": {},
   "source": [
    "# Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fca532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import spacy and download model if needed\n",
    "# import spacy\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# try:\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "# except:\n",
    "#     import sys\n",
    "#     import subprocess\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Function to extract nouns from text\n",
    "# def extract_nouns(text):\n",
    "#     if pd.isna(text):\n",
    "#         return []\n",
    "#     doc = nlp(text)\n",
    "#     nouns = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
    "#     return nouns\n",
    "\n",
    "# # Add nouns column to all_results_df with progress bar\n",
    "# print(\"Processing nouns for all_results_df...\")\n",
    "# start_time = time.time()\n",
    "# tqdm.pandas(desc=\"Extracting nouns\", unit=\"comments\")\n",
    "# all_results_df['nouns'] = all_results_df['comment_text'].progress_apply(extract_nouns)\n",
    "\n",
    "# # First, identify comments in df_joint_nuclear that aren't in all_results_df\n",
    "# missing_comments = df_joint_nuclear[~df_joint_nuclear['Document'].isin(all_results_df['comment_text'])]\n",
    "# GPT_threshold=0\n",
    "\n",
    "# # Create DataFrame with missing comments and 0 scores for all trials\n",
    "# missing_df = pd.DataFrame({\n",
    "#     'comment_text': missing_comments['Document'],\n",
    "#     'trial_1': 0,\n",
    "#     'trial_2': 0, \n",
    "#     'trial_3': 0,\n",
    "#     'trial_4': 0,\n",
    "#     'trial_5': 0,\n",
    "#     'trial_6': 0,\n",
    "#     'trial_7': 0,\n",
    "#     'trial_8': 0,\n",
    "#     'trial_9': 0,\n",
    "#     'mean_score': 0\n",
    "# })\n",
    "\n",
    "# # Add nouns column to missing_df with progress bar\n",
    "# print(\"\\nProcessing nouns for missing comments...\")\n",
    "# tqdm.pandas(desc=\"Extracting nouns\", unit=\"comments\") \n",
    "# missing_df['nouns'] = missing_df['comment_text'].progress_apply(extract_nouns)\n",
    "\n",
    "# # Concatenate with all_results_df\n",
    "# all_results_df = pd.concat([all_results_df, missing_df], ignore_index=True)\n",
    "\n",
    "# # Now create combined DataFrame with all metrics by merging on text content\n",
    "# merged_df = pd.merge(\n",
    "#     all_results_df,\n",
    "#     df_joint_nuclear,\n",
    "#     left_on='comment_text',\n",
    "#     right_on='Document',\n",
    "#     how='inner'\n",
    "# )\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"\\nTotal processing time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# merged_df[['Document','nouns']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_reddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
